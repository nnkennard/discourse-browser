- comment_id: B1eoGoSQpX
  rels:
  - !!python/tuple
    - 5
    - 33
    - 0
    - 5
    - attribution
  - !!python/tuple
    - 5
    - 12
    - 12
    - 33
    - elaboration
  - !!python/tuple
    - 12
    - 24
    - 24
    - 33
    - comparison
  - !!python/tuple
    - 0
    - 33
    - 33
    - 162
    - elaboration
  - !!python/tuple
    - 33
    - 44
    - 44
    - 162
    - elaboration
  - !!python/tuple
    - 44
    - 47
    - 47
    - 70
    - purpose
  - !!python/tuple
    - 47
    - 52
    - 52
    - 70
    - list
  - !!python/tuple
    - 52
    - 70
    - 47
    - 52
    - list
  - !!python/tuple
    - 52
    - 58
    - 58
    - 70
    - purpose
  - !!python/tuple
    - 44
    - 70
    - 70
    - 162
    - elaboration
  - !!python/tuple
    - 70
    - 96
    - 96
    - 162
    - list
  - !!python/tuple
    - 70
    - 74
    - 74
    - 78
    - elaboration
  - !!python/tuple
    - 70
    - 78
    - 78
    - 96
    - elaboration
  - !!python/tuple
    - 96
    - 162
    - 70
    - 96
    - list
  - !!python/tuple
    - 96
    - 105
    - 105
    - 113
    - elaboration
  - !!python/tuple
    - 107
    - 113
    - 105
    - 107
    - attribution
  - !!python/tuple
    - 96
    - 113
    - 113
    - 162
    - elaboration
  - !!python/tuple
    - 113
    - 127
    - 127
    - 162
    - same_unit
  - !!python/tuple
    - 113
    - 119
    - 119
    - 127
    - elaboration
  - !!python/tuple
    - 127
    - 162
    - 113
    - 127
    - same_unit
  - !!python/tuple
    - 127
    - 128
    - 128
    - 162
    - elaboration
  - !!python/tuple
    - 128
    - 139
    - 139
    - 162
    - elaboration
  - !!python/tuple
    - 139
    - 141
    - 141
    - 157
    - elaboration
  - !!python/tuple
    - 139
    - 157
    - 157
    - 162
    - elaboration
  tokens:
  - It
  - has
  - already
  - been
  - noted
  - that
  - GraphSAGE
  - models
  - can
  - achieve
  - superior
  - results
  - -LRB-
  - 20-30
  - '%'
  - improvement
  - -RRB-
  - with
  - a
  - different
  - hyperparameter
  - setting
  - 'on'
  - PPI
  - than
  - what
  - was
  - reported
  - in
  - the
  - original
  - paper
  - .
  - It
  - is
  - even
  - mentioned
  - in
  - the
  - Arxiv
  - version
  - of
  - GraphSAGE
  - .
  - It
  - is
  - unfair
  - to
  - report
  - such
  - low
  - scores
  - and
  - I
  - also
  - encourage
  - the
  - authors
  - to
  - update
  - DGI
  - '''s'
  - hyperparam
  - setting
  - correspondingly
  - and
  - report
  - newer
  - numbers
  - .
  - Hello
  - ','
  - Thank
  - you
  - for
  - your
  - comment
  - '!'
  - All
  - reported
  - improvements
  - in
  - PPI
  - results
  - concern
  - solely
  - the
  - fully
  - supervised
  - setup
  - ;
  - not
  - the
  - unsupervised
  - one
  - .
  - And
  - ','
  - indeed
  - ','
  - this
  - is
  - the
  - supervised
  - result
  - we
  - report
  - --
  - namely
  - ','
  - the
  - avg
  - .
  - pooling
  - architecture
  - from
  - the
  - GaAN
  - paper
  - -LRB-
  - Zhang
  - et
  - al.
  - ','
  - UAI
  - '2018'
  - -RRB-
  - ','
  - which
  - we
  - report
  - ','
  - is
  - one
  - example
  - of
  - a
  - supervised
  - result
  - that
  - substantially
  - -LRB-
  - '30'
  - +
  - '%'
  - -RRB-
  - improves
  - 'on'
  - the
  - supervised
  - result
  - reported
  - in
  - the
  - original
  - GraphSAGE
  - paper
  - -LRB-
  - of
  - '0.612'
  - -RRB-
  - .
- comment_id: B1equOGWCm
  rels:
  - !!python/tuple
    - 0
    - 12
    - 12
    - 18
    - elaboration
  - !!python/tuple
    - 0
    - 18
    - 18
    - 594
    - elaboration
  - !!python/tuple
    - 18
    - 22
    - 22
    - 32
    - list
  - !!python/tuple
    - 22
    - 32
    - 18
    - 22
    - list
  - !!python/tuple
    - 18
    - 32
    - 32
    - 594
    - elaboration
  - !!python/tuple
    - 32
    - 38
    - 38
    - 41
    - purpose
  - !!python/tuple
    - 32
    - 41
    - 41
    - 594
    - elaboration
  - !!python/tuple
    - 41
    - 51
    - 51
    - 75
    - elaboration
  - !!python/tuple
    - 52
    - 75
    - 51
    - 52
    - attribution
  - !!python/tuple
    - 52
    - 55
    - 55
    - 75
    - circumstance
  - !!python/tuple
    - 55
    - 67
    - 67
    - 75
    - elaboration
  - !!python/tuple
    - 41
    - 75
    - 75
    - 594
    - elaboration
  - !!python/tuple
    - 75
    - 96
    - 96
    - 594
    - list
  - !!python/tuple
    - 96
    - 594
    - 75
    - 96
    - list
  - !!python/tuple
    - 96
    - 100
    - 100
    - 594
    - elaboration
  - !!python/tuple
    - 100
    - 102
    - 102
    - 594
    - elaboration
  - !!python/tuple
    - 102
    - 114
    - 114
    - 594
    - list
  - !!python/tuple
    - 114
    - 594
    - 102
    - 114
    - list
  - !!python/tuple
    - 114
    - 264
    - 264
    - 594
    - list
  - !!python/tuple
    - 114
    - 116
    - 116
    - 264
    - list
  - !!python/tuple
    - 116
    - 264
    - 114
    - 116
    - list
  - !!python/tuple
    - 116
    - 127
    - 127
    - 264
    - list
  - !!python/tuple
    - 127
    - 264
    - 116
    - 127
    - list
  - !!python/tuple
    - 127
    - 139
    - 139
    - 148
    - elaboration
  - !!python/tuple
    - 127
    - 148
    - 148
    - 151
    - elaboration
  - !!python/tuple
    - 127
    - 151
    - 151
    - 264
    - elaboration
  - !!python/tuple
    - 151
    - 153
    - 153
    - 264
    - elaboration
  - !!python/tuple
    - 153
    - 156
    - 156
    - 157
    - elaboration
  - !!python/tuple
    - 153
    - 157
    - 157
    - 264
    - elaboration
  - !!python/tuple
    - 157
    - 160
    - 160
    - 264
    - elaboration
  - !!python/tuple
    - 160
    - 172
    - 172
    - 264
    - elaboration
  - !!python/tuple
    - 172
    - 181
    - 181
    - 264
    - elaboration
  - !!python/tuple
    - 181
    - 195
    - 195
    - 207
    - elaboration
  - !!python/tuple
    - 181
    - 207
    - 207
    - 221
    - elaboration
  - !!python/tuple
    - 207
    - 216
    - 216
    - 221
    - elaboration
  - !!python/tuple
    - 181
    - 221
    - 221
    - 264
    - elaboration
  - !!python/tuple
    - 221
    - 234
    - 234
    - 252
    - elaboration
  - !!python/tuple
    - 221
    - 252
    - 252
    - 264
    - elaboration
  - !!python/tuple
    - 252
    - 261
    - 261
    - 264
    - circumstance
  - !!python/tuple
    - 264
    - 594
    - 114
    - 264
    - list
  - !!python/tuple
    - 264
    - 266
    - 266
    - 594
    - elaboration
  - !!python/tuple
    - 266
    - 278
    - 278
    - 594
    - list
  - !!python/tuple
    - 278
    - 594
    - 266
    - 278
    - list
  - !!python/tuple
    - 278
    - 279
    - 279
    - 280
    - elaboration
  - !!python/tuple
    - 278
    - 280
    - 280
    - 290
    - elaboration
  - !!python/tuple
    - 280
    - 284
    - 284
    - 290
    - elaboration
  - !!python/tuple
    - 278
    - 290
    - 290
    - 594
    - elaboration
  - !!python/tuple
    - 290
    - 294
    - 294
    - 307
    - purpose
  - !!python/tuple
    - 294
    - 299
    - 299
    - 307
    - elaboration
  - !!python/tuple
    - 290
    - 307
    - 307
    - 594
    - elaboration
  - !!python/tuple
    - 309
    - 319
    - 307
    - 309
    - attribution
  - !!python/tuple
    - 307
    - 319
    - 319
    - 594
    - elaboration
  - !!python/tuple
    - 319
    - 341
    - 341
    - 594
    - elaboration
  - !!python/tuple
    - 341
    - 480
    - 480
    - 594
    - list
  - !!python/tuple
    - 341
    - 343
    - 343
    - 480
    - list
  - !!python/tuple
    - 343
    - 480
    - 341
    - 343
    - list
  - !!python/tuple
    - 343
    - 354
    - 354
    - 480
    - list
  - !!python/tuple
    - 354
    - 480
    - 343
    - 354
    - list
  - !!python/tuple
    - 354
    - 355
    - 355
    - 370
    - elaboration
  - !!python/tuple
    - 354
    - 370
    - 370
    - 480
    - elaboration
  - !!python/tuple
    - 376
    - 406
    - 370
    - 376
    - attribution
  - !!python/tuple
    - 376
    - 385
    - 385
    - 406
    - elaboration
  - !!python/tuple
    - 385
    - 395
    - 395
    - 406
    - same_unit
  - !!python/tuple
    - 385
    - 386
    - 386
    - 395
    - elaboration
  - !!python/tuple
    - 395
    - 406
    - 385
    - 395
    - same_unit
  - !!python/tuple
    - 395
    - 398
    - 398
    - 406
    - elaboration
  - !!python/tuple
    - 370
    - 406
    - 406
    - 480
    - elaboration
  - !!python/tuple
    - 408
    - 419
    - 406
    - 408
    - attribution
  - !!python/tuple
    - 406
    - 419
    - 419
    - 480
    - elaboration
  - !!python/tuple
    - 431
    - 469
    - 419
    - 431
    - attribution
  - !!python/tuple
    - 419
    - 430
    - 430
    - 431
    - same_unit
  - !!python/tuple
    - 419
    - 425
    - 425
    - 430
    - elaboration
  - !!python/tuple
    - 430
    - 431
    - 419
    - 430
    - same_unit
  - !!python/tuple
    - 431
    - 440
    - 440
    - 469
    - concession
  - !!python/tuple
    - 440
    - 459
    - 459
    - 469
    - elaboration
  - !!python/tuple
    - 419
    - 469
    - 469
    - 480
    - elaboration
  - !!python/tuple
    - 480
    - 594
    - 341
    - 480
    - list
  - !!python/tuple
    - 480
    - 482
    - 482
    - 594
    - elaboration
  - !!python/tuple
    - 482
    - 492
    - 492
    - 501
    - elaboration
  - !!python/tuple
    - 482
    - 501
    - 501
    - 504
    - elaboration
  - !!python/tuple
    - 482
    - 504
    - 504
    - 506
    - elaboration
  - !!python/tuple
    - 482
    - 506
    - 506
    - 594
    - elaboration
  - !!python/tuple
    - 506
    - 509
    - 509
    - 510
    - elaboration
  - !!python/tuple
    - 506
    - 510
    - 510
    - 594
    - elaboration
  - !!python/tuple
    - 510
    - 525
    - 525
    - 594
    - list
  - !!python/tuple
    - 510
    - 513
    - 513
    - 525
    - elaboration
  - !!python/tuple
    - 525
    - 594
    - 510
    - 525
    - list
  - !!python/tuple
    - 525
    - 527
    - 527
    - 539
    - elaboration
  - !!python/tuple
    - 527
    - 531
    - 531
    - 539
    - elaboration
  - !!python/tuple
    - 525
    - 539
    - 539
    - 549
    - elaboration
  - !!python/tuple
    - 525
    - 549
    - 549
    - 594
    - elaboration
  - !!python/tuple
    - 549
    - 556
    - 556
    - 594
    - elaboration
  - !!python/tuple
    - 556
    - 564
    - 564
    - 594
    - elaboration
  - !!python/tuple
    - 569
    - 575
    - 564
    - 569
    - attribution
  - !!python/tuple
    - 564
    - 575
    - 575
    - 594
    - elaboration
  - !!python/tuple
    - 575
    - 583
    - 583
    - 594
    - list
  - !!python/tuple
    - 583
    - 594
    - 575
    - 583
    - list
  tokens:
  - This
  - paper
  - analyzed
  - the
  - global
  - convergence
  - property
  - of
  - SGD
  - in
  - deep
  - learning
  - based
  - 'on'
  - the
  - star-convexity
  - assumption
  - .
  - The
  - claims
  - seem
  - correct
  - and
  - validated
  - empirically
  - with
  - some
  - observations
  - in
  - deep
  - learning
  - .
  - The
  - writing
  - is
  - good
  - and
  - easy
  - to
  - follow
  - .
  - My
  - understanding
  - of
  - the
  - analysis
  - is
  - that
  - all
  - the
  - claims
  - seem
  - to
  - be
  - valid
  - when
  - the
  - solution
  - is
  - in
  - a
  - wide
  - valley
  - of
  - the
  - loss
  - surface
  - where
  - the
  - star-convexity
  - holds
  - ','
  - in
  - general
  - .
  - This
  - has
  - been
  - observed
  - empirically
  - in
  - previous
  - work
  - ','
  - and
  - the
  - experiments
  - 'on'
  - cifar10
  - in
  - Fig.
  - '2'
  - support
  - my
  - hypothesis
  - .
  - My
  - questions
  - are
  - ':'
  - '1'
  - .
  - How
  - to
  - guarantee
  - the
  - star-convexity
  - will
  - be
  - valid
  - in
  - deep
  - learning
  - '?'
  - '2'
  - .
  - What
  - network
  - or
  - data
  - properties
  - can
  - lead
  - to
  - such
  - assumption
  - '?'
  - Also
  - ','
  - this
  - is
  - a
  - missing
  - related
  - work
  - from
  - the
  - algorithmic
  - perspective
  - to
  - explore
  - the
  - global
  - optimization
  - in
  - deep
  - learning
  - ':'
  - Zhang
  - et
  - .
  - al.
  - .
  - CVPR
  - ''''
  - '18'
  - .
  - '``'
  - BPGrad
  - ':'
  - Towards
  - Global
  - Optimality
  - in
  - Deep
  - Learning
  - via
  - Branch
  - and
  - Pruning
  - ''''''
  - .
  - We
  - thank
  - the
  - reviewer
  - for
  - the
  - valuable
  - feedbacks
  - .
  - This
  - paper
  - aims
  - at
  - reporting
  - an
  - interesting
  - star-convex
  - property
  - of
  - the
  - SGD
  - optimization
  - path
  - that
  - has
  - been
  - observed
  - in
  - training
  - a
  - variety
  - of
  - DL
  - models
  - ','
  - including
  - MLP
  - ','
  - CNN
  - ','
  - residual
  - networks
  - and
  - RNN
  - -LRB-
  - verified
  - recently
  - -RRB-
  - .
  - Moreover
  - ','
  - our
  - theory
  - is
  - motivated
  - by
  - such
  - a
  - common
  - observation
  - and
  - attempts
  - to
  - justify
  - the
  - role
  - of
  - this
  - property
  - plays
  - in
  - determining
  - the
  - convergence
  - of
  - the
  - optimization
  - in
  - DL
  - .
  - Our
  - response
  - to
  - the
  - reviewer
  - '''s'
  - comments
  - are
  - provided
  - as
  - follows
  - .
  - '1'
  - .
  - How
  - to
  - guarantee
  - the
  - star-convexity
  - will
  - be
  - valid
  - in
  - deep
  - learning
  - '?'
  - Response
  - ':'
  - We
  - thank
  - the
  - reviewer
  - for
  - pointing
  - out
  - this
  - question
  - .
  - It
  - is
  - definitely
  - interesting
  - to
  - explore
  - the
  - underlying
  - mechanism
  - that
  - leads
  - to
  - such
  - a
  - common
  - observation
  - .
  - We
  - think
  - that
  - over-parameterization
  - can
  - be
  - one
  - of
  - the
  - important
  - factors
  - .
  - We
  - are
  - currently
  - investigating
  - this
  - issue
  - theoretically
  - 'on'
  - some
  - simple
  - networks
  - ','
  - and
  - our
  - understanding
  - so
  - far
  - favors
  - such
  - a
  - direction
  - .
  - '2'
  - .
  - What
  - network
  - or
  - data
  - properties
  - can
  - lead
  - to
  - such
  - assumption
  - '?'
  - Response
  - All
  - our
  - experiments
  - are
  - conducted
  - 'on'
  - practical
  - neural
  - network
  - training
  - tasks
  - with
  - real
  - datasets
  - .
  - From
  - the
  - experiments
  - ','
  - we
  - find
  - that
  - the
  - property
  - holds
  - for
  - a
  - variety
  - of
  - network
  - architectures
  - -LRB-
  - MLP
  - ','
  - CNN
  - ','
  - Inception
  - ','
  - RNN
  - -RRB-
  - and
  - different
  - datasets
  - -LRB-
  - image
  - ','
  - text
  - ','
  - etc
  - -RRB-
  - .
  - We
  - think
  - that
  - this
  - can
  - be
  - an
  - amenable
  - property
  - of
  - over-parameterized
  - network
  - .
  - In
  - fact
  - ','
  - several
  - recent
  - works
  - -LRB-
  - -LSB-
  - 1,2
  - -RSB-
  - -RRB-
  - show
  - that
  - the
  - optimization
  - trajectories
  - of
  - SGD
  - is
  - generally
  - smooth
  - despite
  - the
  - nonconvexity
  - and
  - depth
  - of
  - the
  - networks
  - ','
  - and
  - our
  - star-convexity
  - property
  - can
  - be
  - viewed
  - as
  - another
  - aspect
  - that
  - further
  - promotes
  - theoretical
  - justification
  - to
  - deep
  - learning
  - optimization
  - .
  - We
  - will
  - explore
  - these
  - two
  - questions
  - more
  - in
  - future
  - work
  - .
  - '3'
  - .
  - There
  - is
  - a
  - missing
  - related
  - work
  - from
  - the
  - algorithmic
  - perspective
  - to
  - explore
  - the
  - global
  - optimization
  - in
  - deep
  - learning
  - ':'
  - Zhang
  - et
  - .
  - al.
  - .
  - CVPR
  - ''''
  - '18'
  - .
  - '``'
  - BPGrad
  - ':'
  - Towards
  - Global
  - Optimality
  - in
  - Deep
  - Learning
  - via
  - Branch
  - and
  - Pruning
  - ''''''
  - .
  - Response
  - ':'
  - We
  - thank
  - the
  - reviewer
  - for
  - pointing
  - out
  - this
  - interesting
  - related
  - work
  - .
  - We
  - will
  - cite
  - this
  - work
  - in
  - the
  - upcoming
  - revision
  - .
  - -LSB-
  - '1'
  - -RSB-
  - Li
  - et
  - al.
  - .
  - Visualizing
  - the
  - loss
  - landscape
  - of
  - neural
  - nets
  - .
  - To
  - appear
  - in
  - NIPS
  - '2018'
  - -LSB-
  - '2'
  - -RSB-
  - Eliana
  - Lorch
  - .
  - Visualizing
  - deep
  - network
  - training
  - trajectories
  - with
  - pca
  - .
  - In
  - ICML
  - Workshop
  - 'on'
  - Visualization
  - for
  - Deep
  - Learning
  - ','
  - '2016'
  - .
- comment_id: B1e_Utl5CX
  rels:
  - !!python/tuple
    - 0
    - 353
    - 353
    - 1854
    - textualorganization
  - !!python/tuple
    - 0
    - 2
    - 2
    - 13
    - elaboration
  - !!python/tuple
    - 0
    - 13
    - 13
    - 353
    - elaboration
  - !!python/tuple
    - 13
    - 36
    - 36
    - 230
    - list
  - !!python/tuple
    - 13
    - 18
    - 18
    - 36
    - elaboration
  - !!python/tuple
    - 36
    - 230
    - 13
    - 36
    - list
  - !!python/tuple
    - 36
    - 48
    - 48
    - 91
    - elaboration
  - !!python/tuple
    - 48
    - 67
    - 67
    - 91
    - same_unit
  - !!python/tuple
    - 48
    - 59
    - 59
    - 67
    - elaboration
  - !!python/tuple
    - 67
    - 91
    - 48
    - 67
    - same_unit
  - !!python/tuple
    - 67
    - 69
    - 69
    - 91
    - elaboration
  - !!python/tuple
    - 69
    - 77
    - 77
    - 91
    - elaboration
  - !!python/tuple
    - 36
    - 91
    - 91
    - 230
    - elaboration
  - !!python/tuple
    - 91
    - 106
    - 106
    - 107
    - elaboration
  - !!python/tuple
    - 91
    - 107
    - 107
    - 123
    - purpose
  - !!python/tuple
    - 107
    - 114
    - 114
    - 123
    - elaboration
  - !!python/tuple
    - 91
    - 123
    - 123
    - 230
    - elaboration
  - !!python/tuple
    - 123
    - 149
    - 149
    - 230
    - list
  - !!python/tuple
    - 123
    - 126
    - 126
    - 149
    - purpose
  - !!python/tuple
    - 136
    - 149
    - 126
    - 136
    - attribution
  - !!python/tuple
    - 149
    - 230
    - 123
    - 149
    - list
  - !!python/tuple
    - 149
    - 161
    - 161
    - 230
    - list
  - !!python/tuple
    - 149
    - 155
    - 155
    - 161
    - elaboration
  - !!python/tuple
    - 161
    - 230
    - 149
    - 161
    - list
  - !!python/tuple
    - 161
    - 170
    - 170
    - 230
    - list
  - !!python/tuple
    - 161
    - 163
    - 163
    - 170
    - elaboration
  - !!python/tuple
    - 170
    - 230
    - 161
    - 170
    - list
  - !!python/tuple
    - 170
    - 188
    - 188
    - 195
    - elaboration
  - !!python/tuple
    - 170
    - 195
    - 195
    - 203
    - elaboration
  - !!python/tuple
    - 170
    - 203
    - 203
    - 230
    - elaboration
  - !!python/tuple
    - 203
    - 205
    - 205
    - 230
    - elaboration
  - !!python/tuple
    - 205
    - 212
    - 212
    - 230
    - same_unit
  - !!python/tuple
    - 212
    - 230
    - 205
    - 212
    - same_unit
  - !!python/tuple
    - 212
    - 219
    - 219
    - 230
    - elaboration
  - !!python/tuple
    - 13
    - 230
    - 230
    - 353
    - elaboration
  - !!python/tuple
    - 230
    - 236
    - 236
    - 249
    - elaboration
  - !!python/tuple
    - 236
    - 242
    - 242
    - 249
    - elaboration
  - !!python/tuple
    - 230
    - 249
    - 249
    - 262
    - elaboration
  - !!python/tuple
    - 249
    - 253
    - 253
    - 262
    - elaboration
  - !!python/tuple
    - 230
    - 262
    - 262
    - 353
    - elaboration
  - !!python/tuple
    - 262
    - 266
    - 266
    - 353
    - elaboration
  - !!python/tuple
    - 266
    - 268
    - 268
    - 353
    - elaboration
  - !!python/tuple
    - 268
    - 275
    - 275
    - 353
    - elaboration
  - !!python/tuple
    - 275
    - 298
    - 298
    - 304
    - list
  - !!python/tuple
    - 298
    - 304
    - 275
    - 298
    - list
  - !!python/tuple
    - 275
    - 304
    - 304
    - 353
    - elaboration
  - !!python/tuple
    - 304
    - 321
    - 321
    - 353
    - elaboration
  - !!python/tuple
    - 321
    - 324
    - 324
    - 325
    - elaboration
  - !!python/tuple
    - 321
    - 325
    - 325
    - 353
    - elaboration
  - !!python/tuple
    - 325
    - 328
    - 328
    - 353
    - elaboration
  - !!python/tuple
    - 328
    - 337
    - 337
    - 353
    - elaboration
  - !!python/tuple
    - 337
    - 347
    - 347
    - 353
    - elaboration
  - !!python/tuple
    - 353
    - 1854
    - 0
    - 353
    - textualorganization
  - !!python/tuple
    - 353
    - 864
    - 864
    - 1854
    - topic
  - !!python/tuple
    - 354
    - 370
    - 353
    - 354
    - attribution
  - !!python/tuple
    - 353
    - 370
    - 370
    - 489
    - elaboration
  - !!python/tuple
    - 386
    - 387
    - 370
    - 386
    - attribution
  - !!python/tuple
    - 370
    - 387
    - 387
    - 489
    - elaboration
  - !!python/tuple
    - 387
    - 392
    - 392
    - 401
    - elaboration
  - !!python/tuple
    - 387
    - 401
    - 401
    - 489
    - elaboration
  - !!python/tuple
    - 401
    - 421
    - 421
    - 445
    - elaboration
  - !!python/tuple
    - 401
    - 445
    - 445
    - 489
    - elaboration
  - !!python/tuple
    - 445
    - 449
    - 449
    - 489
    - elaboration
  - !!python/tuple
    - 449
    - 463
    - 463
    - 489
    - same_unit
  - !!python/tuple
    - 449
    - 459
    - 459
    - 463
    - elaboration
  - !!python/tuple
    - 463
    - 489
    - 449
    - 463
    - same_unit
  - !!python/tuple
    - 463
    - 467
    - 467
    - 489
    - elaboration
  - !!python/tuple
    - 467
    - 475
    - 475
    - 489
    - elaboration
  - !!python/tuple
    - 353
    - 489
    - 489
    - 571
    - elaboration
  - !!python/tuple
    - 490
    - 522
    - 489
    - 490
    - attribution
  - !!python/tuple
    - 490
    - 508
    - 508
    - 522
    - purpose
  - !!python/tuple
    - 508
    - 511
    - 511
    - 522
    - elaboration
  - !!python/tuple
    - 518
    - 522
    - 511
    - 518
    - attribution
  - !!python/tuple
    - 511
    - 517
    - 517
    - 518
    - same_unit
  - !!python/tuple
    - 511
    - 512
    - 512
    - 517
    - elaboration
  - !!python/tuple
    - 517
    - 518
    - 511
    - 517
    - same_unit
  - !!python/tuple
    - 489
    - 522
    - 522
    - 571
    - elaboration
  - !!python/tuple
    - 522
    - 538
    - 538
    - 571
    - elaboration
  - !!python/tuple
    - 538
    - 544
    - 544
    - 571
    - elaboration
  - !!python/tuple
    - 353
    - 571
    - 571
    - 864
    - elaboration
  - !!python/tuple
    - 571
    - 576
    - 576
    - 592
    - purpose
  - !!python/tuple
    - 571
    - 592
    - 592
    - 864
    - elaboration
  - !!python/tuple
    - 592
    - 602
    - 602
    - 864
    - elaboration
  - !!python/tuple
    - 610
    - 623
    - 602
    - 610
    - attribution
  - !!python/tuple
    - 602
    - 623
    - 623
    - 864
    - elaboration
  - !!python/tuple
    - 623
    - 635
    - 635
    - 864
    - elaboration
  - !!python/tuple
    - 640
    - 665
    - 635
    - 640
    - attribution
  - !!python/tuple
    - 635
    - 665
    - 665
    - 864
    - circumstance
  - !!python/tuple
    - 665
    - 675
    - 675
    - 689
    - elaboration
  - !!python/tuple
    - 675
    - 683
    - 683
    - 689
    - elaboration
  - !!python/tuple
    - 683
    - 685
    - 685
    - 689
    - list
  - !!python/tuple
    - 685
    - 689
    - 683
    - 685
    - list
  - !!python/tuple
    - 665
    - 689
    - 689
    - 864
    - elaboration
  - !!python/tuple
    - 689
    - 700
    - 700
    - 739
    - elaboration
  - !!python/tuple
    - 700
    - 733
    - 733
    - 738
    - elaboration
  - !!python/tuple
    - 700
    - 738
    - 738
    - 739
    - elaboration
  - !!python/tuple
    - 689
    - 739
    - 739
    - 864
    - elaboration
  - !!python/tuple
    - 739
    - 740
    - 740
    - 757
    - elaboration
  - !!python/tuple
    - 740
    - 745
    - 745
    - 757
    - elaboration
  - !!python/tuple
    - 745
    - 753
    - 753
    - 757
    - same_unit
  - !!python/tuple
    - 745
    - 749
    - 749
    - 753
    - elaboration
  - !!python/tuple
    - 753
    - 757
    - 745
    - 753
    - same_unit
  - !!python/tuple
    - 739
    - 757
    - 757
    - 864
    - elaboration
  - !!python/tuple
    - 773
    - 780
    - 757
    - 773
    - antithesis
  - !!python/tuple
    - 757
    - 761
    - 761
    - 773
    - elaboration
  - !!python/tuple
    - 761
    - 767
    - 767
    - 773
    - elaboration
  - !!python/tuple
    - 757
    - 780
    - 780
    - 864
    - elaboration
  - !!python/tuple
    - 783
    - 801
    - 780
    - 783
    - attribution
  - !!python/tuple
    - 780
    - 801
    - 801
    - 864
    - elaboration
  - !!python/tuple
    - 801
    - 813
    - 813
    - 822
    - contrast
  - !!python/tuple
    - 813
    - 822
    - 801
    - 813
    - contrast
  - !!python/tuple
    - 801
    - 822
    - 822
    - 864
    - elaboration
  - !!python/tuple
    - 822
    - 827
    - 827
    - 836
    - purpose
  - !!python/tuple
    - 822
    - 836
    - 836
    - 864
    - elaboration
  - !!python/tuple
    - 836
    - 847
    - 847
    - 864
    - elaboration
  - !!python/tuple
    - 864
    - 1854
    - 353
    - 864
    - topic
  - !!python/tuple
    - 864
    - 1216
    - 1216
    - 1854
    - textualorganization
  - !!python/tuple
    - 864
    - 875
    - 875
    - 1216
    - textualorganization
  - !!python/tuple
    - 875
    - 1216
    - 864
    - 875
    - textualorganization
  - !!python/tuple
    - 875
    - 889
    - 889
    - 922
    - elaboration
  - !!python/tuple
    - 889
    - 913
    - 913
    - 922
    - elaboration
  - !!python/tuple
    - 875
    - 922
    - 922
    - 1216
    - elaboration
  - !!python/tuple
    - 922
    - 941
    - 941
    - 1216
    - same_unit
  - !!python/tuple
    - 922
    - 934
    - 934
    - 941
    - elaboration
  - !!python/tuple
    - 934
    - 935
    - 935
    - 941
    - elaboration
  - !!python/tuple
    - 941
    - 1216
    - 922
    - 941
    - same_unit
  - !!python/tuple
    - 941
    - 958
    - 958
    - 1216
    - elaboration
  - !!python/tuple
    - 958
    - 975
    - 975
    - 1216
    - list
  - !!python/tuple
    - 974
    - 975
    - 958
    - 974
    - attribution
  - !!python/tuple
    - 975
    - 1216
    - 958
    - 975
    - list
  - !!python/tuple
    - 975
    - 994
    - 994
    - 1216
    - elaboration
  - !!python/tuple
    - 994
    - 999
    - 999
    - 1018
    - elaboration
  - !!python/tuple
    - 994
    - 1018
    - 1018
    - 1216
    - elaboration
  - !!python/tuple
    - 1018
    - 1035
    - 1035
    - 1216
    - elaboration
  - !!python/tuple
    - 1035
    - 1040
    - 1040
    - 1216
    - same_unit
  - !!python/tuple
    - 1035
    - 1037
    - 1037
    - 1040
    - purpose
  - !!python/tuple
    - 1040
    - 1216
    - 1035
    - 1040
    - same_unit
  - !!python/tuple
    - 1040
    - 1045
    - 1045
    - 1216
    - temporal
  - !!python/tuple
    - 1045
    - 1053
    - 1053
    - 1216
    - elaboration
  - !!python/tuple
    - 1053
    - 1054
    - 1054
    - 1067
    - list
  - !!python/tuple
    - 1054
    - 1067
    - 1053
    - 1054
    - list
  - !!python/tuple
    - 1054
    - 1058
    - 1058
    - 1067
    - elaboration
  - !!python/tuple
    - 1053
    - 1067
    - 1067
    - 1216
    - elaboration
  - !!python/tuple
    - 1067
    - 1087
    - 1087
    - 1216
    - elaboration
  - !!python/tuple
    - 1087
    - 1111
    - 1111
    - 1216
    - elaboration
  - !!python/tuple
    - 1111
    - 1123
    - 1123
    - 1129
    - circumstance
  - !!python/tuple
    - 1111
    - 1129
    - 1129
    - 1216
    - elaboration
  - !!python/tuple
    - 1129
    - 1154
    - 1154
    - 1216
    - elaboration
  - !!python/tuple
    - 1154
    - 1165
    - 1165
    - 1172
    - elaboration
  - !!python/tuple
    - 1154
    - 1172
    - 1172
    - 1216
    - elaboration
  - !!python/tuple
    - 1172
    - 1176
    - 1176
    - 1216
    - list
  - !!python/tuple
    - 1173
    - 1176
    - 1172
    - 1173
    - attribution
  - !!python/tuple
    - 1176
    - 1216
    - 1172
    - 1176
    - list
  - !!python/tuple
    - 1176
    - 1190
    - 1190
    - 1216
    - same_unit
  - !!python/tuple
    - 1176
    - 1186
    - 1186
    - 1190
    - elaboration
  - !!python/tuple
    - 1190
    - 1216
    - 1176
    - 1190
    - same_unit
  - !!python/tuple
    - 1190
    - 1194
    - 1194
    - 1216
    - elaboration
  - !!python/tuple
    - 1194
    - 1202
    - 1202
    - 1216
    - elaboration
  - !!python/tuple
    - 1216
    - 1854
    - 864
    - 1216
    - textualorganization
  - !!python/tuple
    - 1217
    - 1249
    - 1216
    - 1217
    - attribution
  - !!python/tuple
    - 1217
    - 1235
    - 1235
    - 1249
    - purpose
  - !!python/tuple
    - 1235
    - 1238
    - 1238
    - 1249
    - elaboration
  - !!python/tuple
    - 1245
    - 1249
    - 1238
    - 1245
    - attribution
  - !!python/tuple
    - 1238
    - 1244
    - 1244
    - 1245
    - same_unit
  - !!python/tuple
    - 1238
    - 1239
    - 1239
    - 1244
    - elaboration
  - !!python/tuple
    - 1244
    - 1245
    - 1238
    - 1244
    - same_unit
  - !!python/tuple
    - 1216
    - 1249
    - 1249
    - 1854
    - elaboration
  - !!python/tuple
    - 1249
    - 1582
    - 1582
    - 1854
    - topic
  - !!python/tuple
    - 1249
    - 1265
    - 1265
    - 1582
    - elaboration
  - !!python/tuple
    - 1265
    - 1269
    - 1269
    - 1582
    - elaboration
  - !!python/tuple
    - 1269
    - 1277
    - 1277
    - 1318
    - elaboration
  - !!python/tuple
    - 1277
    - 1285
    - 1285
    - 1318
    - elaboration
  - !!python/tuple
    - 1269
    - 1318
    - 1318
    - 1582
    - elaboration
  - !!python/tuple
    - 1326
    - 1340
    - 1318
    - 1326
    - attribution
  - !!python/tuple
    - 1329
    - 1340
    - 1326
    - 1329
    - attribution
  - !!python/tuple
    - 1318
    - 1340
    - 1340
    - 1582
    - elaboration
  - !!python/tuple
    - 1353
    - 1379
    - 1340
    - 1353
    - attribution
  - !!python/tuple
    - 1340
    - 1379
    - 1379
    - 1549
    - elaboration
  - !!python/tuple
    - 1387
    - 1402
    - 1379
    - 1387
    - attribution
  - !!python/tuple
    - 1387
    - 1396
    - 1396
    - 1402
    - elaboration
  - !!python/tuple
    - 1396
    - 1399
    - 1399
    - 1402
    - purpose
  - !!python/tuple
    - 1379
    - 1402
    - 1402
    - 1549
    - elaboration
  - !!python/tuple
    - 1414
    - 1549
    - 1402
    - 1414
    - attribution
  - !!python/tuple
    - 1414
    - 1421
    - 1421
    - 1427
    - elaboration
  - !!python/tuple
    - 1414
    - 1427
    - 1427
    - 1440
    - elaboration
  - !!python/tuple
    - 1414
    - 1440
    - 1440
    - 1549
    - elaboration
  - !!python/tuple
    - 1440
    - 1452
    - 1452
    - 1467
    - elaboration
  - !!python/tuple
    - 1440
    - 1467
    - 1467
    - 1549
    - elaboration
  - !!python/tuple
    - 1467
    - 1512
    - 1512
    - 1549
    - list
  - !!python/tuple
    - 1467
    - 1468
    - 1468
    - 1512
    - elaboration
  - !!python/tuple
    - 1468
    - 1494
    - 1494
    - 1512
    - elaboration
  - !!python/tuple
    - 1494
    - 1506
    - 1506
    - 1512
    - question
  - !!python/tuple
    - 1494
    - 1500
    - 1500
    - 1506
    - elaboration
  - !!python/tuple
    - 1506
    - 1512
    - 1494
    - 1506
    - question
  - !!python/tuple
    - 1512
    - 1549
    - 1467
    - 1512
    - list
  - !!python/tuple
    - 1512
    - 1518
    - 1518
    - 1549
    - reason
  - !!python/tuple
    - 1518
    - 1531
    - 1531
    - 1549
    - elaboration
  - !!python/tuple
    - 1536
    - 1549
    - 1531
    - 1536
    - attribution
  - !!python/tuple
    - 1531
    - 1534
    - 1534
    - 1536
    - purpose
  - !!python/tuple
    - 1340
    - 1549
    - 1549
    - 1582
    - elaboration
  - !!python/tuple
    - 1555
    - 1582
    - 1549
    - 1555
    - attribution
  - !!python/tuple
    - 1555
    - 1564
    - 1564
    - 1582
    - elaboration
  - !!python/tuple
    - 1564
    - 1565
    - 1565
    - 1582
    - elaboration
  - !!python/tuple
    - 1565
    - 1570
    - 1570
    - 1582
    - elaboration
  - !!python/tuple
    - 1570
    - 1574
    - 1574
    - 1582
    - elaboration
  - !!python/tuple
    - 1574
    - 1578
    - 1578
    - 1582
    - elaboration
  - !!python/tuple
    - 1582
    - 1854
    - 1249
    - 1582
    - topic
  - !!python/tuple
    - 1582
    - 1739
    - 1739
    - 1854
    - list
  - !!python/tuple
    - 1582
    - 1598
    - 1598
    - 1605
    - contrast
  - !!python/tuple
    - 1582
    - 1586
    - 1586
    - 1598
    - elaboration
  - !!python/tuple
    - 1586
    - 1592
    - 1592
    - 1598
    - elaboration
  - !!python/tuple
    - 1598
    - 1605
    - 1582
    - 1598
    - contrast
  - !!python/tuple
    - 1582
    - 1605
    - 1605
    - 1739
    - elaboration
  - !!python/tuple
    - 1608
    - 1626
    - 1605
    - 1608
    - attribution
  - !!python/tuple
    - 1605
    - 1626
    - 1626
    - 1739
    - elaboration
  - !!python/tuple
    - 1626
    - 1631
    - 1631
    - 1665
    - elaboration
  - !!python/tuple
    - 1631
    - 1642
    - 1642
    - 1665
    - elaboration
  - !!python/tuple
    - 1643
    - 1665
    - 1642
    - 1643
    - attribution
  - !!python/tuple
    - 1655
    - 1665
    - 1643
    - 1655
    - antithesis
  - !!python/tuple
    - 1643
    - 1648
    - 1648
    - 1655
    - elaboration
  - !!python/tuple
    - 1655
    - 1661
    - 1661
    - 1665
    - purpose
  - !!python/tuple
    - 1663
    - 1665
    - 1661
    - 1663
    - attribution
  - !!python/tuple
    - 1626
    - 1665
    - 1665
    - 1739
    - elaboration
  - !!python/tuple
    - 1665
    - 1670
    - 1670
    - 1694
    - elaboration
  - !!python/tuple
    - 1670
    - 1689
    - 1689
    - 1694
    - elaboration
  - !!python/tuple
    - 1665
    - 1694
    - 1694
    - 1739
    - elaboration
  - !!python/tuple
    - 1697
    - 1723
    - 1694
    - 1697
    - attribution
  - !!python/tuple
    - 1697
    - 1706
    - 1706
    - 1723
    - circumstance
  - !!python/tuple
    - 1706
    - 1717
    - 1717
    - 1722
    - elaboration
  - !!python/tuple
    - 1706
    - 1722
    - 1722
    - 1723
    - elaboration
  - !!python/tuple
    - 1694
    - 1723
    - 1723
    - 1739
    - elaboration
  - !!python/tuple
    - 1739
    - 1854
    - 1582
    - 1739
    - list
  - !!python/tuple
    - 1739
    - 1750
    - 1750
    - 1854
    - textualorganization
  - !!python/tuple
    - 1750
    - 1854
    - 1739
    - 1750
    - textualorganization
  - !!python/tuple
    - 1750
    - 1764
    - 1764
    - 1797
    - elaboration
  - !!python/tuple
    - 1764
    - 1788
    - 1788
    - 1797
    - elaboration
  - !!python/tuple
    - 1750
    - 1797
    - 1797
    - 1854
    - elaboration
  - !!python/tuple
    - 1797
    - 1826
    - 1826
    - 1854
    - elaboration
  - !!python/tuple
    - 1826
    - 1834
    - 1834
    - 1842
    - purpose
  - !!python/tuple
    - 1826
    - 1842
    - 1842
    - 1854
    - elaboration
  tokens:
  - Summary
  - ':'
  - The
  - authors
  - study
  - building
  - models
  - for
  - edits
  - in
  - source
  - code
  - .
  - The
  - application
  - is
  - obvious
  - ':'
  - a
  - system
  - to
  - accurately
  - predict
  - what
  - the
  - next
  - edit
  - should
  - be
  - would
  - be
  - very
  - valuable
  - for
  - developers
  - .
  - Here
  - ','
  - edits
  - are
  - modeled
  - by
  - two
  - types
  - of
  - sequences
  - ':'
  - one
  - that
  - tracks
  - the
  - state
  - of
  - all
  - edits
  - at
  - each
  - time
  - step
  - -LRB-
  - and
  - is
  - thus
  - very
  - long
  - -RRB-
  - ','
  - and
  - one
  - that
  - contains
  - the
  - initial
  - step
  - and
  - a
  - changelist
  - that
  - contains
  - the
  - minimal
  - information
  - required
  - to
  - derive
  - the
  - state
  - at
  - any
  - time
  - .
  - The
  - authors
  - train
  - models
  - 'on'
  - top
  - of
  - both
  - of
  - these
  - representations
  - ','
  - with
  - the
  - idea
  - being
  - to
  - match
  - the
  - performance
  - of
  - the
  - explicit
  - -LRB-
  - heavy
  - -RRB-
  - model
  - with
  - the
  - implicit
  - model
  - .
  - This
  - is
  - shown
  - to
  - be
  - challenging
  - ','
  - but
  - a
  - clever
  - model
  - is
  - introduced
  - that
  - achieves
  - this
  - ','
  - and
  - is
  - thus
  - the
  - best
  - of
  - both
  - worlds
  - .
  - There
  - are
  - synthetic
  - and
  - real-world
  - code
  - -LRB-
  - text
  - -RRB-
  - edit
  - experiments
  - .
  - Strengths
  - ':'
  - The
  - problem
  - is
  - well-posed
  - and
  - well-motivated
  - .
  - There
  - '''s'
  - a
  - nice
  - application
  - of
  - powerful
  - existing
  - models
  - ','
  - combined
  - and
  - tailored
  - to
  - the
  - current
  - work
  - .
  - The
  - writing
  - is
  - generally
  - quite
  - clear
  - .
  - The
  - number
  - of
  - experiments
  - is
  - quite
  - solid
  - .
  - Weaknesses
  - ':'
  - The
  - main
  - flaw
  - is
  - that
  - nothing
  - here
  - is
  - really
  - specifically
  - for
  - souce
  - code
  - ;
  - the
  - authors
  - are
  - really
  - just
  - modeling
  - edits
  - in
  - text
  - sequences
  - .
  - There
  - '''s'
  - not
  - an
  - obvious
  - way
  - to
  - integrate
  - the
  - kinds
  - of
  - constraints
  - that
  - source
  - code
  - typically
  - satisfies
  - either
  - .
  - There
  - '''s'
  - some
  - confusion
  - -LRB-
  - for
  - me
  - -RRB-
  - about
  - the
  - implicit/explicit
  - representations
  - .
  - More
  - questions
  - below
  - .
  - Verdict
  - ':'
  - This
  - is
  - a
  - pretty
  - solid
  - paper
  - .
  - It
  - does
  - n't
  - quite
  - match
  - up
  - to
  - its
  - title
  - ','
  - but
  - it
  - sets
  - out
  - a
  - clearly
  - defined
  - problem
  - ','
  - achieves
  - its
  - aims
  - ','
  - and
  - introduces
  - some
  - nice
  - tricks
  - .
  - Although
  - it
  - does
  - n't
  - produce
  - anything
  - genuinely
  - groundbreaking
  - ','
  - it
  - seems
  - like
  - a
  - nice
  - step
  - forward
  - .
  - Comments
  - and
  - Questions
  - ':'
  - '-'
  - The
  - problem
  - is
  - written
  - in
  - the
  - context
  - of
  - source
  - code
  - ','
  - but
  - it
  - '''s'
  - really
  - setup
  - just
  - for
  - text
  - sequences
  - ','
  - which
  - is
  - a
  - broader
  - problem
  - .
  - Is
  - there
  - a
  - way
  - the
  - authors
  - take
  - can
  - advantage
  - of
  - the
  - structural
  - requirements
  - for
  - source
  - code
  - '?'
  - I
  - do
  - n't
  - see
  - an
  - obvious
  - way
  - ','
  - but
  - I
  - '''m'
  - curious
  - what
  - the
  - authors
  - think
  - .
  - '-'
  - What
  - '''s'
  - the
  - benefit
  - of
  - using
  - the
  - implicit
  - representation
  - for
  - the
  - positions
  - '?'
  - The
  - explicit/implicit
  - position
  - forms
  - are
  - basically
  - just
  - using
  - the
  - permutation
  - or
  - the
  - inverse
  - permutation
  - form
  - ','
  - which
  - are
  - equivalent
  - .
  - I
  - do
  - n't
  - see
  - directly
  - what
  - '''s'
  - saved
  - here
  - ','
  - the
  - alphabet
  - size
  - and
  - the
  - number
  - of
  - integers
  - to
  - store
  - is
  - the
  - same
  - .
  - '-'
  - Similar
  - question
  - .
  - The
  - implicit
  - likelihood
  - is
  - s
  - ^
  - '0'
  - ','
  - e
  - ^
  - -LRB-
  - '1'
  - -RRB-
  - ','
  - '...'
  - ','
  - e
  - ^
  - -LRB-
  - t-1
  - -RRB-
  - ','
  - with
  - the
  - e
  - ^
  - -LRB-
  - i
  - -RRB-
  - '''s'
  - being
  - based
  - 'on'
  - the
  - implicit
  - representations
  - of
  - the
  - positions
  - .
  - Seems
  - like
  - you
  - could
  - do
  - this
  - with
  - the
  - '*'
  - explicit
  - '*'
  - positions
  - just
  - fine
  - ','
  - they
  - carry
  - enough
  - information
  - to
  - derive
  - s
  - ^
  - -LRB-
  - i
  - -RRB-
  - from
  - s
  - ^
  - -LRB-
  - i-1
  - -RRB-
  - .
  - That
  - is
  - ','
  - the
  - explicit/implicit
  - problems
  - are
  - not
  - really
  - related
  - to
  - the
  - explicit/implicit
  - position
  - representations
  - .
  - '-'
  - Just
  - wanted
  - to
  - point
  - out
  - that
  - this
  - type
  - of
  - approach
  - to
  - sequences
  - and
  - edits
  - has
  - been
  - studied
  - pretty
  - often
  - in
  - the
  - information/coding
  - theory
  - communities
  - ','
  - especially
  - in
  - the
  - area
  - of
  - synchronization
  - .
  - There
  - ','
  - the
  - idea
  - is
  - to
  - create
  - the
  - minimal
  - '``'
  - changelist
  - ''''''
  - of
  - insertions/deletions
  - from
  - two
  - versions
  - of
  - a
  - file
  - .
  - This
  - could
  - come
  - in
  - handy
  - when
  - building
  - the
  - datasets
  - .
  - See
  - ','
  - for
  - example
  - ','
  - Sala
  - et
  - al
  - '``'
  - Synchronizing
  - files
  - from
  - a
  - large
  - number
  - of
  - insertions
  - and
  - deletions
  - ''''''
  - .
  - '-'
  - The
  - problem
  - statement
  - should
  - be
  - stated
  - a
  - bit
  - more
  - rigorously
  - .
  - We
  - '''d'
  - like
  - to
  - say
  - that
  - the
  - initial
  - state
  - is
  - drawn
  - from
  - some
  - distribution
  - and
  - that
  - the
  - state
  - at
  - each
  - time
  - forms
  - a
  - stochastic
  - process
  - with
  - some
  - transition
  - law
  - .
  - As
  - it
  - stands
  - the
  - problem
  - is
  - n't
  - well-defined
  - ','
  - since
  - with
  - 'no'
  - probability
  - distribution
  - ','
  - there
  - '''s'
  - nothing
  - to
  - predict
  - and
  - 'no'
  - likelihood
  - .
  - '-'
  - The
  - '``'
  - analogical
  - decoder
  - ''''''
  - idea
  - is
  - really
  - nice
  - .
  - '-'
  - For
  - the
  - synthetic
  - dataset
  - ','
  - why
  - are
  - you
  - selecting
  - a
  - random
  - initial
  - string
  - ','
  - rather
  - than
  - using
  - some
  - existing
  - generative
  - text
  - or
  - source
  - code
  - model
  - ','
  - which
  - would
  - get
  - you
  - synthetic
  - data
  - that
  - more
  - closely
  - resembles
  - code
  - '?'
  - '-'
  - I
  - really
  - liked
  - the
  - idea
  - of
  - using
  - an
  - oracle
  - that
  - gives
  - the
  - position
  - as
  - upper
  - bound
  - .
  - Would
  - it
  - make
  - sense
  - to
  - also
  - have
  - the
  - opposite
  - oracle
  - that
  - gives
  - the
  - edit
  - symbol
  - ','
  - but
  - does
  - n't
  - tell
  - the
  - location
  - '?'
  - I
  - '''m'
  - really
  - curious
  - which
  - is
  - the
  - '``'
  - harder
  - ''''''
  - task
  - ','
  - predicting
  - the
  - next
  - symbol
  - or
  - the
  - next
  - location
  - .
  - In
  - the
  - information-theory
  - setting
  - ','
  - these
  - two
  - are
  - actually
  - equally
  - hard
  - ','
  - but
  - the
  - real-world
  - setting
  - might
  - be
  - pretty
  - different
  - .
  - It
  - would
  - also
  - be
  - interesting
  - to
  - train
  - models
  - 'on'
  - top
  - of
  - the
  - POMP
  - .
  - That
  - would
  - produce
  - genuine
  - upper
  - bounds
  - to
  - the
  - model
  - performances
  - .
  - '-'
  - The
  - explicit
  - baseline
  - model
  - performs
  - very
  - well
  - 'on'
  - all
  - the
  - edit
  - types
  - in
  - Table
  - '1'
  - .
  - Are
  - there
  - cases
  - where
  - even
  - this
  - explicit
  - case
  - works
  - poorly
  - '?'
  - Is
  - the
  - improved
  - implicit
  - model
  - '*'
  - always
  - '*'
  - upper
  - bounded
  - by
  - the
  - explicit
  - model
  - -LRB-
  - to
  - me
  - it
  - seems
  - like
  - the
  - answer
  - should
  - always
  - be
  - 'yes'
  - ','
  - but
  - it
  - would
  - be
  - interesting
  - to
  - check
  - it
  - out
  - for
  - cases
  - where
  - explicit
  - is
  - not
  - very
  - high
  - accuracy
  - -RRB-
  - .
  - Thanks
  - for
  - the
  - positive
  - review
  - and
  - the
  - careful
  - attention
  - to
  - detail
  - '!'
  - Questions
  - are
  - answered
  - inline
  - below
  - .
  - '>'
  - Is
  - there
  - a
  - way
  - the
  - authors
  - take
  - can
  - advantage
  - of
  - the
  - structural
  - requirements
  - for
  - source
  - code
  - '?'
  - I
  - do
  - n't
  - see
  - an
  - obvious
  - way
  - ','
  - but
  - I
  - '''m'
  - curious
  - what
  - the
  - authors
  - think
  - .
  - This
  - is
  - an
  - excellent
  - insight
  - ','
  - and
  - we
  - have
  - definitely
  - considered
  - extensions
  - of
  - our
  - models
  - to
  - structured
  - data
  - .
  - As
  - this
  - was
  - a
  - question
  - that
  - was
  - highlighted
  - by
  - multiple
  - reviewers
  - ','
  - please
  - see
  - our
  - response
  - in
  - the
  - general
  - comment
  - to
  - all
  - reviewers
  - .
  - The
  - short
  - answer
  - is
  - that
  - we
  - do
  - believe
  - such
  - extensions
  - are
  - possible
  - ','
  - but
  - in
  - this
  - work
  - we
  - wanted
  - to
  - stay
  - focused
  - 'on'
  - formulating
  - the
  - problem
  - itself
  - before
  - taking
  - advantage
  - of
  - more
  - domain-specific
  - structures
  - .
  - '>'
  - What
  - '''s'
  - the
  - benefit
  - of
  - using
  - the
  - implicit
  - representation
  - for
  - the
  - positions
  - '?'
  - The
  - explicit/implicit
  - position
  - forms
  - are
  - basically
  - just
  - using
  - the
  - permutation
  - or
  - the
  - inverse
  - permutation
  - form
  - ','
  - which
  - are
  - equivalent
  - .
  - I
  - do
  - n't
  - see
  - directly
  - what
  - '''s'
  - saved
  - here
  - ','
  - the
  - alphabet
  - size
  - and
  - the
  - number
  - of
  - integers
  - to
  - store
  - is
  - the
  - same
  - .
  - A
  - key
  - property
  - of
  - implicit
  - positions
  - is
  - that
  - they
  - do
  - n't
  - change
  - as
  - more
  - edits
  - are
  - made
  - .
  - This
  - enables
  - the
  - training
  - efficiency
  - like
  - in
  - Vaswani
  - et
  - al
  - ','
  - where
  - we
  - can
  - train
  - 'on'
  - all
  - output
  - steps
  - with
  - a
  - single
  - forward-backward
  - pass
  - .
  - We
  - would
  - n't
  - be
  - able
  - to
  - do
  - this
  - with
  - explicit
  - positions
  - that
  - potentially
  - changed
  - after
  - each
  - edit
  - .
  - '>'
  - Similar
  - question
  - .
  - The
  - implicit
  - likelihood
  - is
  - s
  - ^
  - '0'
  - ','
  - e
  - ^
  - -LRB-
  - '1'
  - -RRB-
  - ','
  - '...'
  - ','
  - e
  - ^
  - -LRB-
  - t-1
  - -RRB-
  - ','
  - with
  - the
  - e
  - ^
  - -LRB-
  - i
  - -RRB-
  - '''s'
  - being
  - based
  - 'on'
  - the
  - implicit
  - representations
  - of
  - the
  - positions
  - .
  - Seems
  - like
  - you
  - could
  - do
  - this
  - with
  - the
  - '*'
  - explicit
  - '*'
  - positions
  - just
  - fine
  - ','
  - they
  - carry
  - enough
  - information
  - to
  - derive
  - s
  - ^
  - -LRB-
  - i
  - -RRB-
  - from
  - s
  - ^
  - -LRB-
  - i-1
  - -RRB-
  - .
  - That
  - is
  - ','
  - the
  - explicit/implicit
  - problems
  - are
  - not
  - really
  - related
  - to
  - the
  - explicit/implicit
  - position
  - representations
  - .
  - This
  - is
  - fair
  - .
  - It
  - '''s'
  - possible
  - to
  - derive
  - the
  - explicit
  - positions
  - given
  - the
  - previous
  - edits
  - in
  - the
  - implicit
  - representation
  - -LRB-
  - like
  - applying
  - a
  - diff
  - -RRB-
  - ','
  - and
  - indeed
  - ','
  - in
  - the
  - experiments
  - we
  - are
  - leveraging
  - the
  - fact
  - that
  - it
  - is
  - valid
  - to
  - compare
  - results
  - between
  - models
  - trained
  - 'on'
  - the
  - different
  - representations
  - .
  - The
  - point
  - we
  - '''re'
  - trying
  - to
  - make
  - here
  - is
  - just
  - one
  - of
  - how
  - the
  - data
  - is
  - presented
  - to
  - the
  - neural
  - network
  - .
  - In
  - the
  - implicit
  - case
  - ','
  - it
  - '''s'
  - up
  - to
  - the
  - neural
  - network
  - to
  - '``'
  - learn
  - how
  - to
  - apply
  - the
  - diff
  - ''''''
  - whereas
  - in
  - the
  - explicit
  - case
  - ','
  - an
  - external
  - deterministic
  - algorithm
  - applies
  - the
  - diff
  - for
  - the
  - neural
  - network
  - .
  - So
  - implicit
  - vs
  - explicit
  - is
  - more
  - about
  - distinguishing
  - between
  - what
  - kinds
  - of
  - representations
  - the
  - network
  - is
  - given
  - vs
  - being
  - forced
  - to
  - learn
  - .
  - We
  - '''ll'
  - clarify
  - the
  - wording
  - in
  - '``'
  - Problem
  - Statement
  - ''''''
  - to
  - say
  - that
  - there
  - is
  - only
  - one
  - problem
  - ','
  - which
  - is
  - to
  - predict
  - next
  - edits
  - given
  - initial
  - state
  - and
  - previous
  - edits
  - ','
  - but
  - two
  - families
  - of
  - architectures
  - ':'
  - explicit
  - assumes
  - an
  - external
  - algorithm
  - applies
  - the
  - diffs
  - to
  - get
  - explicit
  - states
  - that
  - are
  - fed
  - into
  - the
  - network
  - ','
  - and
  - implicit
  - works
  - directly
  - 'off'
  - the
  - edits
  - .
  - '>'
  - For
  - the
  - synthetic
  - dataset
  - ','
  - why
  - are
  - you
  - selecting
  - a
  - random
  - initial
  - string
  - ','
  - rather
  - than
  - using
  - some
  - existing
  - generative
  - text
  - or
  - source
  - code
  - model
  - ','
  - which
  - would
  - get
  - you
  - synthetic
  - data
  - that
  - more
  - closely
  - resembles
  - code
  - '?'
  - It
  - would
  - certainly
  - be
  - possible
  - .
  - The
  - random
  - initial
  - strings
  - were
  - simpler
  - because
  - we
  - have
  - control
  - over
  - the
  - vocabulary
  - size
  - and
  - length
  - of
  - sequences
  - ','
  - which
  - allowed
  - us
  - to
  - control
  - how
  - long
  - the
  - initial
  - strings
  - were
  - and
  - how
  - many
  - edits
  - were
  - applied
  - .
  - With
  - real
  - code
  - ','
  - we
  - think
  - it
  - would
  - introduce
  - a
  - few
  - more
  - confounding
  - factors
  - .
  - '>'
  - I
  - really
  - liked
  - the
  - idea
  - of
  - using
  - an
  - oracle
  - that
  - gives
  - the
  - position
  - as
  - upper
  - bound
  - .
  - Would
  - it
  - make
  - sense
  - to
  - also
  - have
  - the
  - opposite
  - oracle
  - that
  - gives
  - the
  - edit
  - symbol
  - ','
  - but
  - does
  - n't
  - tell
  - the
  - location
  - '?'
  - I
  - '''m'
  - really
  - curious
  - which
  - is
  - the
  - '``'
  - harder
  - ''''''
  - task
  - ','
  - predicting
  - the
  - next
  - symbol
  - or
  - the
  - next
  - location
  - .
  - Thanks
  - for
  - the
  - suggestion
  - .
  - It
  - '''s'
  - not
  - '100'
  - '%'
  - clear
  - to
  - us
  - what
  - an
  - oracle
  - that
  - '``'
  - pattern-matched
  - ''''''
  - 'on'
  - position
  - given
  - edit
  - symbol
  - would
  - look
  - like
  - ','
  - but
  - we
  - agree
  - it
  - '''s'
  - interesting
  - to
  - think
  - about
  - .
  - To
  - answer
  - the
  - larger
  - question
  - of
  - what
  - the
  - hardest
  - part
  - of
  - the
  - prediction
  - is
  - ','
  - we
  - include
  - some
  - additional
  - plots
  - in
  - Appendix
  - B.
  - '3'
  - of
  - an
  - updated
  - version
  - .
  - The
  - plots
  - show
  - that
  - the
  - most
  - difficult
  - part
  - is
  - predicting
  - the
  - jumps
  - when
  - one
  - pattern
  - is
  - completed
  - and
  - then
  - the
  - model
  - must
  - decide
  - where
  - to
  - edit
  - next
  - .
  - '>'
  - The
  - explicit
  - baseline
  - model
  - performs
  - very
  - well
  - 'on'
  - all
  - the
  - edit
  - types
  - in
  - Table
  - '1'
  - .
  - Are
  - there
  - cases
  - where
  - even
  - this
  - explicit
  - case
  - works
  - poorly
  - '?'
  - Is
  - the
  - improved
  - implicit
  - model
  - '*'
  - always
  - '*'
  - upper
  - bounded
  - by
  - the
  - explicit
  - model
  - -LRB-
  - to
  - me
  - it
  - seems
  - like
  - the
  - answer
  - should
  - always
  - be
  - 'yes'
  - ','
  - but
  - it
  - would
  - be
  - interesting
  - to
  - check
  - it
  - out
  - for
  - cases
  - where
  - explicit
  - is
  - not
  - very
  - high
  - accuracy
  - -RRB-
  - .
  - The
  - implicit
  - model
  - gets
  - slightly
  - better
  - results
  - 'on'
  - the
  - '``'
  - MultiTask
  - ''''''
  - problem
  - in
  - Table
  - '1'
  - ','
  - so
  - the
  - explicit
  - model
  - is
  - not
  - '*'
  - always
  - '*'
  - more
  - accurate
  - .
  - However
  - ','
  - we
  - did
  - find
  - the
  - explicit
  - model
  - to
  - produce
  - good
  - accuracy
  - across
  - the
  - board
  - .
  - The
  - main
  - issue
  - with
  - it
  - is
  - the
  - memory
  - and
  - computational
  - cost
  - .
- comment_id: B1ef1_Gg57
  rels:
  - !!python/tuple
    - 0
    - 16
    - 16
    - 32
    - same_unit
  - !!python/tuple
    - 0
    - 5
    - 5
    - 16
    - elaboration
  - !!python/tuple
    - 16
    - 32
    - 0
    - 16
    - same_unit
  - !!python/tuple
    - 16
    - 29
    - 29
    - 32
    - purpose
  - !!python/tuple
    - 0
    - 32
    - 32
    - 39
    - circumstance
  - !!python/tuple
    - 0
    - 39
    - 39
    - 54
    - elaboration
  - !!python/tuple
    - 0
    - 54
    - 54
    - 543
    - elaboration
  - !!python/tuple
    - 54
    - 319
    - 319
    - 543
    - topic
  - !!python/tuple
    - 63
    - 68
    - 54
    - 63
    - attribution
  - !!python/tuple
    - 64
    - 68
    - 63
    - 64
    - attribution
  - !!python/tuple
    - 54
    - 68
    - 68
    - 319
    - elaboration
  - !!python/tuple
    - 68
    - 72
    - 72
    - 82
    - purpose
  - !!python/tuple
    - 68
    - 82
    - 82
    - 319
    - elaboration
  - !!python/tuple
    - 82
    - 94
    - 94
    - 319
    - list
  - !!python/tuple
    - 94
    - 319
    - 82
    - 94
    - list
  - !!python/tuple
    - 94
    - 125
    - 125
    - 319
    - list
  - !!python/tuple
    - 94
    - 119
    - 119
    - 125
    - elaboration
  - !!python/tuple
    - 125
    - 319
    - 94
    - 125
    - list
  - !!python/tuple
    - 125
    - 146
    - 146
    - 319
    - list
  - !!python/tuple
    - 125
    - 132
    - 132
    - 146
    - same_unit
  - !!python/tuple
    - 125
    - 128
    - 128
    - 132
    - elaboration
  - !!python/tuple
    - 132
    - 146
    - 125
    - 132
    - same_unit
  - !!python/tuple
    - 132
    - 140
    - 140
    - 146
    - elaboration
  - !!python/tuple
    - 146
    - 319
    - 125
    - 146
    - list
  - !!python/tuple
    - 146
    - 154
    - 154
    - 319
    - list
  - !!python/tuple
    - 154
    - 319
    - 146
    - 154
    - list
  - !!python/tuple
    - 154
    - 155
    - 155
    - 159
    - elaboration
  - !!python/tuple
    - 154
    - 159
    - 159
    - 165
    - elaboration
  - !!python/tuple
    - 154
    - 165
    - 165
    - 215
    - elaboration
  - !!python/tuple
    - 165
    - 169
    - 169
    - 215
    - purpose
  - !!python/tuple
    - 169
    - 177
    - 177
    - 215
    - same_unit
  - !!python/tuple
    - 169
    - 174
    - 174
    - 177
    - elaboration
  - !!python/tuple
    - 177
    - 215
    - 169
    - 177
    - same_unit
  - !!python/tuple
    - 177
    - 178
    - 178
    - 198
    - elaboration
  - !!python/tuple
    - 177
    - 198
    - 198
    - 215
    - elaboration
  - !!python/tuple
    - 198
    - 210
    - 210
    - 215
    - elaboration
  - !!python/tuple
    - 154
    - 215
    - 215
    - 319
    - elaboration
  - !!python/tuple
    - 220
    - 242
    - 215
    - 220
    - attribution
  - !!python/tuple
    - 220
    - 234
    - 234
    - 242
    - elaboration
  - !!python/tuple
    - 215
    - 242
    - 242
    - 319
    - elaboration
  - !!python/tuple
    - 242
    - 251
    - 251
    - 258
    - elaboration
  - !!python/tuple
    - 242
    - 258
    - 258
    - 265
    - elaboration
  - !!python/tuple
    - 258
    - 261
    - 261
    - 265
    - same_unit
  - !!python/tuple
    - 261
    - 265
    - 258
    - 261
    - same_unit
  - !!python/tuple
    - 242
    - 265
    - 265
    - 319
    - elaboration
  - !!python/tuple
    - 265
    - 274
    - 274
    - 276
    - elaboration
  - !!python/tuple
    - 265
    - 276
    - 276
    - 319
    - elaboration
  - !!python/tuple
    - 276
    - 279
    - 279
    - 293
    - list
  - !!python/tuple
    - 279
    - 293
    - 276
    - 279
    - list
  - !!python/tuple
    - 279
    - 292
    - 292
    - 293
    - same_unit
  - !!python/tuple
    - 279
    - 286
    - 286
    - 292
    - elaboration
  - !!python/tuple
    - 292
    - 293
    - 279
    - 292
    - same_unit
  - !!python/tuple
    - 276
    - 293
    - 293
    - 319
    - elaboration
  - !!python/tuple
    - 293
    - 301
    - 301
    - 319
    - temporal
  - !!python/tuple
    - 301
    - 311
    - 311
    - 319
    - elaboration
  - !!python/tuple
    - 319
    - 543
    - 54
    - 319
    - topic
  - !!python/tuple
    - 319
    - 338
    - 338
    - 345
    - elaboration
  - !!python/tuple
    - 319
    - 345
    - 345
    - 484
    - elaboration
  - !!python/tuple
    - 345
    - 359
    - 359
    - 372
    - elaboration
  - !!python/tuple
    - 359
    - 364
    - 364
    - 372
    - elaboration
  - !!python/tuple
    - 345
    - 372
    - 372
    - 387
    - elaboration
  - !!python/tuple
    - 372
    - 373
    - 373
    - 387
    - list
  - !!python/tuple
    - 373
    - 387
    - 372
    - 373
    - list
  - !!python/tuple
    - 373
    - 378
    - 378
    - 387
    - elaboration
  - !!python/tuple
    - 345
    - 387
    - 387
    - 396
    - elaboration
  - !!python/tuple
    - 392
    - 396
    - 387
    - 392
    - attribution
  - !!python/tuple
    - 345
    - 396
    - 396
    - 484
    - elaboration
  - !!python/tuple
    - 396
    - 398
    - 398
    - 417
    - purpose
  - !!python/tuple
    - 401
    - 417
    - 398
    - 401
    - attribution
  - !!python/tuple
    - 401
    - 409
    - 409
    - 417
    - elaboration
  - !!python/tuple
    - 409
    - 413
    - 413
    - 417
    - elaboration
  - !!python/tuple
    - 396
    - 417
    - 417
    - 484
    - elaboration
  - !!python/tuple
    - 417
    - 441
    - 441
    - 446
    - elaboration
  - !!python/tuple
    - 417
    - 446
    - 446
    - 484
    - elaboration
  - !!python/tuple
    - 446
    - 450
    - 450
    - 455
    - elaboration
  - !!python/tuple
    - 446
    - 455
    - 455
    - 484
    - elaboration
  - !!python/tuple
    - 455
    - 468
    - 468
    - 484
    - same_unit
  - !!python/tuple
    - 455
    - 457
    - 457
    - 468
    - elaboration
  - !!python/tuple
    - 468
    - 484
    - 455
    - 468
    - same_unit
  - !!python/tuple
    - 468
    - 471
    - 471
    - 484
    - elaboration
  - !!python/tuple
    - 319
    - 484
    - 484
    - 543
    - means
  - !!python/tuple
    - 484
    - 499
    - 499
    - 511
    - elaboration
  - !!python/tuple
    - 499
    - 506
    - 506
    - 511
    - elaboration
  - !!python/tuple
    - 484
    - 511
    - 511
    - 512
    - elaboration
  - !!python/tuple
    - 484
    - 512
    - 512
    - 525
    - elaboration
  - !!python/tuple
    - 512
    - 516
    - 516
    - 525
    - list
  - !!python/tuple
    - 516
    - 525
    - 512
    - 516
    - list
  - !!python/tuple
    - 484
    - 525
    - 525
    - 543
    - elaboration
  - !!python/tuple
    - 525
    - 536
    - 536
    - 543
    - elaboration
  - !!python/tuple
    - 538
    - 543
    - 536
    - 538
    - attribution
  tokens:
  - '1'
  - -RRB-
  - Implicit
  - reparameterization
  - gradients
  - -LRB-
  - Jankowiak
  - '&'
  - Obermeyer
  - '2018'
  - ','
  - Figurnov
  - et
  - al.
  - '2018'
  - -RRB-
  - already
  - show
  - improvements
  - over
  - GRep
  - and
  - RSVI
  - ','
  - so
  - it
  - would
  - seem
  - natural
  - to
  - use
  - them
  - as
  - the
  - baseline
  - for
  - Sec
  - '7.1'
  - .
  - In
  - this
  - setting
  - ','
  - what
  - is
  - the
  - relationship
  - between
  - GO
  - and
  - Implicit
  - Reparameterization
  - Gradients
  - .
  - '2'
  - -RRB-
  - In
  - Sec
  - '7.2'
  - ','
  - GO
  - gradients
  - require
  - evaluating
  - f
  - many
  - times
  - .
  - It
  - would
  - seem
  - natural
  - to
  - compare
  - to
  - Local
  - Expectation
  - gradients
  - in
  - this
  - case
  - .
  - What
  - is
  - the
  - relationship
  - between
  - GO
  - and
  - LEgrad
  - in
  - this
  - case
  - '?'
  - '3'
  - -RRB-
  - For
  - the
  - discrete
  - case
  - ','
  - because
  - we
  - are
  - making
  - many
  - calls
  - to
  - f
  - ','
  - would
  - it
  - make
  - sense
  - to
  - compare
  - to
  - multisample
  - techniques
  - -LRB-
  - e.g.
  - ','
  - VIMCO
  - -RRB-
  - '?'
  - '4'
  - -RRB-
  - ARM
  - -LRB-
  - Yin
  - '2018'
  - -RRB-
  - is
  - a
  - recent
  - technique
  - for
  - discrete
  - random
  - variables
  - that
  - uses
  - multiple
  - function
  - evals
  - .
  - What
  - is
  - the
  - relation
  - with
  - GO
  - gradients
  - '?'
  - Thanks
  - for
  - your
  - interest
  - .
  - Your
  - comments
  - are
  - addressed
  - below
  - .
  - -LRB-
  - '1'
  - -RRB-
  - Similar
  - to
  - GO
  - ','
  - Implicit
  - Reparameterization
  - -LRB-
  - ImplicitRep
  - -RRB-
  - Gradients
  - -LRB-
  - Jankowiak
  - '&'
  - Obermeyer
  - '2018'
  - ','
  - Figurnov
  - et
  - al.
  - '2018'
  - -RRB-
  - tried
  - to
  - exploit
  - the
  - gradient
  - information
  - of
  - function
  - f
  - -LRB-
  - z
  - -RRB-
  - for
  - lower
  - Monte
  - Carlo
  - variance
  - ','
  - via
  - a
  - technique
  - they
  - termed
  - implicit
  - differentiation
  - .
  - Although
  - seeming
  - different
  - ','
  - ImplicitRep
  - is
  - more
  - or
  - less
  - a
  - special
  - case
  - of
  - GO
  - in
  - the
  - single-layer
  - continuous
  - situation
  - -LRB-
  - thus
  - 'no'
  - need
  - for
  - comparison
  - -RRB-
  - .
  - One
  - can
  - reveal
  - this
  - by
  - comparing
  - their
  - Eq
  - .
  - -LRB-
  - '5'
  - -RRB-
  - with
  - our
  - Eq
  - .
  - -LRB-
  - '9'
  - -RRB-
  - in
  - Theorem
  - '1'
  - .
  - The
  - difference
  - is
  - that
  - GO
  - generalizes
  - to
  - discrete
  - situations
  - -LRB-
  - Theorem
  - '1'
  - -RRB-
  - ','
  - and
  - also
  - to
  - deep
  - probabilistic
  - graphical
  - models
  - -LRB-
  - Theorem
  - '2'
  - and
  - '3'
  - -RRB-
  - .
  - -LRB-
  - '2'
  - -RRB-
  - As
  - stated
  - in
  - the
  - paragraph
  - before
  - Section
  - '4'
  - ','
  - we
  - adopt
  - the
  - local
  - expectation
  - idea
  - when
  - it
  - is
  - applicable
  - and
  - computationally
  - acceptable
  - .
  - In
  - some
  - specific
  - cases
  - ','
  - like
  - discrete
  - random
  - variables
  - with
  - finite
  - support
  - ','
  - fully
  - applying
  - the
  - local
  - expectation
  - idea
  - will
  - reduce
  - GO
  - to
  - the
  - LEgrad
  - .
  - However
  - ','
  - GO
  - has
  - the
  - advantages
  - that
  - it
  - is
  - applicable
  - to
  - discrete
  - situations
  - with
  - -LRB-
  - '1'
  - -RRB-
  - infinite
  - support
  - -LRB-
  - where
  - LEgrad
  - may
  - not
  - be
  - applicable
  - -RRB-
  - ;
  - -LRB-
  - '2'
  - -RRB-
  - finite
  - support
  - -LRB-
  - where
  - LEgrad
  - may
  - be
  - computationally
  - expensive
  - -RRB-
  - .
  - -LRB-
  - '3'
  - -RRB-
  - Thank
  - you
  - for
  - your
  - suggestions
  - .
  - We
  - plan
  - to
  - fully
  - exploit
  - -LRB-
  - and
  - potentially
  - improve
  - -RRB-
  - GO
  - under
  - various
  - -LRB-
  - discrete
  - -RRB-
  - cases
  - in
  - the
  - future
  - .
  - However
  - ','
  - we
  - consider
  - it
  - beyond
  - the
  - scope
  - of
  - this
  - conference
  - paper
  - ','
  - which
  - is
  - meant
  - for
  - presenting
  - the
  - derivation
  - of
  - a
  - unified
  - gradient
  - that
  - is
  - widely
  - applicable
  - .
  - -LRB-
  - '4'
  - -RRB-
  - ARM
  - -LRB-
  - Yin
  - '2018'
  - -RRB-
  - ','
  - using
  - techniques
  - -LRB-
  - including
  - data
  - augmentation
  - ','
  - permutation
  - ','
  - and
  - variance
  - reduction
  - -RRB-
  - to
  - aid
  - REINFORCE
  - for
  - gradient
  - calculation
  - ','
  - is
  - applicable
  - to
  - discrete
  - situations
  - with
  - finite
  - support
  - .
  - By
  - comparison
  - ','
  - GO
  - ','
  - motivated
  - by
  - the
  - connection
  - of
  - REINFORCE
  - and
  - Rep
  - ','
  - is
  - -LRB-
  - '1'
  - -RRB-
  - a
  - widely
  - applicable
  - gradient
  - -LRB-
  - continuous
  - or
  - discrete
  - -RRB-
  - ;
  - -LRB-
  - '2'
  - -RRB-
  - can
  - be
  - applied
  - to
  - discrete
  - situations
  - with
  - infinite
  - support
  - .
  - There
  - might
  - be
  - some
  - implicit
  - relations
  - between
  - ARM
  - and
  - GO
  - .
  - We
  - leave
  - that
  - as
  - future
  - work
  - .
- comment_id: B1eqVtIKRm
  rels:
  - !!python/tuple
    - 0
    - 9
    - 9
    - 1691
    - same_unit
  - !!python/tuple
    - 0
    - 6
    - 6
    - 9
    - elaboration
  - !!python/tuple
    - 9
    - 1691
    - 0
    - 9
    - same_unit
  - !!python/tuple
    - 11
    - 25
    - 9
    - 11
    - attribution
  - !!python/tuple
    - 11
    - 15
    - 15
    - 25
    - elaboration
  - !!python/tuple
    - 9
    - 25
    - 25
    - 1691
    - elaboration
  - !!python/tuple
    - 25
    - 42
    - 42
    - 61
    - elaboration
  - !!python/tuple
    - 25
    - 61
    - 61
    - 1691
    - elaboration
  - !!python/tuple
    - 61
    - 81
    - 81
    - 1691
    - elaboration
  - !!python/tuple
    - 81
    - 95
    - 95
    - 108
    - elaboration
  - !!python/tuple
    - 95
    - 100
    - 100
    - 108
    - elaboration
  - !!python/tuple
    - 81
    - 108
    - 108
    - 1691
    - elaboration
  - !!python/tuple
    - 108
    - 116
    - 116
    - 137
    - purpose
  - !!python/tuple
    - 116
    - 124
    - 124
    - 129
    - elaboration
  - !!python/tuple
    - 116
    - 129
    - 129
    - 137
    - elaboration
  - !!python/tuple
    - 108
    - 137
    - 137
    - 1691
    - elaboration
  - !!python/tuple
    - 137
    - 155
    - 155
    - 161
    - elaboration
  - !!python/tuple
    - 137
    - 161
    - 161
    - 1691
    - elaboration
  - !!python/tuple
    - 161
    - 175
    - 175
    - 1691
    - elaboration
  - !!python/tuple
    - 175
    - 209
    - 209
    - 1691
    - list
  - !!python/tuple
    - 175
    - 182
    - 182
    - 209
    - list
  - !!python/tuple
    - 182
    - 209
    - 175
    - 182
    - list
  - !!python/tuple
    - 182
    - 192
    - 192
    - 209
    - list
  - !!python/tuple
    - 192
    - 209
    - 182
    - 192
    - list
  - !!python/tuple
    - 209
    - 1691
    - 175
    - 209
    - list
  - !!python/tuple
    - 209
    - 220
    - 220
    - 232
    - contrast
  - !!python/tuple
    - 209
    - 213
    - 213
    - 220
    - elaboration
  - !!python/tuple
    - 220
    - 232
    - 209
    - 220
    - contrast
  - !!python/tuple
    - 209
    - 232
    - 232
    - 1691
    - elaboration
  - !!python/tuple
    - 232
    - 270
    - 270
    - 1691
    - list
  - !!python/tuple
    - 232
    - 247
    - 247
    - 270
    - elaboration
  - !!python/tuple
    - 247
    - 252
    - 252
    - 270
    - elaboration
  - !!python/tuple
    - 261
    - 270
    - 252
    - 261
    - attribution
  - !!python/tuple
    - 270
    - 1691
    - 232
    - 270
    - list
  - !!python/tuple
    - 270
    - 278
    - 278
    - 284
    - elaboration
  - !!python/tuple
    - 270
    - 284
    - 284
    - 1691
    - elaboration
  - !!python/tuple
    - 284
    - 289
    - 289
    - 1691
    - textualorganization
  - !!python/tuple
    - 289
    - 1691
    - 284
    - 289
    - textualorganization
  - !!python/tuple
    - 289
    - 296
    - 296
    - 1691
    - textualorganization
  - !!python/tuple
    - 296
    - 1691
    - 289
    - 296
    - textualorganization
  - !!python/tuple
    - 296
    - 341
    - 341
    - 1691
    - topic
  - !!python/tuple
    - 296
    - 307
    - 307
    - 341
    - elaboration
  - !!python/tuple
    - 309
    - 341
    - 307
    - 309
    - attribution
  - !!python/tuple
    - 309
    - 312
    - 312
    - 341
    - elaboration
  - !!python/tuple
    - 312
    - 328
    - 328
    - 341
    - same_unit
  - !!python/tuple
    - 312
    - 318
    - 318
    - 328
    - elaboration
  - !!python/tuple
    - 328
    - 341
    - 312
    - 328
    - same_unit
  - !!python/tuple
    - 328
    - 337
    - 337
    - 341
    - elaboration
  - !!python/tuple
    - 341
    - 1691
    - 296
    - 341
    - topic
  - !!python/tuple
    - 341
    - 351
    - 351
    - 356
    - elaboration
  - !!python/tuple
    - 341
    - 356
    - 356
    - 1691
    - elaboration
  - !!python/tuple
    - 356
    - 359
    - 359
    - 1691
    - textualorganization
  - !!python/tuple
    - 359
    - 1691
    - 356
    - 359
    - textualorganization
  - !!python/tuple
    - 359
    - 372
    - 372
    - 395
    - elaboration
  - !!python/tuple
    - 374
    - 386
    - 372
    - 374
    - attribution
  - !!python/tuple
    - 372
    - 386
    - 386
    - 395
    - concession
  - !!python/tuple
    - 359
    - 395
    - 395
    - 1691
    - elaboration
  - !!python/tuple
    - 400
    - 426
    - 395
    - 400
    - attribution
  - !!python/tuple
    - 395
    - 426
    - 426
    - 1691
    - elaboration
  - !!python/tuple
    - 426
    - 430
    - 430
    - 445
    - purpose
  - !!python/tuple
    - 426
    - 445
    - 445
    - 1691
    - condition
  - !!python/tuple
    - 445
    - 467
    - 467
    - 476
    - elaboration
  - !!python/tuple
    - 467
    - 473
    - 473
    - 476
    - purpose
  - !!python/tuple
    - 445
    - 476
    - 476
    - 1691
    - elaboration
  - !!python/tuple
    - 476
    - 489
    - 489
    - 506
    - elaboration
  - !!python/tuple
    - 476
    - 506
    - 506
    - 1691
    - elaboration
  - !!python/tuple
    - 506
    - 522
    - 522
    - 1691
    - topic
  - !!python/tuple
    - 510
    - 522
    - 506
    - 510
    - attribution
  - !!python/tuple
    - 510
    - 515
    - 515
    - 522
    - purpose
  - !!python/tuple
    - 522
    - 1691
    - 506
    - 522
    - topic
  - !!python/tuple
    - 525
    - 535
    - 522
    - 525
    - attribution
  - !!python/tuple
    - 525
    - 530
    - 530
    - 535
    - list
  - !!python/tuple
    - 530
    - 535
    - 525
    - 530
    - list
  - !!python/tuple
    - 522
    - 535
    - 535
    - 567
    - elaboration
  - !!python/tuple
    - 535
    - 539
    - 539
    - 543
    - elaboration
  - !!python/tuple
    - 535
    - 543
    - 543
    - 567
    - elaboration
  - !!python/tuple
    - 543
    - 548
    - 548
    - 567
    - elaboration
  - !!python/tuple
    - 548
    - 552
    - 552
    - 567
    - elaboration
  - !!python/tuple
    - 552
    - 557
    - 557
    - 567
    - circumstance
  - !!python/tuple
    - 522
    - 567
    - 567
    - 1691
    - elaboration
  - !!python/tuple
    - 567
    - 576
    - 576
    - 603
    - elaboration
  - !!python/tuple
    - 576
    - 580
    - 580
    - 603
    - elaboration
  - !!python/tuple
    - 593
    - 603
    - 580
    - 593
    - attribution
  - !!python/tuple
    - 593
    - 594
    - 594
    - 603
    - elaboration
  - !!python/tuple
    - 567
    - 603
    - 603
    - 1691
    - elaboration
  - !!python/tuple
    - 603
    - 645
    - 645
    - 1691
    - list
  - !!python/tuple
    - 603
    - 607
    - 607
    - 645
    - elaboration
  - !!python/tuple
    - 607
    - 620
    - 620
    - 645
    - elaboration
  - !!python/tuple
    - 635
    - 645
    - 620
    - 635
    - attribution
  - !!python/tuple
    - 645
    - 1691
    - 603
    - 645
    - list
  - !!python/tuple
    - 645
    - 677
    - 677
    - 1691
    - list
  - !!python/tuple
    - 677
    - 1691
    - 645
    - 677
    - list
  - !!python/tuple
    - 677
    - 692
    - 692
    - 1691
    - list
  - !!python/tuple
    - 677
    - 682
    - 682
    - 692
    - purpose
  - !!python/tuple
    - 692
    - 1691
    - 677
    - 692
    - list
  - !!python/tuple
    - 692
    - 696
    - 696
    - 1691
    - textualorganization
  - !!python/tuple
    - 696
    - 1691
    - 692
    - 696
    - textualorganization
  - !!python/tuple
    - 696
    - 717
    - 717
    - 1691
    - elaboration
  - !!python/tuple
    - 717
    - 723
    - 723
    - 755
    - elaboration
  - !!python/tuple
    - 723
    - 725
    - 725
    - 755
    - reason
  - !!python/tuple
    - 725
    - 730
    - 730
    - 755
    - elaboration
  - !!python/tuple
    - 730
    - 732
    - 732
    - 755
    - purpose
  - !!python/tuple
    - 734
    - 755
    - 732
    - 734
    - attribution
  - !!python/tuple
    - 734
    - 743
    - 743
    - 755
    - elaboration
  - !!python/tuple
    - 744
    - 755
    - 743
    - 744
    - attribution
  - !!python/tuple
    - 744
    - 749
    - 749
    - 755
    - contrast
  - !!python/tuple
    - 749
    - 755
    - 744
    - 749
    - contrast
  - !!python/tuple
    - 717
    - 755
    - 755
    - 1691
    - elaboration
  - !!python/tuple
    - 755
    - 795
    - 795
    - 1691
    - list
  - !!python/tuple
    - 755
    - 757
    - 757
    - 795
    - elaboration
  - !!python/tuple
    - 757
    - 782
    - 782
    - 795
    - elaboration
  - !!python/tuple
    - 795
    - 1691
    - 755
    - 795
    - list
  - !!python/tuple
    - 795
    - 799
    - 799
    - 821
    - purpose
  - !!python/tuple
    - 799
    - 809
    - 809
    - 821
    - elaboration
  - !!python/tuple
    - 795
    - 821
    - 821
    - 1691
    - elaboration
  - !!python/tuple
    - 821
    - 825
    - 825
    - 857
    - elaboration
  - !!python/tuple
    - 825
    - 829
    - 829
    - 857
    - elaboration
  - !!python/tuple
    - 829
    - 849
    - 849
    - 857
    - circumstance
  - !!python/tuple
    - 853
    - 857
    - 849
    - 853
    - attribution
  - !!python/tuple
    - 821
    - 857
    - 857
    - 1691
    - elaboration
  - !!python/tuple
    - 857
    - 863
    - 863
    - 908
    - circumstance
  - !!python/tuple
    - 863
    - 865
    - 865
    - 908
    - elaboration
  - !!python/tuple
    - 865
    - 870
    - 870
    - 908
    - elaboration
  - !!python/tuple
    - 857
    - 908
    - 908
    - 1691
    - elaboration
  - !!python/tuple
    - 908
    - 1075
    - 1075
    - 1691
    - list
  - !!python/tuple
    - 908
    - 921
    - 921
    - 940
    - example
  - !!python/tuple
    - 921
    - 931
    - 931
    - 940
    - purpose
  - !!python/tuple
    - 931
    - 934
    - 934
    - 940
    - list
  - !!python/tuple
    - 934
    - 940
    - 931
    - 934
    - list
  - !!python/tuple
    - 908
    - 940
    - 940
    - 1075
    - elaboration
  - !!python/tuple
    - 940
    - 959
    - 959
    - 1075
    - example
  - !!python/tuple
    - 959
    - 985
    - 985
    - 1075
    - elaboration
  - !!python/tuple
    - 997
    - 1007
    - 985
    - 997
    - attribution
  - !!python/tuple
    - 998
    - 1007
    - 997
    - 998
    - attribution
  - !!python/tuple
    - 985
    - 1007
    - 1007
    - 1075
    - elaboration
  - !!python/tuple
    - 1007
    - 1054
    - 1054
    - 1075
    - list
  - !!python/tuple
    - 1054
    - 1075
    - 1007
    - 1054
    - list
  - !!python/tuple
    - 1059
    - 1075
    - 1054
    - 1059
    - attribution
  - !!python/tuple
    - 1059
    - 1065
    - 1065
    - 1075
    - elaboration
  - !!python/tuple
    - 1075
    - 1691
    - 908
    - 1075
    - list
  - !!python/tuple
    - 1081
    - 1095
    - 1075
    - 1081
    - antithesis
  - !!python/tuple
    - 1075
    - 1095
    - 1095
    - 1691
    - elaboration
  - !!python/tuple
    - 1095
    - 1104
    - 1104
    - 1691
    - elaboration
  - !!python/tuple
    - 1127
    - 1691
    - 1104
    - 1127
    - antithesis
  - !!python/tuple
    - 1104
    - 1118
    - 1118
    - 1127
    - elaboration
  - !!python/tuple
    - 1127
    - 1132
    - 1132
    - 1162
    - purpose
  - !!python/tuple
    - 1134
    - 1162
    - 1132
    - 1134
    - attribution
  - !!python/tuple
    - 1134
    - 1136
    - 1136
    - 1162
    - elaboration
  - !!python/tuple
    - 1136
    - 1144
    - 1144
    - 1162
    - elaboration
  - !!python/tuple
    - 1144
    - 1151
    - 1151
    - 1162
    - means
  - !!python/tuple
    - 1151
    - 1155
    - 1155
    - 1162
    - list
  - !!python/tuple
    - 1155
    - 1162
    - 1151
    - 1155
    - list
  - !!python/tuple
    - 1127
    - 1162
    - 1162
    - 1691
    - elaboration
  - !!python/tuple
    - 1162
    - 1163
    - 1163
    - 1177
    - list
  - !!python/tuple
    - 1163
    - 1177
    - 1162
    - 1163
    - list
  - !!python/tuple
    - 1163
    - 1168
    - 1168
    - 1177
    - purpose
  - !!python/tuple
    - 1162
    - 1177
    - 1177
    - 1180
    - elaboration
  - !!python/tuple
    - 1162
    - 1180
    - 1180
    - 1691
    - elaboration
  - !!python/tuple
    - 1187
    - 1212
    - 1180
    - 1187
    - attribution
  - !!python/tuple
    - 1190
    - 1212
    - 1187
    - 1190
    - attribution
  - !!python/tuple
    - 1195
    - 1212
    - 1190
    - 1195
    - attribution
  - !!python/tuple
    - 1180
    - 1212
    - 1212
    - 1415
    - elaboration
  - !!python/tuple
    - 1212
    - 1260
    - 1260
    - 1280
    - elaboration
  - !!python/tuple
    - 1212
    - 1280
    - 1280
    - 1415
    - elaboration
  - !!python/tuple
    - 1282
    - 1323
    - 1280
    - 1282
    - attribution
  - !!python/tuple
    - 1282
    - 1283
    - 1283
    - 1298
    - elaboration
  - !!python/tuple
    - 1282
    - 1298
    - 1298
    - 1323
    - elaboration
  - !!python/tuple
    - 1280
    - 1323
    - 1323
    - 1415
    - elaboration
  - !!python/tuple
    - 1323
    - 1360
    - 1360
    - 1415
    - elaboration
  - !!python/tuple
    - 1180
    - 1415
    - 1415
    - 1691
    - elaboration
  - !!python/tuple
    - 1431
    - 1691
    - 1415
    - 1431
    - attribution
  - !!python/tuple
    - 1431
    - 1442
    - 1442
    - 1691
    - elaboration
  - !!python/tuple
    - 1442
    - 1464
    - 1464
    - 1691
    - list
  - !!python/tuple
    - 1464
    - 1691
    - 1442
    - 1464
    - list
  - !!python/tuple
    - 1464
    - 1470
    - 1470
    - 1691
    - elaboration
  - !!python/tuple
    - 1470
    - 1516
    - 1516
    - 1691
    - elaboration
  - !!python/tuple
    - 1516
    - 1531
    - 1531
    - 1691
    - explanation
  - !!python/tuple
    - 1531
    - 1543
    - 1543
    - 1691
    - list
  - !!python/tuple
    - 1531
    - 1538
    - 1538
    - 1543
    - elaboration
  - !!python/tuple
    - 1543
    - 1691
    - 1531
    - 1543
    - list
  - !!python/tuple
    - 1543
    - 1549
    - 1549
    - 1567
    - elaboration
  - !!python/tuple
    - 1549
    - 1551
    - 1551
    - 1567
    - elaboration
  - !!python/tuple
    - 1543
    - 1567
    - 1567
    - 1691
    - elaboration
  - !!python/tuple
    - 1567
    - 1571
    - 1571
    - 1691
    - textualorganization
  - !!python/tuple
    - 1571
    - 1691
    - 1567
    - 1571
    - textualorganization
  - !!python/tuple
    - 1571
    - 1580
    - 1580
    - 1691
    - elaboration
  - !!python/tuple
    - 1580
    - 1584
    - 1584
    - 1603
    - elaboration
  - !!python/tuple
    - 1580
    - 1603
    - 1603
    - 1691
    - elaboration
  - !!python/tuple
    - 1603
    - 1617
    - 1617
    - 1691
    - list
  - !!python/tuple
    - 1603
    - 1605
    - 1605
    - 1617
    - elaboration
  - !!python/tuple
    - 1617
    - 1691
    - 1603
    - 1617
    - list
  - !!python/tuple
    - 1617
    - 1622
    - 1622
    - 1691
    - elaboration
  - !!python/tuple
    - 1622
    - 1641
    - 1641
    - 1691
    - elaboration
  - !!python/tuple
    - 1641
    - 1646
    - 1646
    - 1650
    - purpose
  - !!python/tuple
    - 1641
    - 1650
    - 1650
    - 1655
    - elaboration
  - !!python/tuple
    - 1641
    - 1655
    - 1655
    - 1691
    - elaboration
  - !!python/tuple
    - 1655
    - 1660
    - 1660
    - 1675
    - elaboration
  - !!python/tuple
    - 1655
    - 1675
    - 1675
    - 1691
    - elaboration
  - !!python/tuple
    - 1675
    - 1681
    - 1681
    - 1691
    - elaboration
  tokens:
  - The
  - paper
  - presents
  - a
  - framework
  - ','
  - called
  - ChoiceNet
  - ','
  - for
  - learning
  - when
  - the
  - supervision
  - outputs
  - -LRB-
  - e.g.
  - ','
  - labels
  - -RRB-
  - are
  - corrupted
  - by
  - noise
  - .
  - The
  - method
  - relies
  - 'on'
  - estimating
  - the
  - correlation
  - between
  - the
  - training
  - data
  - distribution
  - and
  - a
  - target
  - distribution
  - ','
  - where
  - training
  - data
  - distribution
  - is
  - assumed
  - to
  - be
  - a
  - mixture
  - of
  - that
  - target
  - distribution
  - and
  - other
  - unknown
  - distributions
  - .
  - The
  - paper
  - also
  - presents
  - some
  - compelling
  - results
  - 'on'
  - synthetic
  - and
  - real
  - datasets
  - ','
  - for
  - both
  - regression
  - and
  - classification
  - problems
  - .
  - The
  - proposed
  - idea
  - builds
  - 'on'
  - top
  - of
  - previously
  - published
  - work
  - 'on'
  - Mixture
  - Density
  - Networks
  - -LRB-
  - MDNs
  - -RRB-
  - and
  - Mixup
  - -LRB-
  - Zhang
  - et
  - al
  - ','
  - '2017'
  - -RRB-
  - .
  - The
  - main
  - difference
  - is
  - the
  - MDN
  - are
  - modified
  - to
  - construct
  - the
  - Mixture
  - of
  - Correlated
  - Density
  - Network
  - -LRB-
  - MCDN
  - -RRB-
  - block
  - ','
  - that
  - forms
  - the
  - main
  - component
  - of
  - ChoiceNets
  - .
  - I
  - like
  - the
  - overall
  - direction
  - and
  - idea
  - of
  - modelling
  - correlation
  - between
  - the
  - target
  - distribution
  - and
  - the
  - data
  - distribution
  - to
  - deal
  - with
  - noisy
  - labels
  - .
  - The
  - results
  - are
  - also
  - compelling
  - and
  - I
  - thus
  - lean
  - towards
  - accepting
  - this
  - paper
  - .
  - My
  - decision
  - 'on'
  - '``'
  - marginal
  - accept
  - ''''''
  - is
  - based
  - primarily
  - 'on'
  - my
  - unfamiliarity
  - with
  - this
  - specific
  - area
  - and
  - that
  - some
  - parts
  - of
  - the
  - paper
  - are
  - not
  - very
  - easy
  - or
  - intuitive
  - to
  - read
  - through
  - .
  - ==
  - Related
  - Work
  - ==
  - I
  - like
  - the
  - related
  - work
  - discussion
  - ','
  - but
  - would
  - emphasize
  - more
  - the
  - connection
  - to
  - MDNs
  - and
  - to
  - Mixup
  - .
  - Only
  - one
  - sentence
  - is
  - mentioned
  - about
  - Mixup
  - but
  - reading
  - through
  - the
  - abstract
  - and
  - the
  - introduction
  - that
  - is
  - the
  - first
  - paper
  - that
  - came
  - to
  - my
  - mind
  - and
  - thus
  - I
  - believe
  - that
  - it
  - may
  - deserve
  - a
  - bit
  - more
  - discussion
  - .
  - Also
  - ','
  - there
  - are
  - a
  - couple
  - more
  - papers
  - that
  - felt
  - relevant
  - to
  - this
  - work
  - but
  - are
  - not
  - mentioned
  - ':'
  - '-'
  - Estimating
  - Accuracy
  - from
  - Unlabeled
  - Data
  - ':'
  - A
  - Bayesian
  - Approach
  - ','
  - Platanios
  - et
  - al.
  - ','
  - ICML
  - '2016'
  - .
  - I
  - believe
  - this
  - is
  - related
  - in
  - how
  - noisy
  - labels
  - are
  - modeled
  - -LRB-
  - i.e.
  - ','
  - section
  - '3'
  - in
  - the
  - reviewed
  - paper
  - -RRB-
  - and
  - in
  - the
  - idea
  - of
  - correlation/consistency
  - as
  - a
  - means
  - to
  - detect
  - errors
  - .
  - There
  - are
  - couple
  - more
  - papers
  - in
  - this
  - line
  - of
  - work
  - that
  - may
  - be
  - relevant
  - .
  - '-'
  - ADIOS
  - ':'
  - Architectures
  - Deep
  - In
  - Output
  - Space
  - ','
  - Al-Shedivat
  - et
  - al.
  - ','
  - ICML
  - '2016'
  - .
  - I
  - believe
  - this
  - is
  - related
  - in
  - learning
  - some
  - structure
  - in
  - the
  - output
  - space
  - ','
  - even
  - though
  - not
  - directly
  - dealing
  - with
  - noisy
  - labels
  - .
  - ==
  - Method
  - ==
  - I
  - believe
  - the
  - methods
  - section
  - could
  - have
  - been
  - written
  - in
  - a
  - more
  - clear/easy-to-follow
  - way
  - ','
  - but
  - this
  - may
  - also
  - be
  - due
  - to
  - my
  - unfamiliarity
  - with
  - this
  - area
  - .
  - Figure
  - '1'
  - is
  - hard
  - to
  - parse
  - and
  - does
  - not
  - really
  - offer
  - much
  - more
  - than
  - section
  - '3.2'
  - currently
  - does
  - .
  - If
  - the
  - figure
  - is
  - improved
  - with
  - some
  - more
  - text/labels
  - 'on'
  - boxes
  - rather
  - than
  - plain
  - equations
  - ','
  - it
  - may
  - go
  - a
  - long
  - way
  - in
  - making
  - the
  - methods
  - section
  - easier
  - to
  - follow
  - .
  - I
  - would
  - also
  - point
  - out
  - MCDN
  - as
  - the
  - key
  - contribution
  - of
  - this
  - paper
  - as
  - ChoiceNet
  - is
  - just
  - any
  - base
  - network
  - with
  - an
  - MCDN
  - block
  - stacked
  - 'on'
  - top
  - of
  - this
  - .
  - Thus
  - ','
  - I
  - believe
  - this
  - should
  - be
  - emphasized
  - more
  - to
  - make
  - your
  - key
  - contribution
  - clear
  - .
  - ==
  - Experiments
  - ==
  - The
  - experiments
  - are
  - nicely
  - presented
  - and
  - are
  - quite
  - thorough
  - .
  - A
  - couple
  - minor
  - comments
  - I
  - have
  - are
  - ':'
  - '-'
  - It
  - would
  - be
  - nice
  - to
  - run
  - regression
  - experiments
  - for
  - bigger
  - real-world
  - datasets
  - ','
  - as
  - the
  - ones
  - used
  - seem
  - to
  - be
  - quite
  - small
  - .
  - '-'
  - I
  - am
  - a
  - bit
  - confused
  - at
  - the
  - fact
  - that
  - in
  - table
  - '3'
  - you
  - compare
  - your
  - method
  - to
  - mixup
  - and
  - in
  - table
  - '4'
  - you
  - also
  - show
  - results
  - when
  - using
  - both
  - your
  - method
  - and
  - mixup
  - combined
  - .
  - Up
  - until
  - that
  - point
  - I
  - thought
  - that
  - mixup
  - was
  - posed
  - as
  - an
  - alternative
  - method
  - ','
  - but
  - here
  - it
  - seems
  - it
  - '''s'
  - quite
  - orthogonal
  - and
  - can
  - be
  - used
  - together
  - ','
  - which
  - I
  - think
  - makes
  - sense
  - ','
  - but
  - would
  - be
  - good
  - to
  - clarify
  - .
  - Also
  - ','
  - given
  - that
  - you
  - show
  - combined
  - results
  - in
  - table
  - '4'
  - ','
  - why
  - not
  - also
  - perform
  - exactly
  - the
  - same
  - analysis
  - for
  - table
  - '3'
  - and
  - also
  - show
  - numbers
  - for
  - CN
  - +
  - Mixup
  - '?'
  - It
  - would
  - also
  - be
  - nice
  - to
  - use
  - the
  - same
  - naming
  - scheme
  - for
  - both
  - tables
  - .
  - I
  - would
  - use
  - ':'
  - ConvNet
  - ','
  - ConvNet
  - +
  - CN
  - ','
  - ConvNet
  - +
  - CN
  - +
  - Mixup
  - ','
  - and
  - the
  - same
  - with
  - WRN
  - for
  - table
  - '4'
  - .
  - This
  - would
  - make
  - the
  - tables
  - easier
  - to
  - read
  - because
  - currently
  - the
  - first
  - thing
  - that
  - comes
  - to
  - mind
  - is
  - what
  - may
  - be
  - different
  - between
  - the
  - two
  - setups
  - given
  - that
  - they
  - are
  - presented
  - side-by-side
  - but
  - use
  - different
  - naming
  - conventions
  - .
  - One
  - question
  - that
  - comes
  - to
  - mind
  - is
  - that
  - you
  - make
  - certain
  - assumptions
  - 'on'
  - the
  - kinds
  - of
  - noise
  - your
  - model
  - can
  - capture
  - ','
  - so
  - are
  - there
  - any
  - cases
  - where
  - you
  - have
  - good
  - intuition
  - as
  - to
  - why
  - your
  - model
  - may
  - fail
  - '?'
  - It
  - would
  - be
  - good
  - to
  - present
  - a
  - short
  - discussion
  - 'on'
  - this
  - to
  - help
  - readers
  - understand
  - whether
  - they
  - can
  - benefit
  - by
  - using
  - your
  - model
  - or
  - not
  - .
  - We
  - conducted
  - additional
  - experiments
  - based
  - 'on'
  - other
  - reviews
  - where
  - we
  - observe
  - that
  - the
  - proposed
  - method
  - show
  - superior
  - performance
  - to
  - symmetric
  - noises
  - but
  - vulnerable
  - to
  - asymmetric
  - noise
  - 'on'
  - CIFAR-10
  - following
  - the
  - settings
  - in
  - -LSB-
  - '3'
  - -RSB-
  - .
  - We
  - implement
  - the
  - 9-layer
  - CNN
  - architecture
  - following
  - VAT
  - -LSB-
  - '5'
  - -RSB-
  - and
  - Co-teaching
  - -LSB-
  - '3'
  - -RSB-
  - to
  - fairly
  - evaluate
  - the
  - performance
  - of
  - CIFAR10
  - experiments
  - with
  - both
  - symmetric
  - and
  - asymmetric
  - noise
  - settings
  - ':'
  - Pair-45
  - '%'
  - ','
  - Symmetry-50
  - '%'
  - ','
  - and
  - Symmetry-20
  - '%'
  - ','
  - using
  - the
  - authors
  - ''''
  - implementations
  - available
  - 'on'
  - github
  - .
  - Pair-45
  - '%'
  - flips
  - '45'
  - '%'
  - of
  - each
  - label
  - to
  - the
  - next
  - label
  - .
  - For
  - example
  - ','
  - randomly
  - flipping
  - '45'
  - '%'
  - of
  - label
  - '1'
  - to
  - label
  - '2'
  - and
  - label
  - '2'
  - to
  - label3
  - .
  - 'On'
  - the
  - other
  - hand
  - ','
  - Symmetriy-50
  - '%'
  - randomly
  - assigns
  - '50'
  - '%'
  - of
  - each
  - label
  - to
  - other
  - labels
  - uniformly
  - .
  - For
  - example
  - ','
  - Symmetriy-50
  - '%'
  - randomly
  - flips
  - '50'
  - '%'
  - the
  - labels
  - of
  - instances
  - whose
  - original
  - label
  - is
  - '1'
  - to
  - a
  - random
  - label
  - sampled
  - from
  - 2-10
  - .
  - We
  - set
  - other
  - configurations
  - such
  - as
  - the
  - network
  - topology
  - and
  - an
  - activation
  - functions
  - to
  - be
  - the
  - same
  - as
  - -LSB-
  - '3'
  - -RSB-
  - .
  - -LRB-
  - Single-run
  - ','
  - last
  - validation
  - accuracy
  - -RRB-
  - Pair-45
  - '%'
  - sym-50
  - '%'
  - sym-20
  - '%'
  - '------------------------------------------'
  - ChoiceNet
  - '70.3'
  - '%'
  - '85.2'
  - '%'
  - '91.0'
  - '%'
  - '------------------------------------------'
  - MentorNet
  - '58.14'
  - '%'
  - '71.10'
  - '%'
  - '80.76'
  - '%'
  - Co-teaching
  - '72.62'
  - '%'
  - '74.02'
  - '%'
  - '82.32'
  - '%'
  - F-correction
  - '6.61'
  - '%'
  - '59.83'
  - '%'
  - '84.55'
  - '%'
  - The
  - results
  - of
  - MentorNet
  - -LSB-
  - '6'
  - -RSB-
  - ','
  - Co-teaching
  - -LSB-
  - '3'
  - -RSB-
  - ','
  - and
  - F-correction
  - -LSB-
  - '7'
  - -RSB-
  - are
  - copied
  - from
  - -LSB-
  - '3'
  - -RSB-
  - .
  - While
  - our
  - proposed
  - method
  - outperforms
  - all
  - compared
  - methods
  - 'on'
  - symmetric
  - noise
  - settings
  - ','
  - it
  - shows
  - inferior
  - performances
  - to
  - Co-teaching
  - .
  - This
  - shows
  - the
  - weakness
  - of
  - the
  - proposed
  - method
  - .
  - In
  - other
  - words
  - ','
  - our
  - mixture
  - distribution
  - failed
  - to
  - correctly
  - infer
  - the
  - dominant
  - distribution
  - which
  - shows
  - the
  - weakness
  - of
  - the
  - mixture-based
  - method
  - .
  - However
  - ','
  - we
  - would
  - like
  - to
  - note
  - that
  - Co-teaching
  - -LSB-
  - '5'
  - -RSB-
  - is
  - complementary
  - to
  - our
  - method
  - where
  - one
  - can
  - combine
  - these
  - two
  - methods
  - by
  - using
  - two
  - ChoiceNets
  - and
  - update
  - each
  - network
  - using
  - Co-teaching
  - .
  - '*'
  - We
  - also
  - conducted
  - additional
  - experiments
  - to
  - show
  - the
  - strength
  - of
  - the
  - proposed
  - method
  - .
  - a
  - -RRB-
  - .
  - More
  - baselines
  - to
  - current
  - CIFAR-10
  - experiments
  - ':'
  - We
  - implemented
  - MentorNet
  - -LSB-
  - '6'
  - -RSB-
  - and
  - VAT
  - -LSB-
  - '5'
  - -RSB-
  - to
  - better
  - evaluate
  - the
  - performance
  - of
  - the
  - proposed
  - method
  - 'on'
  - current
  - CIFAR-10
  - setting
  - .
  - corruption
  - rate
  - '20'
  - '%'
  - '50'
  - '%'
  - '80'
  - '%'
  - '----------------------------------------------'
  - MentorNet
  - PD
  - '64.0'
  - '%'
  - '49.0'
  - '%'
  - '21.4'
  - '%'
  - MentorNet
  - DD
  - '62.0'
  - '%'
  - '43.1'
  - '%'
  - '21.8'
  - '%'
  - VAT
  - '82.0'
  - '%'
  - '71.6'
  - '%'
  - '16.9'
  - '%'
  - '----------------------------------------------'
  - CN+M
  - ixup
  - '92.3'
  - '%'
  - '87.9'
  - '%'
  - '75.4'
  - '%'
  - b
  - -RRB-
  - Natural
  - language
  - processing
  - experiments
  - ':'
  - We
  - used
  - a
  - Large
  - Movie
  - Review
  - Dataset
  - consist
  - of
  - 25,000
  - movie
  - reviews
  - for
  - training
  - and
  - 25,000
  - reviews
  - for
  - testing
  - .
  - Each
  - movie
  - review
  - -LRB-
  - sentences
  - -RRB-
  - is
  - mapped
  - to
  - a
  - 128-dimensional
  - feature
  - vector
  - using
  - feed-forward
  - Neural-Net
  - Language
  - Models
  - -LSB-
  - '8'
  - -RSB-
  - and
  - we
  - tested
  - the
  - robustness
  - of
  - the
  - proposed
  - method
  - ','
  - mix-up
  - ','
  - and
  - naive
  - MLP
  - baseline
  - by
  - randomly
  - flipping
  - the
  - labels
  - .
  - random
  - flip
  - rate
  - '0'
  - '%'
  - '10'
  - '%'
  - '20'
  - '%'
  - '30'
  - '%'
  - '40'
  - '%'
  - '-------------------------------------------------------------'
  - ChoiceNet
  - '79.43'
  - '%'
  - '79.50'
  - '%'
  - '78.66'
  - '%'
  - '77.10'
  - '%'
  - '73.98'
  - '%'
  - Mix-up
  - '79.77'
  - '%'
  - '78.73'
  - '%'
  - '77.58'
  - '%'
  - '75.85'
  - '%'
  - '69.63'
  - '%'
  - Baseline
  - -LRB-
  - MLP
  - -RRB-
  - '79.04'
  - '%'
  - '77.88'
  - '%'
  - '75.70'
  - '%'
  - '69.05'
  - '%'
  - '62.83'
  - '%'
  - VAT
  - '76.40'
  - '%'
  - '72.50'
  - '%'
  - '69.20'
  - '%'
  - '65.20'
  - '%'
  - '58.30'
  - '%'
  - Similar
  - to
  - regression
  - experiments
  - ','
  - ChoiceNet
  - shows
  - the
  - superior
  - performance
  - in
  - the
  - presence
  - of
  - outliers
  - where
  - we
  - observe
  - that
  - the
  - proposed
  - method
  - can
  - be
  - used
  - for
  - NLP
  - tasks
  - as
  - well
  - .
  - -LSB-
  - '1'
  - -RSB-
  - V.
  - Belagiannis
  - ','
  - C.
  - Rupprecht
  - ','
  - G
  - ','
  - Carneiro
  - ','
  - N.
  - Navab
  - ','
  - '``'
  - Robust
  - Optimization
  - for
  - Deep
  - Regression
  - ''''''
  - ','
  - ICCV
  - ','
  - '2015'
  - -LSB-
  - '2'
  - -RSB-
  - H.
  - Zhang
  - ','
  - M.
  - Cisse
  - ','
  - Y.
  - Dauphin
  - ','
  - D.
  - Lopez-Paz
  - ','
  - '``'
  - mixup
  - ':'
  - Beyond
  - Empirical
  - Risk
  - Minimization
  - '``'
  - ','
  - ICLR
  - ','
  - '2018'
  - .
  - -LSB-
  - '3'
  - -RSB-
  - B.
  - Han
  - ','
  - Q.
  - Yao
  - ','
  - X.
  - Yu
  - ','
  - G.
  - Niu
  - ','
  - M.
  - Xu
  - ','
  - W.
  - Hu
  - ','
  - I.
  - Tsang
  - ','
  - M.
  - Sugiyama
  - ','
  - '``'
  - Co-teaching
  - ':'
  - Robust
  - Training
  - of
  - Deep
  - Neural
  - Networks
  - with
  - Extremely
  - Noisy
  - Labels
  - ''''''
  - ','
  - NIPS
  - ','
  - '2018'
  - .
  - -LSB-
  - '4'
  - -RSB-
  - Platanios
  - ','
  - E.
  - Antonios
  - ','
  - A.
  - Dubey
  - ','
  - and
  - T.
  - Mitchell
  - .
  - '``'
  - Estimating
  - accuracy
  - from
  - unlabeled
  - data
  - ':'
  - A
  - bayesian
  - approach
  - .
  - ''''''
  - International
  - Conference
  - 'on'
  - Machine
  - Learning
  - .
  - '2016'
  - .
  - -LSB-
  - '5'
  - -RSB-
  - T.
  - Miyato
  - ','
  - S.
  - Maeda
  - ','
  - M.
  - Koyama
  - ','
  - and
  - S.
  - Ishii
  - .
  - Virtual
  - adversarial
  - training
  - ':'
  - A
  - regularization
  - method
  - for
  - supervised
  - and
  - semi-supervised
  - learning
  - .
  - ICLR
  - ','
  - '2016'
  - .
  - -LSB-
  - '6'
  - -RSB-
  - L.
  - Jiang
  - ','
  - Z.
  - Zhou
  - ','
  - T.
  - Leung
  - ','
  - L.
  - Li
  - ','
  - and
  - L.
  - Fei-Fei
  - .
  - Mentornet
  - ':'
  - Learning
  - data-driven
  - curriculum
  - for
  - very
  - deep
  - neural
  - networks
  - 'on'
  - corrupted
  - labels
  - .
  - In
  - ICML
  - ','
  - '2018'
  - .
  - -LSB-
  - '7'
  - -RSB-
  - G.
  - Patrini
  - ','
  - A.
  - Rozza
  - ','
  - A.
  - Menon
  - ','
  - R.
  - Nock
  - ','
  - and
  - L.
  - Qu
  - .
  - Making
  - deep
  - neural
  - networks
  - robust
  - to
  - label
  - noise
  - ':'
  - A
  - loss
  - correction
  - approach
  - .
  - In
  - CVPR
  - ','
  - '2017'
  - .
  - -LSB-
  - '8'
  - -RSB-
  - Y.
  - Bengio
  - ','
  - R.
  - Ducharme
  - ','
  - P.
  - Vincent
  - ','
  - C.
  - Jauvin
  - .
  - A
  - Neural
  - Probabilistic
  - Language
  - Model
  - .
  - Journal
  - of
  - Machine
  - Learning
  - Research
  - ','
  - 3:1137-1155
  - ','
  - '2003'
  - .
- comment_id: B1eVJiKyCX
  rels:
  - !!python/tuple
    - 0
    - 5
    - 5
    - 23
    - list
  - !!python/tuple
    - 5
    - 23
    - 0
    - 5
    - list
  - !!python/tuple
    - 0
    - 23
    - 23
    - 647
    - elaboration
  - !!python/tuple
    - 23
    - 34
    - 34
    - 56
    - list
  - !!python/tuple
    - 34
    - 56
    - 23
    - 34
    - list
  - !!python/tuple
    - 23
    - 56
    - 56
    - 647
    - elaboration
  - !!python/tuple
    - 56
    - 67
    - 67
    - 73
    - elaboration
  - !!python/tuple
    - 67
    - 68
    - 68
    - 73
    - elaboration
  - !!python/tuple
    - 56
    - 73
    - 73
    - 647
    - elaboration
  - !!python/tuple
    - 73
    - 100
    - 100
    - 647
    - list
  - !!python/tuple
    - 77
    - 100
    - 73
    - 77
    - attribution
  - !!python/tuple
    - 77
    - 96
    - 96
    - 100
    - attribution
  - !!python/tuple
    - 100
    - 647
    - 73
    - 100
    - list
  - !!python/tuple
    - 100
    - 140
    - 140
    - 196
    - elaboration
  - !!python/tuple
    - 140
    - 154
    - 154
    - 196
    - list
  - !!python/tuple
    - 140
    - 147
    - 147
    - 154
    - elaboration
  - !!python/tuple
    - 154
    - 196
    - 140
    - 154
    - list
  - !!python/tuple
    - 154
    - 163
    - 163
    - 196
    - list
  - !!python/tuple
    - 163
    - 196
    - 154
    - 163
    - list
  - !!python/tuple
    - 100
    - 196
    - 196
    - 647
    - elaboration
  - !!python/tuple
    - 196
    - 201
    - 201
    - 203
    - elaboration
  - !!python/tuple
    - 196
    - 203
    - 203
    - 647
    - example
  - !!python/tuple
    - 203
    - 223
    - 223
    - 647
    - list
  - !!python/tuple
    - 223
    - 647
    - 203
    - 223
    - list
  - !!python/tuple
    - 223
    - 225
    - 225
    - 647
    - list
  - !!python/tuple
    - 225
    - 647
    - 223
    - 225
    - list
  - !!python/tuple
    - 232
    - 260
    - 225
    - 232
    - attribution
  - !!python/tuple
    - 237
    - 260
    - 232
    - 237
    - attribution
  - !!python/tuple
    - 237
    - 241
    - 241
    - 260
    - circumstance
  - !!python/tuple
    - 241
    - 251
    - 251
    - 259
    - elaboration
  - !!python/tuple
    - 241
    - 259
    - 259
    - 260
    - elaboration
  - !!python/tuple
    - 225
    - 260
    - 260
    - 647
    - elaboration
  - !!python/tuple
    - 260
    - 280
    - 280
    - 647
    - list
  - !!python/tuple
    - 280
    - 647
    - 260
    - 280
    - list
  - !!python/tuple
    - 280
    - 282
    - 282
    - 647
    - elaboration
  - !!python/tuple
    - 282
    - 312
    - 312
    - 647
    - list
  - !!python/tuple
    - 298
    - 312
    - 282
    - 298
    - attribution
  - !!python/tuple
    - 282
    - 297
    - 297
    - 298
    - elaboration
  - !!python/tuple
    - 312
    - 647
    - 282
    - 312
    - list
  - !!python/tuple
    - 312
    - 315
    - 315
    - 324
    - purpose
  - !!python/tuple
    - 312
    - 324
    - 324
    - 456
    - explanation
  - !!python/tuple
    - 324
    - 335
    - 335
    - 353
    - list
  - !!python/tuple
    - 335
    - 353
    - 324
    - 335
    - list
  - !!python/tuple
    - 335
    - 339
    - 339
    - 353
    - elaboration
  - !!python/tuple
    - 324
    - 353
    - 353
    - 456
    - elaboration
  - !!python/tuple
    - 353
    - 362
    - 362
    - 389
    - elaboration
  - !!python/tuple
    - 353
    - 389
    - 389
    - 456
    - elaboration
  - !!python/tuple
    - 389
    - 434
    - 434
    - 456
    - elaboration
  - !!python/tuple
    - 441
    - 456
    - 434
    - 441
    - attribution
  - !!python/tuple
    - 312
    - 456
    - 456
    - 647
    - elaboration
  - !!python/tuple
    - 456
    - 458
    - 458
    - 492
    - purpose
  - !!python/tuple
    - 458
    - 471
    - 471
    - 492
    - purpose
  - !!python/tuple
    - 456
    - 492
    - 492
    - 647
    - elaboration
  - !!python/tuple
    - 492
    - 500
    - 500
    - 530
    - elaboration
  - !!python/tuple
    - 500
    - 522
    - 522
    - 530
    - elaboration
  - !!python/tuple
    - 522
    - 526
    - 526
    - 530
    - elaboration
  - !!python/tuple
    - 492
    - 530
    - 530
    - 647
    - elaboration
  - !!python/tuple
    - 530
    - 551
    - 551
    - 647
    - contrast
  - !!python/tuple
    - 530
    - 536
    - 536
    - 551
    - purpose
  - !!python/tuple
    - 536
    - 548
    - 548
    - 551
    - elaboration
  - !!python/tuple
    - 551
    - 647
    - 530
    - 551
    - contrast
  - !!python/tuple
    - 551
    - 559
    - 559
    - 567
    - same_unit
  - !!python/tuple
    - 551
    - 554
    - 554
    - 559
    - elaboration
  - !!python/tuple
    - 559
    - 567
    - 551
    - 559
    - same_unit
  - !!python/tuple
    - 559
    - 564
    - 564
    - 567
    - elaboration
  - !!python/tuple
    - 551
    - 567
    - 567
    - 647
    - elaboration
  - !!python/tuple
    - 567
    - 586
    - 586
    - 647
    - list
  - !!python/tuple
    - 568
    - 586
    - 567
    - 568
    - attribution
  - !!python/tuple
    - 586
    - 647
    - 567
    - 586
    - list
  - !!python/tuple
    - 587
    - 593
    - 586
    - 587
    - attribution
  - !!python/tuple
    - 588
    - 593
    - 587
    - 588
    - attribution
  - !!python/tuple
    - 586
    - 593
    - 593
    - 647
    - elaboration
  - !!python/tuple
    - 593
    - 598
    - 598
    - 608
    - elaboration
  - !!python/tuple
    - 593
    - 608
    - 608
    - 626
    - elaboration
  - !!python/tuple
    - 608
    - 609
    - 609
    - 626
    - elaboration
  - !!python/tuple
    - 609
    - 617
    - 617
    - 626
    - elaboration
  - !!python/tuple
    - 617
    - 625
    - 625
    - 626
    - elaboration
  - !!python/tuple
    - 593
    - 626
    - 626
    - 647
    - elaboration
  - !!python/tuple
    - 626
    - 635
    - 635
    - 647
    - elaboration
  - !!python/tuple
    - 639
    - 647
    - 635
    - 639
    - attribution
  - !!python/tuple
    - 639
    - 643
    - 643
    - 647
    - elaboration
  tokens:
  - The
  - authors
  - made
  - several
  - claims
  - and
  - provide
  - suggestions
  - 'on'
  - training
  - binary
  - networks
  - ','
  - however
  - ','
  - they
  - are
  - not
  - proved
  - or
  - theoretically
  - analyzed
  - .
  - The
  - empirical
  - verification
  - of
  - the
  - proposed
  - hypothesis
  - was
  - viewed
  - as
  - weak
  - as
  - the
  - only
  - two
  - datasets
  - used
  - are
  - small
  - datasets
  - MNIST
  - and
  - CIFAR-10
  - ','
  - and
  - the
  - used
  - network
  - architectures
  - are
  - also
  - limited
  - .
  - Much
  - more
  - rigorous
  - and
  - thorough
  - testing
  - is
  - required
  - for
  - an
  - empirical
  - paper
  - which
  - proposes
  - new
  - claims
  - .
  - Take
  - the
  - first
  - claim
  - '``'
  - end-to-end
  - training
  - of
  - binary
  - networks
  - crucially
  - relies
  - 'on'
  - the
  - optimiser
  - taking
  - advantage
  - of
  - second
  - moment
  - gradient
  - estimates
  - ''''''
  - as
  - an
  - example
  - .
  - As
  - it
  - is
  - known
  - that
  - choice
  - of
  - optimizer
  - is
  - highly
  - dependent
  - 'on'
  - the
  - specific
  - dataset
  - and
  - network
  - structure
  - ','
  - it
  - is
  - not
  - convincing
  - to
  - jump
  - to
  - this
  - conclusion
  - using
  - the
  - observations
  - 'on'
  - two
  - small
  - datasets
  - and
  - limited
  - network
  - architectures
  - .
  - E.g
  - ','
  - many
  - binarization
  - papers
  - use
  - momentum
  - for
  - ImageNet
  - dataset
  - with
  - residual
  - networks
  - .
  - Does
  - Adam
  - also
  - outperforms
  - momentum
  - in
  - this
  - case
  - '?'
  - Similarly
  - ','
  - it
  - is
  - also
  - hard
  - for
  - me
  - to
  - judge
  - whether
  - the
  - other
  - conclusions
  - made
  - about
  - weight/gradient
  - clipping
  - ','
  - the
  - momentum
  - in
  - batch
  - normalization
  - and
  - learning
  - rate
  - ','
  - are
  - correct
  - or
  - not
  - .
  - Some
  - minor
  - issues
  - are
  - ':'
  - '1'
  - .
  - In
  - Figure
  - '4'
  - ','
  - different
  - methods
  - are
  - not
  - run
  - to
  - convergence
  - ','
  - and
  - the
  - comparison
  - may
  - not
  - be
  - fair
  - .
  - '2'
  - .
  - The
  - second
  - paragraph
  - in
  - section
  - '4'
  - ':'
  - '``'
  - It
  - can
  - be
  - seen
  - that
  - not
  - clipping
  - weights
  - when
  - learning
  - rates
  - are
  - large
  - can
  - completely
  - halt
  - the
  - optimisation
  - -LRB-
  - red
  - curve
  - in
  - Figure
  - '5'
  - -RRB-
  - .
  - ''''''
  - However
  - ','
  - in
  - figure
  - '5'
  - ','
  - the
  - red
  - curve
  - is
  - '``'
  - Clipping
  - gradients
  - ''''''
  - ','
  - which
  - one
  - is
  - correct
  - '?'
  - '3'
  - .
  - The
  - authors
  - propose
  - a
  - recipe
  - for
  - faster
  - training
  - of
  - binary
  - networks
  - ','
  - is
  - there
  - experiments
  - supporting
  - that
  - training
  - networks
  - with
  - the
  - proposed
  - recipe
  - is
  - faster
  - than
  - the
  - original
  - counterpart
  - '?'
  - We
  - would
  - like
  - to
  - thank
  - the
  - reviewer
  - for
  - the
  - constructive
  - comments
  - .
  - Our
  - aim
  - in
  - this
  - paper
  - is
  - to
  - provide
  - useful
  - empirical
  - observations
  - and
  - generate
  - possible
  - hypotheses
  - that
  - explain
  - them
  - ','
  - rather
  - than
  - to
  - make
  - new
  - claims
  - or
  - theoretical
  - analysis
  - .
  - It
  - is
  - 'true'
  - that
  - we
  - have
  - provided
  - some
  - hypotheses
  - about
  - what
  - might
  - be
  - going
  - 'on'
  - ','
  - but
  - at
  - the
  - end
  - of
  - the
  - day
  - ','
  - it
  - is
  - difficult
  - to
  - prove
  - such
  - new
  - claims
  - through
  - empirical
  - research
  - .
  - We
  - did
  - not
  - aim
  - to
  - present
  - conclusive
  - observations
  - for
  - '``'
  - X
  - is
  - necessary
  - for
  - Y
  - ''''''
  - but
  - rather
  - give
  - empirical
  - support
  - that
  - '``'
  - X
  - seems
  - to
  - be
  - tightly
  - connected
  - to
  - Y
  - ''''''
  - ','
  - i.e.
  - generating
  - hypothesis
  - to
  - be
  - validated
  - in
  - future
  - theoretical
  - research
  - .
  - '>'
  - More
  - datasets
  - and
  - architectures
  - We
  - do
  - agree
  - that
  - in
  - this
  - line
  - of
  - work
  - would
  - benefit
  - from
  - more
  - datasets
  - and
  - model
  - architectures
  - .
  - We
  - intended
  - to
  - repeat
  - our
  - experiments
  - with
  - larger
  - datasets
  - ','
  - but
  - a
  - hyperparameter
  - search
  - similar
  - to
  - what
  - we
  - have
  - done
  - for
  - smaller
  - standard
  - datasets
  - is
  - computationally
  - difficult
  - 'on'
  - much
  - larger
  - datasets
  - such
  - as
  - ImageNet
  - dataset
  - .
  - However
  - ','
  - we
  - have
  - now
  - updated
  - the
  - paper
  - to
  - include
  - results
  - 'on'
  - ImageNet
  - for
  - Section
  - '4'
  - of
  - the
  - paper
  - '>'
  - Convergence
  - in
  - Figure
  - '4'
  - This
  - touches
  - 'on'
  - the
  - same
  - points
  - raised
  - by
  - another
  - reviewer
  - regarding
  - early
  - stopping
  - .
  - In
  - our
  - experiments
  - ','
  - we
  - tried
  - to
  - give
  - the
  - training
  - phase
  - in
  - all
  - experiments
  - more
  - than
  - enough
  - time
  - to
  - converge
  - ','
  - but
  - some
  - optimizers
  - -LRB-
  - like
  - vanilla
  - SGD
  - -RRB-
  - simply
  - fail
  - in
  - many
  - scenarios
  - to
  - converge
  - .
  - '>'
  - In
  - figure
  - '5'
  - ','
  - the
  - red
  - curve
  - is
  - '``'
  - Clipping
  - gradients
  - ''''''
  - ','
  - which
  - one
  - is
  - correct
  - '?'
  - Thank
  - you
  - for
  - reporting
  - this
  - error
  - .
  - We
  - have
  - updated
  - the
  - paper
  - to
  - correct
  - the
  - order
  - of
  - items
  - in
  - the
  - legend
  - .
  - '>'
  - Baselines
  - for
  - training
  - binary
  - networks
  - faster
  - We
  - believe
  - these
  - results
  - are
  - already
  - included
  - in
  - Table
  - '5'
  - where
  - '``'
  - end-to-end
  - ''''''
  - denotes
  - the
  - original
  - counterpart
  - experiments
  - .
  - Do
  - let
  - us
  - know
  - if
  - you
  - have
  - something
  - different
  - in
  - mind
  - .
- comment_id: B1eBI3bX1V
  rels:
  - !!python/tuple
    - 0
    - 338
    - 338
    - 1324
    - topic
  - !!python/tuple
    - 0
    - 5
    - 5
    - 15
    - elaboration
  - !!python/tuple
    - 5
    - 9
    - 9
    - 15
    - elaboration
  - !!python/tuple
    - 0
    - 15
    - 15
    - 338
    - elaboration
  - !!python/tuple
    - 15
    - 23
    - 23
    - 53
    - elaboration
  - !!python/tuple
    - 23
    - 35
    - 35
    - 53
    - elaboration
  - !!python/tuple
    - 35
    - 40
    - 40
    - 53
    - elaboration
  - !!python/tuple
    - 15
    - 53
    - 53
    - 151
    - elaboration
  - !!python/tuple
    - 53
    - 59
    - 59
    - 85
    - elaboration
  - !!python/tuple
    - 59
    - 62
    - 62
    - 85
    - elaboration
  - !!python/tuple
    - 53
    - 85
    - 85
    - 151
    - elaboration
  - !!python/tuple
    - 85
    - 92
    - 92
    - 105
    - purpose
  - !!python/tuple
    - 85
    - 105
    - 105
    - 151
    - elaboration
  - !!python/tuple
    - 105
    - 115
    - 115
    - 151
    - elaboration
  - !!python/tuple
    - 15
    - 151
    - 151
    - 338
    - elaboration
  - !!python/tuple
    - 151
    - 167
    - 167
    - 188
    - elaboration
  - !!python/tuple
    - 167
    - 172
    - 172
    - 188
    - elaboration
  - !!python/tuple
    - 151
    - 188
    - 188
    - 338
    - elaboration
  - !!python/tuple
    - 188
    - 198
    - 198
    - 209
    - purpose
  - !!python/tuple
    - 188
    - 209
    - 209
    - 304
    - elaboration
  - !!python/tuple
    - 209
    - 217
    - 217
    - 243
    - elaboration
  - !!python/tuple
    - 217
    - 225
    - 225
    - 243
    - elaboration
  - !!python/tuple
    - 209
    - 243
    - 243
    - 304
    - elaboration
  - !!python/tuple
    - 243
    - 244
    - 244
    - 286
    - elaboration
  - !!python/tuple
    - 243
    - 286
    - 286
    - 304
    - elaboration
  - !!python/tuple
    - 286
    - 288
    - 288
    - 304
    - elaboration
  - !!python/tuple
    - 288
    - 294
    - 294
    - 304
    - list
  - !!python/tuple
    - 294
    - 304
    - 288
    - 294
    - list
  - !!python/tuple
    - 188
    - 304
    - 304
    - 338
    - elaboration
  - !!python/tuple
    - 304
    - 309
    - 309
    - 324
    - elaboration
  - !!python/tuple
    - 304
    - 324
    - 324
    - 338
    - elaboration
  - !!python/tuple
    - 338
    - 1324
    - 0
    - 338
    - topic
  - !!python/tuple
    - 338
    - 417
    - 417
    - 1324
    - list
  - !!python/tuple
    - 338
    - 352
    - 352
    - 417
    - elaboration
  - !!python/tuple
    - 352
    - 376
    - 376
    - 417
    - list
  - !!python/tuple
    - 352
    - 368
    - 368
    - 369
    - elaboration
  - !!python/tuple
    - 352
    - 369
    - 369
    - 376
    - purpose
  - !!python/tuple
    - 376
    - 417
    - 352
    - 376
    - list
  - !!python/tuple
    - 376
    - 382
    - 382
    - 393
    - purpose
  - !!python/tuple
    - 382
    - 385
    - 385
    - 393
    - elaboration
  - !!python/tuple
    - 376
    - 393
    - 393
    - 417
    - elaboration
  - !!python/tuple
    - 417
    - 1324
    - 338
    - 417
    - list
  - !!python/tuple
    - 417
    - 428
    - 428
    - 1324
    - list
  - !!python/tuple
    - 428
    - 1324
    - 417
    - 428
    - list
  - !!python/tuple
    - 428
    - 440
    - 440
    - 1324
    - list
  - !!python/tuple
    - 440
    - 1324
    - 428
    - 440
    - list
  - !!python/tuple
    - 440
    - 452
    - 452
    - 1324
    - list
  - !!python/tuple
    - 452
    - 1324
    - 440
    - 452
    - list
  - !!python/tuple
    - 452
    - 479
    - 479
    - 1324
    - elaboration
  - !!python/tuple
    - 479
    - 505
    - 505
    - 1324
    - list
  - !!python/tuple
    - 479
    - 500
    - 500
    - 505
    - elaboration
  - !!python/tuple
    - 505
    - 1324
    - 479
    - 505
    - list
  - !!python/tuple
    - 505
    - 526
    - 526
    - 1324
    - list
  - !!python/tuple
    - 505
    - 509
    - 509
    - 526
    - purpose
  - !!python/tuple
    - 509
    - 516
    - 516
    - 526
    - elaboration
  - !!python/tuple
    - 519
    - 526
    - 516
    - 519
    - attribution
  - !!python/tuple
    - 526
    - 1324
    - 505
    - 526
    - list
  - !!python/tuple
    - 526
    - 538
    - 538
    - 563
    - list
  - !!python/tuple
    - 538
    - 563
    - 526
    - 538
    - list
  - !!python/tuple
    - 541
    - 563
    - 538
    - 541
    - attribution
  - !!python/tuple
    - 541
    - 547
    - 547
    - 563
    - list
  - !!python/tuple
    - 547
    - 563
    - 541
    - 547
    - list
  - !!python/tuple
    - 547
    - 552
    - 552
    - 563
    - purpose
  - !!python/tuple
    - 526
    - 563
    - 563
    - 1324
    - example
  - !!python/tuple
    - 563
    - 579
    - 579
    - 1324
    - list
  - !!python/tuple
    - 579
    - 1324
    - 563
    - 579
    - list
  - !!python/tuple
    - 579
    - 589
    - 589
    - 1324
    - list
  - !!python/tuple
    - 589
    - 1324
    - 579
    - 589
    - list
  - !!python/tuple
    - 589
    - 610
    - 610
    - 1324
    - question
  - !!python/tuple
    - 610
    - 1324
    - 589
    - 610
    - question
  - !!python/tuple
    - 610
    - 648
    - 648
    - 1324
    - list
  - !!python/tuple
    - 610
    - 616
    - 616
    - 648
    - same_unit
  - !!python/tuple
    - 610
    - 615
    - 615
    - 616
    - elaboration
  - !!python/tuple
    - 616
    - 648
    - 610
    - 616
    - same_unit
  - !!python/tuple
    - 627
    - 648
    - 616
    - 627
    - attribution
  - !!python/tuple
    - 627
    - 630
    - 630
    - 635
    - elaboration
  - !!python/tuple
    - 627
    - 635
    - 635
    - 648
    - elaboration
  - !!python/tuple
    - 648
    - 1324
    - 610
    - 648
    - list
  - !!python/tuple
    - 648
    - 973
    - 973
    - 1324
    - topic
  - !!python/tuple
    - 648
    - 650
    - 650
    - 973
    - elaboration
  - !!python/tuple
    - 650
    - 656
    - 656
    - 660
    - elaboration
  - !!python/tuple
    - 650
    - 660
    - 660
    - 973
    - elaboration
  - !!python/tuple
    - 660
    - 679
    - 679
    - 973
    - elaboration
  - !!python/tuple
    - 679
    - 692
    - 692
    - 973
    - elaboration
  - !!python/tuple
    - 692
    - 706
    - 706
    - 731
    - elaboration
  - !!python/tuple
    - 706
    - 717
    - 717
    - 731
    - same_unit
  - !!python/tuple
    - 717
    - 731
    - 706
    - 717
    - same_unit
  - !!python/tuple
    - 723
    - 731
    - 717
    - 723
    - concession
  - !!python/tuple
    - 692
    - 731
    - 731
    - 769
    - elaboration
  - !!python/tuple
    - 731
    - 748
    - 748
    - 769
    - elaboration
  - !!python/tuple
    - 692
    - 769
    - 769
    - 973
    - elaboration
  - !!python/tuple
    - 769
    - 771
    - 771
    - 973
    - list
  - !!python/tuple
    - 771
    - 973
    - 769
    - 771
    - list
  - !!python/tuple
    - 774
    - 783
    - 771
    - 774
    - attribution
  - !!python/tuple
    - 771
    - 783
    - 783
    - 973
    - elaboration
  - !!python/tuple
    - 783
    - 793
    - 793
    - 797
    - circumstance
  - !!python/tuple
    - 783
    - 797
    - 797
    - 973
    - elaboration
  - !!python/tuple
    - 797
    - 802
    - 802
    - 973
    - elaboration
  - !!python/tuple
    - 802
    - 818
    - 818
    - 973
    - elaboration
  - !!python/tuple
    - 818
    - 832
    - 832
    - 852
    - same_unit
  - !!python/tuple
    - 818
    - 821
    - 821
    - 832
    - elaboration
  - !!python/tuple
    - 832
    - 852
    - 818
    - 832
    - same_unit
  - !!python/tuple
    - 832
    - 835
    - 835
    - 852
    - elaboration
  - !!python/tuple
    - 818
    - 852
    - 852
    - 973
    - elaboration
  - !!python/tuple
    - 852
    - 866
    - 866
    - 973
    - elaboration
  - !!python/tuple
    - 866
    - 871
    - 871
    - 882
    - elaboration
  - !!python/tuple
    - 866
    - 882
    - 882
    - 973
    - elaboration
  - !!python/tuple
    - 882
    - 908
    - 908
    - 973
    - elaboration
  - !!python/tuple
    - 908
    - 911
    - 911
    - 922
    - elaboration
  - !!python/tuple
    - 911
    - 913
    - 913
    - 922
    - elaboration
  - !!python/tuple
    - 908
    - 922
    - 922
    - 973
    - elaboration
  - !!python/tuple
    - 922
    - 937
    - 937
    - 948
    - list
  - !!python/tuple
    - 922
    - 931
    - 931
    - 937
    - purpose
  - !!python/tuple
    - 937
    - 948
    - 922
    - 937
    - list
  - !!python/tuple
    - 922
    - 948
    - 948
    - 973
    - circumstance
  - !!python/tuple
    - 973
    - 1324
    - 648
    - 973
    - topic
  - !!python/tuple
    - 973
    - 1075
    - 1075
    - 1324
    - topic
  - !!python/tuple
    - 978
    - 1009
    - 973
    - 978
    - attribution
  - !!python/tuple
    - 997
    - 1009
    - 978
    - 997
    - attribution
  - !!python/tuple
    - 973
    - 1009
    - 1009
    - 1075
    - elaboration
  - !!python/tuple
    - 1009
    - 1011
    - 1011
    - 1032
    - elaboration
  - !!python/tuple
    - 1015
    - 1032
    - 1011
    - 1015
    - attribution
  - !!python/tuple
    - 1015
    - 1020
    - 1020
    - 1032
    - same_unit
  - !!python/tuple
    - 1020
    - 1032
    - 1015
    - 1020
    - same_unit
  - !!python/tuple
    - 1009
    - 1032
    - 1032
    - 1075
    - elaboration
  - !!python/tuple
    - 1032
    - 1043
    - 1043
    - 1053
    - same_unit
  - !!python/tuple
    - 1032
    - 1042
    - 1042
    - 1043
    - elaboration
  - !!python/tuple
    - 1043
    - 1053
    - 1032
    - 1043
    - same_unit
  - !!python/tuple
    - 1032
    - 1053
    - 1053
    - 1075
    - elaboration
  - !!python/tuple
    - 1053
    - 1067
    - 1067
    - 1075
    - elaboration
  - !!python/tuple
    - 1075
    - 1324
    - 973
    - 1075
    - topic
  - !!python/tuple
    - 1075
    - 1078
    - 1078
    - 1324
    - textualorganization
  - !!python/tuple
    - 1078
    - 1324
    - 1075
    - 1078
    - textualorganization
  - !!python/tuple
    - 1078
    - 1100
    - 1100
    - 1324
    - list
  - !!python/tuple
    - 1088
    - 1100
    - 1078
    - 1088
    - attribution
  - !!python/tuple
    - 1088
    - 1091
    - 1091
    - 1100
    - elaboration
  - !!python/tuple
    - 1091
    - 1095
    - 1095
    - 1100
    - elaboration
  - !!python/tuple
    - 1100
    - 1324
    - 1078
    - 1100
    - list
  - !!python/tuple
    - 1100
    - 1103
    - 1103
    - 1324
    - list
  - !!python/tuple
    - 1103
    - 1324
    - 1100
    - 1103
    - list
  - !!python/tuple
    - 1103
    - 1105
    - 1105
    - 1324
    - textualorganization
  - !!python/tuple
    - 1105
    - 1324
    - 1103
    - 1105
    - textualorganization
  - !!python/tuple
    - 1105
    - 1123
    - 1123
    - 1324
    - list
  - !!python/tuple
    - 1105
    - 1115
    - 1115
    - 1123
    - elaboration
  - !!python/tuple
    - 1123
    - 1324
    - 1105
    - 1123
    - list
  - !!python/tuple
    - 1123
    - 1125
    - 1125
    - 1148
    - example
  - !!python/tuple
    - 1125
    - 1135
    - 1135
    - 1148
    - elaboration
  - !!python/tuple
    - 1135
    - 1141
    - 1141
    - 1148
    - purpose
  - !!python/tuple
    - 1141
    - 1145
    - 1145
    - 1148
    - elaboration
  - !!python/tuple
    - 1123
    - 1148
    - 1148
    - 1324
    - elaboration
  - !!python/tuple
    - 1148
    - 1156
    - 1156
    - 1167
    - list
  - !!python/tuple
    - 1156
    - 1167
    - 1148
    - 1156
    - list
  - !!python/tuple
    - 1148
    - 1167
    - 1167
    - 1324
    - elaboration
  - !!python/tuple
    - 1167
    - 1178
    - 1178
    - 1324
    - list
  - !!python/tuple
    - 1178
    - 1324
    - 1167
    - 1178
    - list
  - !!python/tuple
    - 1178
    - 1181
    - 1181
    - 1196
    - purpose
  - !!python/tuple
    - 1178
    - 1196
    - 1196
    - 1210
    - elaboration
  - !!python/tuple
    - 1178
    - 1210
    - 1210
    - 1324
    - elaboration
  - !!python/tuple
    - 1210
    - 1231
    - 1231
    - 1324
    - list
  - !!python/tuple
    - 1214
    - 1231
    - 1210
    - 1214
    - attribution
  - !!python/tuple
    - 1218
    - 1231
    - 1214
    - 1218
    - attribution
  - !!python/tuple
    - 1231
    - 1324
    - 1210
    - 1231
    - list
  - !!python/tuple
    - 1237
    - 1242
    - 1231
    - 1237
    - attribution
  - !!python/tuple
    - 1231
    - 1242
    - 1242
    - 1255
    - elaboration
  - !!python/tuple
    - 1242
    - 1250
    - 1250
    - 1255
    - elaboration
  - !!python/tuple
    - 1231
    - 1255
    - 1255
    - 1324
    - elaboration
  - !!python/tuple
    - 1255
    - 1278
    - 1278
    - 1294
    - elaboration
  - !!python/tuple
    - 1278
    - 1289
    - 1289
    - 1294
    - elaboration
  - !!python/tuple
    - 1255
    - 1294
    - 1294
    - 1324
    - elaboration
  - !!python/tuple
    - 1294
    - 1322
    - 1322
    - 1324
    - list
  - !!python/tuple
    - 1294
    - 1307
    - 1307
    - 1322
    - purpose
  - !!python/tuple
    - 1311
    - 1322
    - 1307
    - 1311
    - attribution
  - !!python/tuple
    - 1311
    - 1317
    - 1317
    - 1322
    - elaboration
  - !!python/tuple
    - 1322
    - 1324
    - 1294
    - 1322
    - list
  - !!python/tuple
    - 1322
    - 1323
    - 1323
    - 1324
    - elaboration
  tokens:
  - This
  - paper
  - presents
  - a
  - methodology
  - to
  - infer
  - shape
  - programs
  - that
  - can
  - describe
  - 3D
  - objects
  - .
  - The
  - key
  - intuition
  - of
  - the
  - shape
  - programs
  - is
  - to
  - integrate
  - bottom-up
  - low-level
  - feature
  - recognition
  - with
  - symbolic
  - high-level
  - program
  - structure
  - ','
  - which
  - allows
  - the
  - shape
  - programs
  - to
  - capture
  - both
  - high-level
  - structure
  - and
  - the
  - low-level
  - geometry
  - of
  - the
  - shapes
  - .
  - The
  - paper
  - proposes
  - a
  - domain-specific
  - language
  - for
  - 3D
  - shapes
  - that
  - consists
  - of
  - '``'
  - For
  - ''''''
  - loops
  - for
  - capturing
  - high-level
  - regularity
  - ','
  - and
  - associates
  - objects
  - with
  - both
  - their
  - geometric
  - and
  - semantic
  - attributes
  - .
  - It
  - then
  - proposes
  - an
  - end-to-end
  - differentiable
  - architecture
  - to
  - learn
  - such
  - 3D
  - programs
  - from
  - shapes
  - using
  - an
  - interesting
  - self-supervised
  - mechanism
  - .
  - The
  - neural
  - program
  - generator
  - proposes
  - a
  - program
  - in
  - the
  - DSL
  - that
  - is
  - executed
  - by
  - a
  - neural
  - program
  - execution
  - module
  - to
  - render
  - the
  - corresponding
  - output
  - shape
  - ','
  - which
  - is
  - then
  - compared
  - with
  - the
  - original
  - shape
  - and
  - the
  - difference
  - loss
  - is
  - back-propagated
  - to
  - improve
  - the
  - program
  - distribution
  - .
  - The
  - technique
  - is
  - evaluated
  - 'on'
  - both
  - synthetic
  - and
  - ShapeNet
  - tasks
  - ','
  - and
  - leads
  - to
  - significant
  - improvements
  - compared
  - to
  - Tulsiani
  - et
  - al.
  - that
  - embed
  - a
  - prior
  - structure
  - 'on'
  - learning
  - shape
  - representations
  - as
  - a
  - composition
  - of
  - primitive
  - abstractions
  - .
  - In
  - addition
  - ','
  - the
  - technique
  - is
  - also
  - paired
  - with
  - MarrNet
  - to
  - allow
  - for
  - a
  - better
  - 3D
  - reconstruction
  - from
  - 2D
  - images
  - .
  - Overall
  - ','
  - this
  - paper
  - presents
  - an
  - elegant
  - idea
  - to
  - describe
  - 3D
  - shapes
  - as
  - a
  - DSL
  - program
  - that
  - captures
  - both
  - geometric
  - and
  - spatial
  - abstractions
  - ','
  - and
  - at
  - the
  - same
  - time
  - captures
  - regularities
  - using
  - loops
  - .
  - CSGNet
  - -LSB-
  - Sharma
  - et
  - al.
  - '2018'
  - -RSB-
  - also
  - uses
  - programs
  - to
  - describe
  - 2D
  - and
  - 3D
  - shapes
  - ','
  - but
  - the
  - DSL
  - used
  - here
  - is
  - richer
  - as
  - it
  - captures
  - more
  - high-level
  - regularities
  - using
  - loops
  - and
  - also
  - semantic
  - relationships
  - such
  - as
  - top
  - ','
  - support
  - etc.
  - .
  - The
  - idea
  - of
  - training
  - a
  - neural
  - program
  - executor
  - and
  - using
  - it
  - for
  - self-supervised
  - training
  - is
  - quite
  - elegant
  - .
  - I
  - also
  - liked
  - the
  - idea
  - of
  - guided
  - adaption
  - to
  - make
  - the
  - program
  - generator
  - generalize
  - beyond
  - the
  - synthetic
  - template
  - programs
  - .
  - Finally
  - ','
  - the
  - results
  - show
  - impressive
  - improvements
  - and
  - generalization
  - capability
  - of
  - the
  - model
  - .
  - Can
  - the
  - authors
  - comment
  - 'on'
  - some
  - notion
  - of
  - completeness
  - of
  - the
  - proposed
  - DSL
  - '?'
  - In
  - other
  - words
  - ','
  - is
  - this
  - the
  - only
  - set
  - of
  - operators
  - ','
  - shapes
  - ','
  - and
  - semantics
  - needed
  - to
  - represent
  - all
  - of
  - ShapeNet
  - objects
  - '?'
  - Also
  - ','
  - it
  - might
  - be
  - interesting
  - to
  - comment
  - more
  - 'on'
  - how
  - this
  - particular
  - DSL
  - was
  - derived
  - .
  - Some
  - of
  - the
  - semantics
  - operator
  - such
  - as
  - '``'
  - Support
  - ''''''
  - ','
  - '``'
  - Locker
  - ''''''
  - ','
  - etc.
  - look
  - overly
  - specific
  - to
  - chair
  - and
  - tables
  - .
  - Is
  - there
  - a
  - way
  - to
  - possibly
  - learn
  - such
  - abstractions
  - automatically
  - '?'
  - What
  - is
  - the
  - total
  - search
  - space
  - of
  - programs
  - in
  - this
  - DSL
  - '?'
  - How
  - would
  - a
  - naive
  - random
  - search
  - perform
  - in
  - this
  - synthesis
  - task
  - '?'
  - I
  - also
  - particularly
  - liked
  - the
  - decomposition
  - of
  - programs
  - into
  - draw
  - and
  - compound
  - statements
  - ','
  - and
  - the
  - corresponding
  - program
  - generator
  - decomposition
  - into
  - '2'
  - steps
  - BlockLSTM
  - and
  - StepLSTM
  - .
  - At
  - inference
  - time
  - ','
  - does
  - the
  - model
  - use
  - some
  - form
  - of
  - beam
  - search
  - to
  - sample
  - block
  - programs
  - or
  - are
  - the
  - results
  - corresponding
  - to
  - top-1
  - prediction
  - '?'
  - Would
  - it
  - be
  - possible
  - to
  - compare
  - the
  - results
  - to
  - the
  - technique
  - presented
  - in
  - CSGNet
  - -LSB-
  - Sharma
  - et
  - al.
  - '2018'
  - -RSB-
  - '?'
  - There
  - are
  - some
  - key
  - differences
  - in
  - terms
  - of
  - using
  - lower-level
  - DSL
  - primitives
  - and
  - using
  - REINFORCE
  - for
  - training
  - the
  - program
  - generator
  - ','
  - but
  - it
  - would
  - be
  - good
  - to
  - measure
  - how
  - well
  - having
  - higher-level
  - primitives
  - improve
  - the
  - results
  - .
  - I
  - presume
  - the
  - neural
  - program
  - executor
  - module
  - was
  - trained
  - using
  - a
  - manually-written
  - shape
  - program
  - interpreter
  - .
  - How
  - difficult
  - is
  - it
  - to
  - write
  - such
  - an
  - interpreter
  - '?'
  - Also
  - ','
  - how
  - easy/difficult
  - is
  - to
  - extend
  - the
  - DSL
  - with
  - new
  - semantics
  - operator
  - and
  - then
  - write
  - the
  - corresponding
  - interpreter
  - extension
  - '?'
  - Minor
  - typos
  - ':'
  - page
  - '3'
  - ':'
  - consists
  - a
  - variable
  - NN
  - consists
  - of
  - a
  - variable
  - page
  - '5'
  - ':'
  - We
  - executes
  - CD
  - We
  - execute
  - page
  - '6'
  - ':'
  - synthetica
  - dataset
  - CD
  - synthetic
  - dataset
  - Thank
  - you
  - for
  - the
  - very
  - constructive
  - comments
  - .
  - '1'
  - .
  - DSL
  - The
  - current
  - DSL
  - is
  - designed
  - to
  - represent
  - furnitures
  - .
  - Representing
  - all
  - ShapeNet
  - objects
  - needs
  - a
  - richer
  - set
  - of
  - primitives
  - ','
  - e.g.
  - ','
  - curved
  - cylinders
  - for
  - mug
  - handles
  - .
  - When
  - we
  - design
  - such
  - DSL
  - ','
  - the
  - main
  - challenge
  - is
  - 'on'
  - semantics
  - .
  - For
  - humans
  - ','
  - some
  - semantics
  - are
  - shared
  - across
  - different
  - object
  - categories
  - ','
  - e.g.
  - ','
  - '``'
  - top
  - ''''''
  - can
  - be
  - shared
  - by
  - tables
  - and
  - bed
  - ','
  - while
  - some
  - are
  - just
  - category-specific
  - ','
  - '``'
  - armrest
  - ''''''
  - is
  - mainly
  - for
  - chairs
  - .
  - Following
  - this
  - spirit
  - ','
  - we
  - include
  - both
  - category-specific
  - and
  - shared
  - semantics
  - for
  - the
  - instantialization
  - of
  - furnitures
  - .
  - Learning
  - a
  - primitive
  - library
  - from
  - data
  - is
  - a
  - natural
  - research
  - direction
  - ','
  - and
  - we
  - are
  - working
  - 'on'
  - it
  - as
  - follow-up
  - .
  - '2'
  - .
  - Baselines
  - We
  - agree
  - that
  - it
  - '''s'
  - important
  - to
  - add
  - more
  - baselines
  - .
  - In
  - the
  - revision
  - ','
  - we
  - will
  - include
  - comparisons
  - with
  - the
  - following
  - three
  - algorithms
  - ':'
  - '1'
  - -RRB-
  - Nearest
  - neighbors
  - .
  - For
  - a
  - given
  - test
  - shape
  - ','
  - we
  - search
  - its
  - nearest
  - neighbor
  - in
  - the
  - training
  - set
  - .
  - '2'
  - -RRB-
  - CSGNet-original
  - -LRB-
  - the
  - original
  - model
  - released
  - by
  - the
  - authors
  - of
  - CSGNet
  - -RRB-
  - '3'
  - -RRB-
  - CSGNet-augmented
  - -LRB-
  - the
  - augmented
  - CSGNet
  - model
  - trained
  - 'on'
  - our
  - dataset
  - with
  - additional
  - shape
  - primitives
  - we
  - introduced
  - -RRB-
  - .
  - Amortized
  - inference
  - is
  - essential
  - for
  - our
  - task
  - due
  - to
  - its
  - large
  - search
  - space
  - .
  - Our
  - model
  - takes
  - '5'
  - ms
  - to
  - infer
  - a
  - shape
  - program
  - with
  - a
  - Titan
  - X
  - GPU
  - .
  - There
  - are
  - two
  - possible
  - approaches
  - for
  - a
  - structured
  - search
  - over
  - the
  - space
  - of
  - programs
  - ','
  - both
  - of
  - which
  - will
  - be
  - too
  - slow
  - for
  - our
  - task
  - ':'
  - '1'
  - -RRB-
  - Constraint
  - solving
  - ':'
  - we
  - would
  - have
  - to
  - use
  - an
  - SMT
  - solver
  - .
  - Ellis
  - et
  - al
  - -LSB-
  - '1'
  - -RSB-
  - used
  - SMT
  - solvers
  - to
  - infer
  - 2D
  - graphics
  - programs
  - ','
  - and
  - takes
  - 'on'
  - the
  - order
  - of
  - 5-20
  - minutes
  - per
  - program
  - .
  - As
  - 3D
  - shapes
  - have
  - a
  - much
  - larger
  - search
  - space
  - ','
  - such
  - an
  - approach
  - would
  - not
  - be
  - able
  - to
  - find
  - a
  - solution
  - in
  - reasonable
  - time
  - .
  - '2'
  - -RRB-
  - Stochastic
  - search
  - ':'
  - Here
  - the
  - problem
  - would
  - be
  - at
  - least
  - as
  - tough
  - as
  - doing
  - inverse
  - graphics
  - ','
  - so
  - we
  - can
  - safely
  - assume
  - that
  - this
  - would
  - work
  - 'no'
  - better
  - than
  - MCMC
  - for
  - inverse
  - graphics
  - .
  - In
  - Picture
  - -LRB-
  - Kulkarni
  - et
  - al.
  - -LSB-
  - '2'
  - -RSB-
  - -RRB-
  - ','
  - their
  - approach
  - takes
  - minutes
  - for
  - a
  - 2D
  - image
  - with
  - simple
  - contours
  - .
  - We
  - have
  - contacted
  - the
  - authors
  - of
  - these
  - two
  - papers
  - ','
  - who
  - confirmed
  - our
  - estimates
  - of
  - the
  - efficiency
  - of
  - their
  - methods
  - .
  - -LSB-
  - '1'
  - -RSB-
  - Ellis
  - ','
  - Kevin
  - ','
  - Armando
  - Solar-Lezama
  - ','
  - and
  - Josh
  - Tenenbaum
  - .
  - '``'
  - Unsupervised
  - learning
  - by
  - program
  - synthesis
  - .
  - ''''''
  - NIPS
  - '2015'
  - .
  - -LSB-
  - '2'
  - -RSB-
  - Kulkarni
  - ','
  - Tejas
  - D.
  - ','
  - et
  - al.
  - '``'
  - Picture
  - ':'
  - A
  - probabilistic
  - programming
  - language
  - for
  - scene
  - perception
  - .
  - ''''''
  - CVPR
  - '2015'
  - .
  - '3'
  - .
  - Decomposition
  - Thanks
  - for
  - the
  - positive
  - comment
  - 'on'
  - the
  - decomposition
  - .
  - The
  - results
  - just
  - correspond
  - to
  - top-1
  - predictions
  - .
  - '4'
  - .
  - Interpreter
  - Our
  - semantic
  - operators
  - correspond
  - to
  - simple
  - geometric
  - primitives
  - .
  - Therefore
  - ','
  - it
  - '''s'
  - quite
  - straightforward
  - to
  - write
  - an
  - interpreter
  - for
  - them
  - .
  - The
  - programs
  - in
  - our
  - DSL
  - are
  - tokenized
  - vectors
  - and
  - can
  - be
  - directly
  - feed
  - into
  - the
  - neural
  - program
  - executor
  - .
  - Adding
  - new
  - semantic
  - operator
  - to
  - the
  - DSL
  - is
  - thus
  - easy
  - .
  - We
  - just
  - need
  - to
  - re-train
  - or
  - finetune
  - the
  - current
  - program
  - executor
  - with
  - the
  - new
  - semantic
  - operator
  - included
  - .
  - We
  - have
  - also
  - listed
  - all
  - other
  - planned
  - changes
  - in
  - our
  - general
  - response
  - above
  - .
  - Please
  - do
  - n't
  - hesitate
  - to
  - let
  - us
  - know
  - for
  - any
  - additional
  - comments
  - 'on'
  - the
  - paper
  - or
  - 'on'
  - the
  - planned
  - changes
  - .
  - Dear
  - Reviewer
  - '2'
  - ','
  - Thanks
  - again
  - for
  - your
  - constructive
  - comments
  - .
  - We
  - have
  - made
  - substantial
  - changes
  - in
  - the
  - revision
  - according
  - to
  - the
  - reviews
  - .
  - In
  - particular
  - ','
  - we
  - have
  - compared
  - our
  - model
  - with
  - three
  - additional
  - baselines
  - ','
  - including
  - CSGNet
  - ','
  - in
  - Table
  - '2'
  - and
  - Sec
  - '5.2'
  - .
  - We
  - '''ve'
  - also
  - discussed
  - the
  - design
  - of
  - DSL
  - and
  - search-based
  - models
  - -LRB-
  - Sec
  - '6'
  - -RRB-
  - .
  - As
  - the
  - discussion
  - period
  - is
  - about
  - to
  - end
  - ','
  - please
  - do
  - n't
  - hesitate
  - to
  - let
  - us
  - know
  - if
  - there
  - are
  - any
  - additional
  - clarifications
  - that
  - we
  - can
  - offer
  - .
  - Thanks
  - '!'
- comment_id: B1eFHHDnAQ
  rels:
  - !!python/tuple
    - 0
    - 5
    - 5
    - 18
    - purpose
  - !!python/tuple
    - 5
    - 9
    - 9
    - 18
    - elaboration
  - !!python/tuple
    - 9
    - 13
    - 13
    - 18
    - elaboration
  - !!python/tuple
    - 0
    - 18
    - 18
    - 1797
    - elaboration
  - !!python/tuple
    - 18
    - 28
    - 28
    - 43
    - list
  - !!python/tuple
    - 28
    - 43
    - 18
    - 28
    - list
  - !!python/tuple
    - 28
    - 36
    - 36
    - 43
    - list
  - !!python/tuple
    - 36
    - 43
    - 28
    - 36
    - list
  - !!python/tuple
    - 36
    - 37
    - 37
    - 43
    - elaboration
  - !!python/tuple
    - 18
    - 43
    - 43
    - 1797
    - elaboration
  - !!python/tuple
    - 43
    - 58
    - 58
    - 1797
    - textualorganization
  - !!python/tuple
    - 43
    - 52
    - 52
    - 58
    - elaboration
  - !!python/tuple
    - 58
    - 1797
    - 43
    - 58
    - textualorganization
  - !!python/tuple
    - 58
    - 94
    - 94
    - 1797
    - list
  - !!python/tuple
    - 58
    - 72
    - 72
    - 94
    - elaboration
  - !!python/tuple
    - 72
    - 78
    - 78
    - 94
    - list
  - !!python/tuple
    - 78
    - 94
    - 72
    - 78
    - list
  - !!python/tuple
    - 78
    - 88
    - 88
    - 94
    - list
  - !!python/tuple
    - 88
    - 94
    - 78
    - 88
    - list
  - !!python/tuple
    - 94
    - 1797
    - 58
    - 94
    - list
  - !!python/tuple
    - 94
    - 108
    - 108
    - 1797
    - list
  - !!python/tuple
    - 94
    - 99
    - 99
    - 108
    - purpose
  - !!python/tuple
    - 108
    - 1797
    - 94
    - 108
    - list
  - !!python/tuple
    - 116
    - 128
    - 108
    - 116
    - antithesis
  - !!python/tuple
    - 116
    - 122
    - 122
    - 128
    - elaboration
  - !!python/tuple
    - 122
    - 125
    - 125
    - 128
    - purpose
  - !!python/tuple
    - 108
    - 128
    - 128
    - 173
    - elaboration
  - !!python/tuple
    - 128
    - 154
    - 154
    - 173
    - elaboration
  - !!python/tuple
    - 154
    - 168
    - 168
    - 173
    - elaboration
  - !!python/tuple
    - 108
    - 173
    - 173
    - 1797
    - elaboration
  - !!python/tuple
    - 173
    - 191
    - 191
    - 1797
    - list
  - !!python/tuple
    - 173
    - 179
    - 179
    - 191
    - purpose
  - !!python/tuple
    - 179
    - 184
    - 184
    - 191
    - elaboration
  - !!python/tuple
    - 191
    - 1797
    - 173
    - 191
    - list
  - !!python/tuple
    - 191
    - 200
    - 200
    - 219
    - elaboration
  - !!python/tuple
    - 201
    - 219
    - 200
    - 201
    - attribution
  - !!python/tuple
    - 191
    - 219
    - 219
    - 1797
    - elaboration
  - !!python/tuple
    - 221
    - 1797
    - 219
    - 221
    - attribution
  - !!python/tuple
    - 221
    - 237
    - 237
    - 1797
    - textualorganization
  - !!python/tuple
    - 237
    - 1797
    - 221
    - 237
    - textualorganization
  - !!python/tuple
    - 237
    - 243
    - 243
    - 266
    - elaboration
  - !!python/tuple
    - 243
    - 255
    - 255
    - 266
    - same_unit
  - !!python/tuple
    - 243
    - 249
    - 249
    - 255
    - elaboration
  - !!python/tuple
    - 255
    - 266
    - 243
    - 255
    - same_unit
  - !!python/tuple
    - 237
    - 266
    - 266
    - 1797
    - elaboration
  - !!python/tuple
    - 266
    - 296
    - 296
    - 1797
    - list
  - !!python/tuple
    - 266
    - 278
    - 278
    - 296
    - elaboration
  - !!python/tuple
    - 296
    - 1797
    - 266
    - 296
    - list
  - !!python/tuple
    - 296
    - 333
    - 333
    - 1797
    - list
  - !!python/tuple
    - 296
    - 330
    - 330
    - 333
    - same_unit
  - !!python/tuple
    - 296
    - 311
    - 311
    - 330
    - elaboration
  - !!python/tuple
    - 330
    - 333
    - 296
    - 330
    - same_unit
  - !!python/tuple
    - 333
    - 1797
    - 296
    - 333
    - list
  - !!python/tuple
    - 333
    - 340
    - 340
    - 1797
    - textualorganization
  - !!python/tuple
    - 333
    - 339
    - 339
    - 340
    - elaboration
  - !!python/tuple
    - 340
    - 1797
    - 333
    - 340
    - textualorganization
  - !!python/tuple
    - 340
    - 385
    - 385
    - 1797
    - textualorganization
  - !!python/tuple
    - 340
    - 364
    - 364
    - 385
    - same_unit
  - !!python/tuple
    - 340
    - 357
    - 357
    - 364
    - elaboration
  - !!python/tuple
    - 364
    - 385
    - 340
    - 364
    - same_unit
  - !!python/tuple
    - 385
    - 1797
    - 340
    - 385
    - textualorganization
  - !!python/tuple
    - 385
    - 424
    - 424
    - 429
    - elaboration
  - !!python/tuple
    - 385
    - 429
    - 429
    - 1797
    - elaboration
  - !!python/tuple
    - 429
    - 471
    - 471
    - 1797
    - list
  - !!python/tuple
    - 430
    - 471
    - 429
    - 430
    - attribution
  - !!python/tuple
    - 471
    - 1797
    - 429
    - 471
    - list
  - !!python/tuple
    - 471
    - 525
    - 525
    - 1797
    - topic
  - !!python/tuple
    - 471
    - 483
    - 483
    - 525
    - elaboration
  - !!python/tuple
    - 483
    - 489
    - 489
    - 525
    - elaboration
  - !!python/tuple
    - 525
    - 1797
    - 471
    - 525
    - topic
  - !!python/tuple
    - 565
    - 1797
    - 525
    - 565
    - condition
  - !!python/tuple
    - 525
    - 559
    - 559
    - 565
    - elaboration
  - !!python/tuple
    - 565
    - 602
    - 602
    - 1797
    - list
  - !!python/tuple
    - 565
    - 578
    - 578
    - 584
    - purpose
  - !!python/tuple
    - 565
    - 584
    - 584
    - 602
    - elaboration
  - !!python/tuple
    - 590
    - 602
    - 584
    - 590
    - attribution
  - !!python/tuple
    - 602
    - 1797
    - 565
    - 602
    - list
  - !!python/tuple
    - 602
    - 608
    - 608
    - 626
    - purpose
  - !!python/tuple
    - 608
    - 614
    - 614
    - 626
    - elaboration
  - !!python/tuple
    - 602
    - 626
    - 626
    - 1797
    - circumstance
  - !!python/tuple
    - 626
    - 630
    - 630
    - 648
    - same_unit
  - !!python/tuple
    - 630
    - 648
    - 626
    - 630
    - same_unit
  - !!python/tuple
    - 631
    - 648
    - 630
    - 631
    - attribution
  - !!python/tuple
    - 626
    - 648
    - 648
    - 1797
    - elaboration
  - !!python/tuple
    - 650
    - 665
    - 648
    - 650
    - attribution
  - !!python/tuple
    - 650
    - 657
    - 657
    - 665
    - elaboration
  - !!python/tuple
    - 648
    - 665
    - 665
    - 1797
    - elaboration
  - !!python/tuple
    - 693
    - 1797
    - 665
    - 693
    - circumstance
  - !!python/tuple
    - 674
    - 693
    - 665
    - 674
    - attribution
  - !!python/tuple
    - 675
    - 693
    - 674
    - 675
    - attribution
  - !!python/tuple
    - 684
    - 693
    - 675
    - 684
    - condition
  - !!python/tuple
    - 693
    - 709
    - 709
    - 1797
    - elaboration
  - !!python/tuple
    - 709
    - 727
    - 727
    - 735
    - elaboration
  - !!python/tuple
    - 709
    - 735
    - 735
    - 1797
    - example
  - !!python/tuple
    - 735
    - 754
    - 754
    - 1797
    - textualorganization
  - !!python/tuple
    - 739
    - 754
    - 735
    - 739
    - attribution
  - !!python/tuple
    - 739
    - 745
    - 745
    - 754
    - list
  - !!python/tuple
    - 745
    - 754
    - 739
    - 745
    - list
  - !!python/tuple
    - 754
    - 1797
    - 735
    - 754
    - textualorganization
  - !!python/tuple
    - 754
    - 769
    - 769
    - 1797
    - list
  - !!python/tuple
    - 754
    - 760
    - 760
    - 769
    - elaboration
  - !!python/tuple
    - 769
    - 1797
    - 754
    - 769
    - list
  - !!python/tuple
    - 769
    - 774
    - 774
    - 1797
    - textualorganization
  - !!python/tuple
    - 774
    - 1797
    - 769
    - 774
    - textualorganization
  - !!python/tuple
    - 774
    - 794
    - 794
    - 1797
    - list
  - !!python/tuple
    - 794
    - 1797
    - 774
    - 794
    - list
  - !!python/tuple
    - 794
    - 804
    - 804
    - 835
    - elaboration
  - !!python/tuple
    - 804
    - 827
    - 827
    - 835
    - elaboration
  - !!python/tuple
    - 794
    - 835
    - 835
    - 848
    - elaboration
  - !!python/tuple
    - 794
    - 848
    - 848
    - 1797
    - elaboration
  - !!python/tuple
    - 854
    - 871
    - 848
    - 854
    - attribution
  - !!python/tuple
    - 856
    - 871
    - 854
    - 856
    - attribution
  - !!python/tuple
    - 848
    - 871
    - 871
    - 1797
    - elaboration
  - !!python/tuple
    - 871
    - 874
    - 874
    - 888
    - purpose
  - !!python/tuple
    - 876
    - 888
    - 874
    - 876
    - attribution
  - !!python/tuple
    - 871
    - 888
    - 888
    - 909
    - elaboration
  - !!python/tuple
    - 888
    - 892
    - 892
    - 909
    - circumstance
  - !!python/tuple
    - 871
    - 909
    - 909
    - 1797
    - elaboration
  - !!python/tuple
    - 909
    - 913
    - 913
    - 1797
    - textualorganization
  - !!python/tuple
    - 913
    - 1797
    - 909
    - 913
    - textualorganization
  - !!python/tuple
    - 913
    - 934
    - 934
    - 1797
    - list
  - !!python/tuple
    - 934
    - 1797
    - 913
    - 934
    - list
  - !!python/tuple
    - 934
    - 967
    - 967
    - 981
    - elaboration
  - !!python/tuple
    - 934
    - 981
    - 981
    - 1016
    - elaboration
  - !!python/tuple
    - 981
    - 984
    - 984
    - 1016
    - elaboration
  - !!python/tuple
    - 984
    - 992
    - 992
    - 1016
    - condition
  - !!python/tuple
    - 999
    - 1016
    - 992
    - 999
    - condition
  - !!python/tuple
    - 999
    - 1001
    - 1001
    - 1016
    - same_unit
  - !!python/tuple
    - 1001
    - 1016
    - 999
    - 1001
    - same_unit
  - !!python/tuple
    - 934
    - 1016
    - 1016
    - 1797
    - elaboration
  - !!python/tuple
    - 1016
    - 1040
    - 1040
    - 1797
    - topic
  - !!python/tuple
    - 1016
    - 1026
    - 1026
    - 1040
    - list
  - !!python/tuple
    - 1026
    - 1040
    - 1016
    - 1026
    - list
  - !!python/tuple
    - 1026
    - 1027
    - 1027
    - 1040
    - purpose
  - !!python/tuple
    - 1027
    - 1033
    - 1033
    - 1040
    - elaboration
  - !!python/tuple
    - 1040
    - 1797
    - 1016
    - 1040
    - topic
  - !!python/tuple
    - 1040
    - 1068
    - 1068
    - 1080
    - elaboration
  - !!python/tuple
    - 1040
    - 1080
    - 1080
    - 1797
    - elaboration
  - !!python/tuple
    - 1080
    - 1099
    - 1099
    - 1108
    - elaboration
  - !!python/tuple
    - 1080
    - 1108
    - 1108
    - 1797
    - explanation
  - !!python/tuple
    - 1108
    - 1140
    - 1140
    - 1797
    - topic
  - !!python/tuple
    - 1110
    - 1140
    - 1108
    - 1110
    - attribution
  - !!python/tuple
    - 1112
    - 1140
    - 1110
    - 1112
    - attribution
  - !!python/tuple
    - 1112
    - 1113
    - 1113
    - 1140
    - circumstance
  - !!python/tuple
    - 1113
    - 1123
    - 1123
    - 1140
    - sequence
  - !!python/tuple
    - 1123
    - 1140
    - 1113
    - 1123
    - sequence
  - !!python/tuple
    - 1140
    - 1797
    - 1108
    - 1140
    - topic
  - !!python/tuple
    - 1140
    - 1153
    - 1153
    - 1797
    - list
  - !!python/tuple
    - 1153
    - 1797
    - 1140
    - 1153
    - list
  - !!python/tuple
    - 1153
    - 1169
    - 1169
    - 1797
    - list
  - !!python/tuple
    - 1169
    - 1797
    - 1153
    - 1169
    - list
  - !!python/tuple
    - 1169
    - 1242
    - 1242
    - 1797
    - topic
  - !!python/tuple
    - 1169
    - 1183
    - 1183
    - 1193
    - elaboration
  - !!python/tuple
    - 1169
    - 1193
    - 1193
    - 1242
    - elaboration
  - !!python/tuple
    - 1193
    - 1200
    - 1200
    - 1220
    - same_unit
  - !!python/tuple
    - 1193
    - 1196
    - 1196
    - 1200
    - purpose
  - !!python/tuple
    - 1200
    - 1220
    - 1193
    - 1200
    - same_unit
  - !!python/tuple
    - 1200
    - 1204
    - 1204
    - 1220
    - elaboration
  - !!python/tuple
    - 1193
    - 1220
    - 1220
    - 1242
    - elaboration
  - !!python/tuple
    - 1220
    - 1225
    - 1225
    - 1242
    - purpose
  - !!python/tuple
    - 1225
    - 1227
    - 1227
    - 1242
    - condition
  - !!python/tuple
    - 1227
    - 1237
    - 1237
    - 1242
    - elaboration
  - !!python/tuple
    - 1242
    - 1797
    - 1169
    - 1242
    - topic
  - !!python/tuple
    - 1242
    - 1256
    - 1256
    - 1797
    - list
  - !!python/tuple
    - 1256
    - 1797
    - 1242
    - 1256
    - list
  - !!python/tuple
    - 1269
    - 1797
    - 1256
    - 1269
    - antithesis
  - !!python/tuple
    - 1256
    - 1261
    - 1261
    - 1269
    - elaboration
  - !!python/tuple
    - 1261
    - 1263
    - 1263
    - 1269
    - elaboration
  - !!python/tuple
    - 1269
    - 1286
    - 1286
    - 1797
    - topic
  - !!python/tuple
    - 1276
    - 1286
    - 1269
    - 1276
    - attribution
  - !!python/tuple
    - 1276
    - 1282
    - 1282
    - 1286
    - circumstance
  - !!python/tuple
    - 1286
    - 1797
    - 1269
    - 1286
    - topic
  - !!python/tuple
    - 1286
    - 1306
    - 1306
    - 1326
    - elaboration
  - !!python/tuple
    - 1286
    - 1326
    - 1326
    - 1797
    - elaboration
  - !!python/tuple
    - 1326
    - 1362
    - 1362
    - 1797
    - contrast
  - !!python/tuple
    - 1328
    - 1359
    - 1326
    - 1328
    - attribution
  - !!python/tuple
    - 1328
    - 1353
    - 1353
    - 1359
    - elaboration
  - !!python/tuple
    - 1326
    - 1359
    - 1359
    - 1362
    - elaboration
  - !!python/tuple
    - 1359
    - 1360
    - 1360
    - 1362
    - elaboration
  - !!python/tuple
    - 1362
    - 1797
    - 1326
    - 1362
    - contrast
  - !!python/tuple
    - 1362
    - 1375
    - 1375
    - 1797
    - explanation
  - !!python/tuple
    - 1375
    - 1409
    - 1409
    - 1797
    - textualorganization
  - !!python/tuple
    - 1380
    - 1409
    - 1375
    - 1380
    - attribution
  - !!python/tuple
    - 1401
    - 1409
    - 1380
    - 1401
    - condition
  - !!python/tuple
    - 1380
    - 1385
    - 1385
    - 1401
    - elaboration
  - !!python/tuple
    - 1409
    - 1797
    - 1375
    - 1409
    - textualorganization
  - !!python/tuple
    - 1409
    - 1425
    - 1425
    - 1435
    - elaboration
  - !!python/tuple
    - 1425
    - 1427
    - 1427
    - 1435
    - purpose
  - !!python/tuple
    - 1409
    - 1435
    - 1435
    - 1797
    - elaboration
  - !!python/tuple
    - 1435
    - 1448
    - 1448
    - 1797
    - list
  - !!python/tuple
    - 1438
    - 1448
    - 1435
    - 1438
    - attribution
  - !!python/tuple
    - 1448
    - 1797
    - 1435
    - 1448
    - list
  - !!python/tuple
    - 1459
    - 1485
    - 1448
    - 1459
    - attribution
  - !!python/tuple
    - 1448
    - 1458
    - 1458
    - 1459
    - same_unit
  - !!python/tuple
    - 1448
    - 1451
    - 1451
    - 1458
    - elaboration
  - !!python/tuple
    - 1458
    - 1459
    - 1448
    - 1458
    - same_unit
  - !!python/tuple
    - 1459
    - 1477
    - 1477
    - 1479
    - elaboration
  - !!python/tuple
    - 1459
    - 1479
    - 1479
    - 1485
    - elaboration
  - !!python/tuple
    - 1479
    - 1484
    - 1484
    - 1485
    - same_unit
  - !!python/tuple
    - 1479
    - 1480
    - 1480
    - 1484
    - elaboration
  - !!python/tuple
    - 1484
    - 1485
    - 1479
    - 1484
    - same_unit
  - !!python/tuple
    - 1448
    - 1485
    - 1485
    - 1797
    - elaboration
  - !!python/tuple
    - 1485
    - 1491
    - 1491
    - 1494
    - purpose
  - !!python/tuple
    - 1485
    - 1494
    - 1494
    - 1505
    - elaboration
  - !!python/tuple
    - 1485
    - 1505
    - 1505
    - 1529
    - elaboration
  - !!python/tuple
    - 1505
    - 1521
    - 1521
    - 1529
    - condition
  - !!python/tuple
    - 1485
    - 1529
    - 1529
    - 1797
    - explanation
  - !!python/tuple
    - 1560
    - 1797
    - 1529
    - 1560
    - circumstance
  - !!python/tuple
    - 1536
    - 1560
    - 1529
    - 1536
    - circumstance
  - !!python/tuple
    - 1562
    - 1571
    - 1560
    - 1562
    - attribution
  - !!python/tuple
    - 1560
    - 1571
    - 1571
    - 1579
    - elaboration
  - !!python/tuple
    - 1560
    - 1579
    - 1579
    - 1797
    - circumstance
  - !!python/tuple
    - 1579
    - 1612
    - 1612
    - 1649
    - elaboration
  - !!python/tuple
    - 1612
    - 1622
    - 1622
    - 1628
    - temporal
  - !!python/tuple
    - 1612
    - 1628
    - 1628
    - 1649
    - elaboration
  - !!python/tuple
    - 1579
    - 1649
    - 1649
    - 1797
    - elaboration
  - !!python/tuple
    - 1649
    - 1680
    - 1680
    - 1682
    - elaboration
  - !!python/tuple
    - 1649
    - 1682
    - 1682
    - 1694
    - elaboration
  - !!python/tuple
    - 1682
    - 1683
    - 1683
    - 1694
    - elaboration
  - !!python/tuple
    - 1649
    - 1694
    - 1694
    - 1797
    - circumstance
  - !!python/tuple
    - 1694
    - 1795
    - 1795
    - 1797
    - list
  - !!python/tuple
    - 1694
    - 1705
    - 1705
    - 1707
    - elaboration
  - !!python/tuple
    - 1694
    - 1707
    - 1707
    - 1718
    - elaboration
  - !!python/tuple
    - 1707
    - 1708
    - 1708
    - 1718
    - elaboration
  - !!python/tuple
    - 1708
    - 1712
    - 1712
    - 1718
    - elaboration
  - !!python/tuple
    - 1694
    - 1718
    - 1718
    - 1795
    - elaboration
  - !!python/tuple
    - 1718
    - 1726
    - 1726
    - 1763
    - elaboration
  - !!python/tuple
    - 1726
    - 1736
    - 1736
    - 1763
    - same_unit
  - !!python/tuple
    - 1726
    - 1729
    - 1729
    - 1736
    - elaboration
  - !!python/tuple
    - 1736
    - 1763
    - 1726
    - 1736
    - same_unit
  - !!python/tuple
    - 1736
    - 1753
    - 1753
    - 1763
    - same_unit
  - !!python/tuple
    - 1736
    - 1737
    - 1737
    - 1753
    - elaboration
  - !!python/tuple
    - 1753
    - 1763
    - 1736
    - 1753
    - same_unit
  - !!python/tuple
    - 1757
    - 1763
    - 1753
    - 1757
    - attribution
  - !!python/tuple
    - 1762
    - 1763
    - 1757
    - 1762
    - attribution
  - !!python/tuple
    - 1718
    - 1763
    - 1763
    - 1795
    - elaboration
  - !!python/tuple
    - 1763
    - 1766
    - 1766
    - 1795
    - purpose
  - !!python/tuple
    - 1766
    - 1782
    - 1782
    - 1795
    - elaboration
  - !!python/tuple
    - 1782
    - 1790
    - 1790
    - 1795
    - elaboration
  - !!python/tuple
    - 1795
    - 1797
    - 1694
    - 1795
    - list
  tokens:
  - Dear
  - reviewers
  - We
  - would
  - like
  - to
  - thank
  - the
  - reviewers
  - for
  - taking
  - the
  - time
  - to
  - leave
  - thoughtful
  - reviews
  - .
  - Given
  - these
  - feedbacks
  - ','
  - we
  - have
  - significantly
  - improved
  - the
  - draft
  - and
  - hope
  - the
  - reviewers
  - will
  - take
  - this
  - into
  - account
  - when
  - assessing
  - the
  - final
  - scores
  - .
  - We
  - appreciate
  - the
  - reviewers
  - for
  - the
  - time
  - and
  - effort
  - they
  - dedicated
  - to
  - our
  - paper
  - .
  - Please
  - find
  - individual
  - replies
  - to
  - each
  - of
  - the
  - reviews
  - in
  - the
  - respective
  - threads
  - .
  - Based
  - 'on'
  - the
  - reviewers
  - ''''
  - reviews
  - and
  - the
  - comment
  - by
  - Ian
  - Osband
  - we
  - revised
  - the
  - draft
  - and
  - uploaded
  - the
  - new
  - version
  - .
  - Thank
  - you
  - to
  - the
  - authors
  - for
  - trying
  - to
  - improve
  - the
  - paper
  - and
  - analysis
  - .
  - Some
  - parts
  - of
  - the
  - paper
  - have
  - improved
  - ','
  - but
  - there
  - are
  - still
  - many
  - parts
  - that
  - are
  - difficult
  - to
  - follow
  - .
  - -LRB-
  - e.g.
  - confidence
  - set
  - C_t
  - is
  - not
  - introduced
  - before
  - Appendix
  - ','
  - gamma
  - discount
  - appears
  - in
  - undiscounted
  - analysis
  - -RRB-
  - Rather
  - than
  - get
  - bogged
  - down
  - in
  - small
  - details
  - I
  - want
  - to
  - highlight
  - at
  - least
  - one
  - fundamental
  - error
  - in
  - the
  - analysis
  - of
  - PSRL
  - -LRB-
  - Theorem
  - '1'
  - -RRB-
  - .
  - Stating
  - this
  - clearly
  - should
  - be
  - enough
  - to
  - convince
  - a
  - third
  - party
  - that
  - this
  - analysis
  - needs
  - more
  - work
  - .
  - '##'
  - Main
  - theorem
  - claim
  - +
  - prior/posterior
  - disconnect
  - The
  - authors
  - claim
  - '``'
  - the
  - first
  - model-free
  - theoretical
  - guarantee
  - for
  - continuous
  - state-action
  - space
  - MDP
  - ','
  - beyond
  - the
  - tabular
  - setting
  - ''''''
  - .
  - This
  - means
  - that
  - ','
  - the
  - PSRL
  - algorithm
  - of
  - Theorem
  - '1'
  - ','
  - should
  - maintain
  - its
  - posterior
  - over
  - $
  - w
  - $
  - without
  - an
  - underlying
  - model
  - .
  - The
  - description
  - of
  - the
  - PSRL
  - algorithm
  - -LRB-
  - and
  - associated
  - regret
  - bound
  - -RRB-
  - is
  - not
  - tied
  - to
  - any
  - specific
  - choice
  - of
  - prior/likelihood
  - format
  - .
  - This
  - statement
  - is
  - itself
  - unclear
  - ','
  - do
  - the
  - authors
  - mean
  - this
  - result
  - to
  - hold
  - for
  - all
  - choices
  - of
  - prior/likelihood
  - ','
  - or
  - specifically
  - using
  - a
  - Gaussian
  - form
  - for
  - PSRL
  - updates
  - '?'
  - Either
  - way
  - ','
  - the
  - application
  - of
  - the
  - '``'
  - posterior
  - sampling
  - lemma
  - ''''''
  - 'on'
  - page
  - '23'
  - -LRB-
  - that
  - conditioned
  - 'on'
  - any
  - data
  - H_t
  - ','
  - the
  - sampled
  - posterior
  - is
  - identically
  - distributed
  - to
  - the
  - optimal
  - value
  - -RRB-
  - is
  - inconsistent
  - .
  - There
  - are
  - two
  - main
  - options
  - here
  - ':'
  - a
  - '-'
  - If
  - their
  - '``'
  - PSRL
  - ''''''
  - is
  - using
  - a
  - model-free
  - Gaussian
  - form
  - of
  - the
  - optimal
  - value
  - -LRB-
  - per
  - Gaussian
  - linear
  - bandits
  - -RRB-
  - ','
  - then
  - this
  - is
  - not
  - the
  - correct
  - posterior
  - for
  - the
  - Bayesian
  - decision
  - rule
  - 'on'
  - the
  - optimal
  - policy
  - for
  - all
  - underlying
  - MDPs
  - .
  - To
  - see
  - this
  - ','
  - note
  - that
  - the
  - '*'
  - optimal
  - '*'
  - policy
  - includes
  - a
  - max
  - operator
  - over
  - actions
  - ','
  - this
  - breaks
  - Gaussian
  - conjugacy
  - even
  - if
  - the
  - rewards
  - are
  - gaussian
  - '...'
  - this
  - is
  - the
  - main
  - difficulty
  - in
  - prior
  - analyses
  - of
  - RLSVI
  - -LRB-
  - e.g.
  - https://arxiv.org/abs/1402.0635
  - -RRB-
  - .
  - b
  - '-'
  - If
  - PSRL
  - is
  - using
  - the
  - correct
  - form
  - of
  - the
  - posterior
  - ','
  - then
  - it
  - must
  - be
  - using
  - the
  - information
  - about
  - the
  - likelihood
  - of
  - the
  - '``'
  - noise
  - process
  - ''''''
  - $
  - \
  - nu
  - $
  - and
  - thus
  - this
  - is
  - not
  - a
  - model-free
  - algorithm
  - .
  - This
  - setting
  - is
  - most
  - similar
  - to
  - prior
  - work
  - 'on'
  - PSRL
  - with
  - generalization
  - -LRB-
  - e.g.
  - https://arxiv.org/abs/1406.1853
  - ','
  - https://arxiv.org/abs/1709.04047
  - -RRB-
  - To
  - remedy
  - this
  - ','
  - the
  - authors
  - need
  - to
  - be
  - much
  - more
  - clear
  - about
  - what
  - prior/posterior
  - sampling
  - procedure
  - their
  - PSRL
  - algorithm
  - uses
  - '*'
  - and
  - '*'
  - the
  - prior/likelihood
  - of
  - tasks
  - against
  - which
  - they
  - are
  - assessing
  - their
  - algorithm
  - .
  - If
  - the
  - two
  - are
  - the
  - same
  - ','
  - then
  - they
  - need
  - to
  - be
  - clear
  - 'on'
  - '*'
  - how
  - '*'
  - PSRL
  - is
  - able
  - to
  - be
  - model-free
  - algorithm
  - and
  - still
  - match
  - the
  - exact
  - posterior
  - of
  - the
  - underlying
  - MDP
  - given
  - any
  - possible
  - data
  - H_t
  - .
  - This
  - is
  - an
  - interesting
  - line
  - of
  - research
  - ','
  - and
  - it
  - would
  - be
  - impactful
  - to
  - get
  - this
  - answer
  - right
  - '!'
  - Unfortunately
  - ','
  - I
  - do
  - not
  - think
  - this
  - proof
  - is
  - correct
  - and
  - so
  - it
  - should
  - not
  - be
  - accepted
  - .
  - Dear
  - Ian
  - ','
  - We
  - also
  - like
  - to
  - thank
  - you
  - for
  - the
  - time
  - you
  - dedicated
  - and
  - kindly
  - read
  - the
  - revised
  - version
  - of
  - our
  - paper
  - .
  - As
  - you
  - mentioned
  - ','
  - upon
  - you
  - and
  - our
  - four
  - reviewers
  - ''''
  - thoughtful
  - comments
  - ','
  - we
  - improved
  - the
  - clarity
  - of
  - the
  - presentation
  - .
  - We
  - believe
  - that
  - the
  - merit
  - of
  - openreview
  - helps
  - authors
  - to
  - deliver
  - polished
  - and
  - influential
  - research
  - contributions
  - .
  - With
  - this
  - regard
  - ','
  - we
  - would
  - be
  - grateful
  - to
  - you
  - if
  - you
  - could
  - leave
  - a
  - comment
  - for
  - our
  - AnonReviewer1
  - regarding
  - drop-out
  - and
  - its
  - correspondence
  - to
  - Thompson
  - sampling
  - .
  - We
  - already
  - referred
  - our
  - AnonReviewer1
  - to
  - the
  - discussion
  - in
  - Appendix
  - A
  - of
  - your
  - BootstrapDQN
  - paper
  - .
  - Moreover
  - ','
  - we
  - made
  - an
  - additional
  - empirical
  - study
  - 'on'
  - four
  - Atari
  - games
  - to
  - show
  - the
  - deficiency
  - of
  - dropout
  - in
  - providing
  - reasonable
  - exploration
  - and
  - exploitation
  - tradeoff
  - .
  - We
  - would
  - appreciate
  - it
  - if
  - you
  - could
  - take
  - a
  - time
  - and
  - leave
  - a
  - comment
  - in
  - the
  - corresponding
  - thread
  - .
  - Regarding
  - the
  - confidence
  - set
  - C_t
  - ':'
  - The
  - confidence
  - C_t
  - is
  - mentioned
  - in
  - section
  - '4'
  - .
  - Regarding
  - the
  - discount
  - factor
  - ':'
  - Both
  - theorem
  - '1'
  - and
  - theorem
  - '2'
  - hold
  - for
  - any
  - discount
  - factor
  - '0'
  - <
  - '='
  - \
  - gamma
  - <
  - '='
  - '1'
  - .
  - We
  - get
  - a
  - tight
  - bound
  - if
  - we
  - replace
  - \
  - sqrt
  - -LRB-
  - H
  - -RRB-
  - with
  - a
  - smaller
  - quantity
  - '|'
  - '|'
  - '1'
  - ','
  - \
  - gamma
  - ','
  - gamma
  - ^
  - '2'
  - ','
  - '...'
  - ','
  - \
  - gamma
  - ^
  - -LRB-
  - H-1
  - -RRB-
  - '|'
  - '|'
  - _
  - '2'
  - .
  - We
  - addressed
  - this
  - in
  - terms
  - of
  - a
  - remark
  - in
  - the
  - latest
  - version
  - .
  - Regarding
  - the
  - choices
  - of
  - prior/likelihood
  - ':'
  - We
  - apologize
  - that
  - the
  - choices
  - of
  - prior
  - and
  - likelihood
  - were
  - not
  - clear
  - from
  - the
  - main
  - text
  - .
  - We
  - would
  - like
  - to
  - restate
  - that
  - we
  - do
  - not
  - specify
  - the
  - choices
  - of
  - prior
  - and
  - likelihood
  - .
  - They
  - can
  - be
  - anything
  - as
  - long
  - as
  - they
  - satisfy
  - the
  - set
  - of
  - assumptions
  - 'on'
  - page
  - '6'
  - ','
  - e.g.
  - ','
  - sub-Gaussianity
  - .
  - Regarding
  - the
  - inconsistency
  - ':'
  - Conditioned
  - 'on'
  - any
  - data
  - history
  - H_t
  - ','
  - the
  - posterior
  - over
  - w
  - is
  - identical
  - to
  - the
  - posterior
  - of
  - the
  - optimal
  - parameter
  - .
  - Similar
  - theoretical
  - justification
  - is
  - also
  - deployed
  - in
  - Russo
  - et
  - al.
  - '2014'
  - ','
  - '``'
  - Learning
  - to
  - Optimize
  - Via
  - Posterior
  - Sampling
  - ''''''
  - page
  - '10'
  - ','
  - as
  - well
  - as
  - many
  - of
  - your
  - papers
  - ','
  - e.g.
  - ','
  - '``'
  - -LRB-
  - More
  - -RRB-
  - Efficient
  - Reinforcement
  - Learning
  - via
  - Posterior
  - Sampling
  - ''''''
  - lemma
  - '1'
  - .
  - Regarding
  - your
  - statement
  - 'on'
  - '``'
  - this
  - is
  - not
  - a
  - model-free
  - algorithm
  - if
  - we
  - know
  - the
  - likelihood
  - ''''''
  - ':'
  - Theoretically
  - ','
  - given
  - a
  - w
  - ','
  - the
  - knowledge
  - of
  - the
  - likelihood
  - does
  - not
  - determine
  - a
  - model
  - .
  - These
  - algorithms
  - ','
  - also
  - neither
  - require
  - to
  - construct
  - a
  - model
  - nor
  - to
  - store
  - any
  - MDP
  - model
  - parameters
  - -LRB-
  - e.g.
  - ','
  - transition
  - kernel
  - -RRB-
  - .
  - In
  - model-based
  - PSRL
  - ','
  - we
  - generally
  - specify
  - the
  - problem
  - with
  - a
  - prior
  - over
  - MDP
  - models
  - ','
  - as
  - well
  - as
  - a
  - likelihood
  - over
  - state
  - transitions
  - and
  - reward
  - processes
  - .
  - These
  - quantities
  - are
  - given
  - and
  - known
  - in
  - the
  - model-based
  - PSRL
  - framework
  - .
  - Consequently
  - ','
  - in
  - model-free
  - PSRL
  - we
  - specify
  - the
  - problem
  - with
  - a
  - prior
  - over
  - Q
  - and
  - likelihood
  - over
  - the
  - return
  - where
  - similarly
  - these
  - quantities
  - are
  - given
  - and
  - known
  - .
  - We
  - agree
  - with
  - you
  - that
  - when
  - the
  - prior
  - and
  - the
  - likelihood
  - functions
  - are
  - arbitrary
  - ','
  - then
  - computing
  - the
  - posterior
  - ','
  - as
  - well
  - as
  - sampling
  - from
  - it
  - ','
  - can
  - be
  - computationally
  - hard
  - .
  - As
  - you
  - know
  - ','
  - this
  - is
  - a
  - principled
  - issue
  - with
  - Bayesian
  - methods
  - .
  - It
  - is
  - also
  - an
  - unsolved
  - issue
  - for
  - model-based
  - methods
  - ','
  - e.g.
  - ','
  - in
  - continuous
  - MDPs
  - .
  - While
  - we
  - are
  - excited
  - about
  - this
  - line
  - of
  - research
  - ','
  - we
  - left
  - the
  - study
  - of
  - relaxing
  - this
  - computation
  - complexity
  - for
  - the
  - future
  - work
  - .
  - We
  - would
  - like
  - to
  - thank
  - you
  - again
  - for
  - taking
  - the
  - time
  - to
  - leave
  - thoughtful
  - comments
  - and
  - we
  - appreciate
  - your
  - positive
  - assessment
  - of
  - this
  - line
  - of
  - research
  - .
  - We
  - would
  - be
  - also
  - grateful
  - to
  - you
  - if
  - you
  - could
  - leave
  - a
  - comment
  - 'on'
  - our
  - AnonReviewer1
  - review
  - regarding
  - the
  - drop-out
  - discussion
  - .
  - Sincerely
  - ','
  - Authors
  - This
  - issue
  - of
  - '``'
  - exact
  - ''''''
  - posterior
  - inference
  - is
  - crucial
  - .
  - 'Yes'
  - ','
  - in
  - previous
  - papers
  - analysing
  - PSRL
  - they
  - assume
  - exact
  - posterior
  - inference
  - .
  - However
  - ','
  - those
  - papers
  - do
  - not
  - claim
  - that
  - PSRL
  - is
  - a
  - model-free
  - algorithm
  - when
  - doing
  - this
  - .
  - At
  - this
  - point
  - ','
  - the
  - analysis
  - of
  - this
  - '``'
  - model-free
  - ''''''
  - PSRL
  - is
  - quite
  - divorced
  - from
  - your
  - algorithm
  - Bayesian
  - DQN
  - -LRB-
  - which
  - is
  - the
  - model-free
  - RLSVI
  - ','
  - but
  - only
  - applying
  - randomization
  - to
  - the
  - final
  - layer
  - of
  - a
  - DQN
  - -RRB-
  - .
  - I
  - think
  - it
  - would
  - be
  - better
  - to
  - separate
  - these
  - contributions
  - into
  - two
  - separate
  - papers
  - '...'
  - At
  - the
  - moment
  - ','
  - there
  - is
  - an
  - implication
  - that
  - the
  - two
  - algorithms
  - -LRB-
  - model-free
  - PSRL
  - and
  - BDQN
  - -RRB-
  - are
  - conflated
  - '...'
  - but
  - this
  - is
  - confusing
  - because
  - actually
  - they
  - are
  - not
  - the
  - same
  - thing
  - .
  - It
  - '''s'
  - also
  - not
  - clear
  - if
  - there
  - are
  - any
  - examples
  - where
  - you
  - can
  - do
  - this
  - '``'
  - model-free
  - PSRL
  - ''''''
  - exactly
  - ','
  - even
  - with
  - infinite
  - compute
  - ','
  - without
  - an
  - underlying
  - model
  - of
  - the
  - MDP
  - '?'
  - The
  - issue
  - is
  - that
  - ','
  - in
  - order
  - to
  - have
  - the
  - '*'
  - exact
  - '*'
  - prior/likelihood
  - updates
  - ','
  - you
  - need
  - to
  - take
  - into
  - account
  - the
  - max_a
  - dynamics
  - .
  - I
  - also
  - think
  - you
  - should
  - omit
  - the
  - '``'
  - gamma
  - ''''''
  - stuff
  - entirely
  - .
  - Regret
  - bounds
  - O
  - -LRB-
  - \
  - sqrt
  - -LCB-
  - T
  - -RCB-
  - -RRB-
  - make
  - absolutely
  - 'no'
  - sense
  - in
  - a
  - discounted
  - setting
  - '...'
  - for
  - any
  - discount
  - <
  - '1'
  - the
  - regret
  - is
  - bounded
  - O
  - -LRB-
  - '1'
  - /
  - -LRB-
  - 1-gamma
  - -RRB-
  - -RRB-
  - .
  - Dear
  - Ian
  - ','
  - We
  - would
  - like
  - to
  - thank
  - you
  - for
  - kindly
  - leaving
  - a
  - comment
  - 'on'
  - our
  - AnonReviewer1
  - '''s'
  - thread
  - .
  - Regarding
  - the
  - analysis
  - of
  - model-free
  - PSRL
  - ;
  - BDQN
  - up
  - to
  - some
  - modification
  - reduces
  - to
  - PSRL
  - algorithm
  - if
  - the
  - prior
  - and
  - posterior
  - are
  - Gaussian
  - .
  - As
  - we
  - mentioned
  - in
  - the
  - paper
  - ','
  - since
  - maintaining
  - the
  - posterior
  - distribution
  - can
  - be
  - computationally
  - intractable
  - ','
  - in
  - our
  - empirical
  - study
  - ','
  - we
  - approximate
  - the
  - posterior
  - with
  - a
  - Gaussian
  - distribution
  - .
  - We
  - apologize
  - if
  - it
  - is
  - not
  - clear
  - from
  - the
  - paper
  - .
  - We
  - will
  - emphasize
  - more
  - 'on'
  - this
  - statement
  - .
  - As
  - we
  - mentioned
  - before
  - ','
  - in
  - model-based
  - PSRL
  - ','
  - we
  - generally
  - specify
  - the
  - problem
  - with
  - a
  - prior
  - over
  - MDP
  - models
  - ','
  - as
  - well
  - as
  - a
  - likelihood
  - over
  - state
  - transitions
  - and
  - reward
  - processes
  - .
  - As
  - you
  - also
  - agreed
  - ','
  - we
  - are
  - given
  - these
  - quantities
  - before
  - interacting
  - with
  - the
  - environment
  - .
  - Consequently
  - ','
  - in
  - model-free
  - PSRL
  - ','
  - we
  - specify
  - the
  - problem
  - with
  - a
  - prior
  - over
  - Q
  - and
  - likelihood
  - over
  - the
  - return
  - .
  - Similarly
  - ','
  - we
  - are
  - given
  - these
  - quantities
  - before
  - interacting
  - with
  - the
  - environment
  - to
  - pursue
  - PSRL
  - Regarding
  - the
  - discount
  - factor
  - \
  - gamma
  - ;
  - the
  - per
  - episode
  - regret
  - is
  - bounded
  - above
  - with
  - O
  - -LRB-
  - '1'
  - /
  - -LRB-
  - 1-gamma
  - -RRB-
  - -RRB-
  - ','
  - but
  - not
  - the
  - overall
  - regret
  - .
  - Following
  - your
  - statement
  - ','
  - the
  - regret
  - is
  - upper
  - bounded
  - as
  - O
  - -LRB-
  - T
  - /
  - -LRB-
  - 1-gamma
  - -RRB-
  - -RRB-
  - which
  - is
  - linear
  - in
  - T
  - .
  - We
  - derived
  - a
  - sublinear
  - regret
  - of
  - \
  - tild
  - -LCB-
  - O
  - -RCB-
  - -LRB-
  - \
  - sqrt
  - -LCB-
  - T
  - -RCB-
  - '|'
  - '|'
  - -LRB-
  - '1'
  - ','
  - \
  - gamma
  - ','
  - \
  - gamma
  - ^
  - '2'
  - ','
  - '...'
  - ','
  - \
  - gamma
  - ^
  - -LCB-
  - H-1
  - -RCB-
  - -RRB-
  - '|'
  - '|'
  - _
  - '2'
  - -RRB-
  - .
  - We
  - would
  - like
  - to
  - appreciate
  - the
  - thoughtful
  - comments
  - 'on'
  - our
  - paper
  - and
  - also
  - thank
  - you
  - for
  - taking
  - the
  - time
  - to
  - leave
  - a
  - comment
  - 'on'
  - our
  - AnonReviewer1
  - review
  - regarding
  - the
  - drop-out
  - discussion
  - .
  - Sincerely
  - Authors
- comment_id: B1e-bBgiCX
  rels:
  - !!python/tuple
    - 0
    - 26
    - 26
    - 675
    - elaboration
  - !!python/tuple
    - 26
    - 34
    - 34
    - 59
    - purpose
  - !!python/tuple
    - 34
    - 40
    - 40
    - 59
    - elaboration
  - !!python/tuple
    - 40
    - 54
    - 54
    - 59
    - elaboration
  - !!python/tuple
    - 26
    - 59
    - 59
    - 675
    - elaboration
  - !!python/tuple
    - 59
    - 90
    - 90
    - 675
    - list
  - !!python/tuple
    - 59
    - 82
    - 82
    - 90
    - elaboration
  - !!python/tuple
    - 90
    - 675
    - 59
    - 90
    - list
  - !!python/tuple
    - 99
    - 112
    - 90
    - 99
    - attribution
  - !!python/tuple
    - 99
    - 100
    - 100
    - 112
    - elaboration
  - !!python/tuple
    - 90
    - 112
    - 112
    - 675
    - elaboration
  - !!python/tuple
    - 138
    - 197
    - 112
    - 138
    - antithesis
  - !!python/tuple
    - 138
    - 152
    - 152
    - 197
    - elaboration
  - !!python/tuple
    - 112
    - 197
    - 197
    - 238
    - elaboration
  - !!python/tuple
    - 202
    - 238
    - 197
    - 202
    - attribution
  - !!python/tuple
    - 112
    - 238
    - 238
    - 675
    - elaboration
  - !!python/tuple
    - 238
    - 250
    - 250
    - 675
    - elaboration
  - !!python/tuple
    - 250
    - 257
    - 257
    - 263
    - list
  - !!python/tuple
    - 257
    - 263
    - 250
    - 257
    - list
  - !!python/tuple
    - 250
    - 263
    - 263
    - 675
    - purpose
  - !!python/tuple
    - 263
    - 282
    - 282
    - 675
    - elaboration
  - !!python/tuple
    - 282
    - 316
    - 316
    - 675
    - elaboration
  - !!python/tuple
    - 325
    - 341
    - 316
    - 325
    - attribution
  - !!python/tuple
    - 325
    - 333
    - 333
    - 341
    - contrast
  - !!python/tuple
    - 333
    - 341
    - 325
    - 333
    - contrast
  - !!python/tuple
    - 316
    - 341
    - 341
    - 675
    - elaboration
  - !!python/tuple
    - 341
    - 351
    - 351
    - 359
    - elaboration
  - !!python/tuple
    - 341
    - 359
    - 359
    - 675
    - elaboration
  - !!python/tuple
    - 364
    - 379
    - 359
    - 364
    - attribution
  - !!python/tuple
    - 364
    - 369
    - 369
    - 379
    - elaboration
  - !!python/tuple
    - 359
    - 379
    - 379
    - 675
    - elaboration
  - !!python/tuple
    - 379
    - 423
    - 423
    - 675
    - topic
  - !!python/tuple
    - 379
    - 383
    - 383
    - 396
    - elaboration
  - !!python/tuple
    - 379
    - 396
    - 396
    - 423
    - concession
  - !!python/tuple
    - 414
    - 423
    - 396
    - 414
    - concession
  - !!python/tuple
    - 396
    - 404
    - 404
    - 414
    - elaboration
  - !!python/tuple
    - 423
    - 675
    - 379
    - 423
    - topic
  - !!python/tuple
    - 458
    - 675
    - 423
    - 458
    - concession
  - !!python/tuple
    - 423
    - 445
    - 445
    - 458
    - elaboration
  - !!python/tuple
    - 458
    - 478
    - 478
    - 503
    - same_unit
  - !!python/tuple
    - 458
    - 470
    - 470
    - 478
    - elaboration
  - !!python/tuple
    - 478
    - 503
    - 458
    - 478
    - same_unit
  - !!python/tuple
    - 491
    - 503
    - 478
    - 491
    - attribution
  - !!python/tuple
    - 491
    - 496
    - 496
    - 503
    - elaboration
  - !!python/tuple
    - 458
    - 503
    - 503
    - 675
    - elaboration
  - !!python/tuple
    - 503
    - 507
    - 507
    - 514
    - elaboration
  - !!python/tuple
    - 503
    - 514
    - 514
    - 675
    - elaboration
  - !!python/tuple
    - 525
    - 544
    - 514
    - 525
    - attribution
  - !!python/tuple
    - 514
    - 518
    - 518
    - 525
    - elaboration
  - !!python/tuple
    - 518
    - 522
    - 522
    - 525
    - elaboration
  - !!python/tuple
    - 514
    - 544
    - 544
    - 570
    - elaboration
  - !!python/tuple
    - 544
    - 561
    - 561
    - 570
    - list
  - !!python/tuple
    - 544
    - 557
    - 557
    - 561
    - elaboration
  - !!python/tuple
    - 561
    - 570
    - 544
    - 561
    - list
  - !!python/tuple
    - 514
    - 570
    - 570
    - 675
    - elaboration
  - !!python/tuple
    - 576
    - 610
    - 570
    - 576
    - attribution
  - !!python/tuple
    - 577
    - 610
    - 576
    - 577
    - attribution
  - !!python/tuple
    - 570
    - 610
    - 610
    - 675
    - elaboration
  - !!python/tuple
    - 610
    - 632
    - 632
    - 644
    - elaboration
  - !!python/tuple
    - 632
    - 635
    - 635
    - 644
    - circumstance
  - !!python/tuple
    - 610
    - 644
    - 644
    - 675
    - elaboration
  - !!python/tuple
    - 644
    - 651
    - 651
    - 675
    - elaboration
  - !!python/tuple
    - 651
    - 662
    - 662
    - 675
    - same_unit
  - !!python/tuple
    - 651
    - 656
    - 656
    - 662
    - elaboration
  - !!python/tuple
    - 662
    - 675
    - 651
    - 662
    - same_unit
  - !!python/tuple
    - 662
    - 668
    - 668
    - 675
    - elaboration
  - !!python/tuple
    - 668
    - 670
    - 670
    - 675
    - circumstance
  tokens:
  - This
  - paper
  - considers
  - the
  - issue
  - of
  - distribution
  - mismatch
  - between
  - the
  - input
  - data
  - used
  - for
  - training
  - generative
  - models
  - and
  - the
  - new
  - data
  - for
  - new
  - instance
  - generation
  - .
  - Given
  - a
  - sample
  - operation
  - ','
  - the
  - authors
  - propose
  - to
  - use
  - the
  - so-called
  - optimal
  - transport
  - to
  - map
  - the
  - distribution
  - of
  - the
  - new
  - data
  - to
  - that
  - of
  - the
  - input
  - data
  - that
  - were
  - used
  - training
  - .
  - The
  - optimal
  - transport
  - is
  - essentially
  - a
  - monotonic
  - transformation
  - as
  - the
  - composite
  - of
  - the
  - inverse
  - of
  - the
  - target
  - distribution
  - and
  - the
  - source
  - distribution
  - .
  - The
  - paper
  - is
  - in
  - general
  - well
  - written
  - .
  - However
  - ','
  - I
  - am
  - concerned
  - with
  - two
  - issues
  - here
  - ','
  - which
  - are
  - related
  - to
  - the
  - motivation
  - and
  - performance
  - evaluation
  - ','
  - respectively
  - .
  - First
  - ','
  - the
  - authors
  - did
  - n't
  - make
  - it
  - clear
  - what
  - data
  - generation
  - of
  - the
  - trained
  - generative
  - model
  - suffers
  - from
  - the
  - distribution
  - mismatch
  - issue
  - ','
  - although
  - there
  - was
  - some
  - discussion
  - 'on'
  - this
  - in
  - the
  - literature
  - ','
  - as
  - the
  - authors
  - mentioned
  - .
  - To
  - me
  - ','
  - once
  - the
  - generative
  - model
  - is
  - successfully
  - trained
  - ','
  - it
  - is
  - something
  - like
  - a
  - physical
  - process
  - ','
  - and
  - new
  - data
  - ','
  - which
  - are
  - contained
  - in
  - the
  - support
  - of
  - the
  - training
  - data
  - ','
  - can
  - always
  - be
  - used
  - as
  - input
  - to
  - generate
  - new
  - data
  - .
  - -LRB-
  - Personally
  - ','
  - I
  - think
  - this
  - is
  - very
  - different
  - from
  - covariate
  - shift
  - correction
  - in
  - domain
  - adaptation
  - ','
  - in
  - which
  - the
  - correction
  - is
  - necessary
  - because
  - simpler
  - models
  - ','
  - instead
  - of
  - flexible
  - ','
  - nonparametric
  - ones
  - ','
  - are
  - used
  - to
  - make
  - prediction
  - .
  - -RRB-
  - Second
  - ','
  - the
  - authors
  - used
  - the
  - Inception
  - Score
  - for
  - performance
  - evaluation
  - .
  - Please
  - give
  - this
  - score
  - in
  - the
  - paper
  - and
  - make
  - its
  - definition
  - clear
  - .
  - To
  - me
  - ','
  - it
  - is
  - not
  - surprising
  - at
  - all
  - that
  - the
  - proposed
  - method
  - had
  - a
  - better
  - Inception
  - Score
  - ':'
  - roughly
  - speaking
  - ','
  - when
  - we
  - use
  - interpolated
  - values
  - of
  - the
  - training
  - input
  - data
  - to
  - generate
  - images
  - ','
  - the
  - Inception
  - Score
  - is
  - expected
  - to
  - decrease
  - ','
  - compared
  - to
  - that
  - evaluated
  - 'on'
  - the
  - training
  - data
  - .
  - Intuitively
  - ','
  - a
  - very
  - high
  - Inception
  - Score
  - may
  - indicate
  - that
  - we
  - are
  - not
  - trying
  - to
  - generalize
  - ','
  - but
  - just
  - memorize
  - the
  - training
  - input
  - data
  - .
  - An
  - explanation
  - about
  - this
  - point
  - would
  - be
  - highly
  - appreciated
  - .
  - We
  - thank
  - the
  - reviewer
  - for
  - the
  - feedback
  - .
  - Regarding
  - the
  - data-generation
  - process
  - ':'
  - we
  - do
  - use
  - a
  - model
  - that
  - is
  - only
  - trained
  - once
  - to
  - generate
  - new
  - data
  - .
  - However
  - ','
  - we
  - observe
  - -LRB-
  - both
  - theoretically
  - and
  - experimentally
  - -RRB-
  - the
  - opposite
  - of
  - what
  - you
  - claim
  - ':'
  - even
  - though
  - you
  - train
  - 'on'
  - a
  - specific
  - distribution
  - -LRB-
  - say
  - uniform
  - in
  - the
  - '100'
  - dimensional
  - hypercube
  - -RRB-
  - ','
  - it
  - matters
  - where
  - from
  - the
  - support
  - you
  - sample
  - .
  - Of
  - course
  - ','
  - if
  - you
  - use
  - the
  - model
  - as
  - a
  - '``'
  - physical
  - process
  - ''''''
  - and
  - sample
  - new
  - data
  - with
  - the
  - same
  - distribution
  - as
  - you
  - used
  - during
  - training
  - ','
  - you
  - do
  - not
  - have
  - a
  - problem
  - .
  - However
  - ','
  - once
  - you
  - start
  - sampling
  - the
  - distribution
  - in
  - a
  - different
  - way
  - -LRB-
  - e.g.
  - by
  - interpolating
  - between
  - samples
  - -RRB-
  - ','
  - even
  - though
  - you
  - remain
  - in
  - the
  - support
  - of
  - your
  - distribution
  - you
  - start
  - getting
  - '``'
  - abnormal
  - ''''''
  - latent
  - codes
  - which
  - your
  - model
  - performs
  - poorly
  - 'on'
  - .
  - We
  - urge
  - the
  - reviewer
  - to
  - carefully
  - look
  - at
  - Figure
  - '2'
  - .
  - in
  - the
  - paper
  - ','
  - which
  - illustrates
  - how
  - different
  - -LRB-
  - geometrically
  - -RRB-
  - the
  - interpolated
  - samples
  - can
  - be
  - compared
  - to
  - the
  - endpoints
  - ','
  - due
  - to
  - the
  - high
  - dimensionality
  - of
  - the
  - space
  - .
  - Regarding
  - the
  - Inception
  - Score
  - ','
  - it
  - was
  - proposed
  - in
  - by
  - Salimans
  - et
  - al.
  - -LRB-
  - https://arxiv.org/pdf/1606.03498.pdf
  - -RRB-
  - ','
  - and
  - will
  - describe
  - it
  - better
  - in
  - the
  - paper
  - .
  - We
  - do
  - not
  - understand
  - your
  - statement
  - that
  - '``'
  - when
  - we
  - use
  - interpolated
  - values
  - of
  - the
  - training
  - input
  - data
  - to
  - generate
  - images
  - ','
  - the
  - Inception
  - Score
  - is
  - expected
  - to
  - decrease
  - ','
  - compared
  - to
  - that
  - evaluated
  - 'on'
  - the
  - training
  - data
  - ''''''
  - .
  - We
  - are
  - not
  - interpolating
  - training
  - input
  - data
  - ','
  - we
  - are
  - interpolating
  - random
  - latent
  - points
  - during
  - evaluation
  - ','
  - the
  - exact
  - same
  - latent
  - points
  - that
  - are
  - used
  - when
  - evaluating
  - the
  - model
  - in
  - its
  - standard
  - setting
  - .
  - We
  - do
  - not
  - obtain
  - improved
  - Inception
  - Scores
  - compared
  - to
  - the
  - original
  - model
  - -LRB-
  - when
  - sampled
  - randomly
  - -RRB-
  - ','
  - rather
  - we
  - avoid
  - dropping
  - in
  - performance
  - as
  - happens
  - when
  - you
  - linearly
  - interpolate
  - .
- comment_id: B1epRhtspX
  rels:
  - !!python/tuple
    - 0
    - 142
    - 142
    - 1527
    - topic
  - !!python/tuple
    - 0
    - 6
    - 6
    - 17
    - purpose
  - !!python/tuple
    - 6
    - 10
    - 10
    - 17
    - elaboration
  - !!python/tuple
    - 0
    - 17
    - 17
    - 142
    - elaboration
  - !!python/tuple
    - 17
    - 27
    - 27
    - 51
    - elaboration
  - !!python/tuple
    - 27
    - 43
    - 43
    - 51
    - elaboration
  - !!python/tuple
    - 17
    - 51
    - 51
    - 142
    - elaboration
  - !!python/tuple
    - 51
    - 58
    - 58
    - 86
    - elaboration
  - !!python/tuple
    - 58
    - 78
    - 78
    - 86
    - same_unit
  - !!python/tuple
    - 58
    - 74
    - 74
    - 78
    - elaboration
  - !!python/tuple
    - 78
    - 86
    - 58
    - 78
    - same_unit
  - !!python/tuple
    - 78
    - 83
    - 83
    - 86
    - purpose
  - !!python/tuple
    - 51
    - 86
    - 86
    - 142
    - elaboration
  - !!python/tuple
    - 86
    - 104
    - 104
    - 142
    - list
  - !!python/tuple
    - 86
    - 91
    - 91
    - 104
    - purpose
  - !!python/tuple
    - 104
    - 142
    - 86
    - 104
    - list
  - !!python/tuple
    - 104
    - 118
    - 118
    - 142
    - list
  - !!python/tuple
    - 104
    - 112
    - 112
    - 118
    - elaboration
  - !!python/tuple
    - 118
    - 142
    - 104
    - 118
    - list
  - !!python/tuple
    - 118
    - 130
    - 130
    - 142
    - list
  - !!python/tuple
    - 118
    - 122
    - 122
    - 130
    - same_unit
  - !!python/tuple
    - 122
    - 130
    - 118
    - 122
    - same_unit
  - !!python/tuple
    - 122
    - 125
    - 125
    - 130
    - list
  - !!python/tuple
    - 125
    - 130
    - 122
    - 125
    - list
  - !!python/tuple
    - 125
    - 127
    - 127
    - 130
    - purpose
  - !!python/tuple
    - 130
    - 142
    - 118
    - 130
    - list
  - !!python/tuple
    - 130
    - 134
    - 134
    - 142
    - elaboration
  - !!python/tuple
    - 134
    - 137
    - 137
    - 142
    - purpose
  - !!python/tuple
    - 142
    - 1527
    - 0
    - 142
    - topic
  - !!python/tuple
    - 142
    - 164
    - 164
    - 1527
    - elaboration
  - !!python/tuple
    - 164
    - 191
    - 191
    - 1527
    - topic
  - !!python/tuple
    - 170
    - 191
    - 164
    - 170
    - attribution
  - !!python/tuple
    - 170
    - 178
    - 178
    - 191
    - same_unit
  - !!python/tuple
    - 170
    - 174
    - 174
    - 178
    - purpose
  - !!python/tuple
    - 178
    - 191
    - 170
    - 178
    - same_unit
  - !!python/tuple
    - 178
    - 181
    - 181
    - 191
    - purpose
  - !!python/tuple
    - 181
    - 186
    - 186
    - 191
    - manner
  - !!python/tuple
    - 191
    - 1527
    - 164
    - 191
    - topic
  - !!python/tuple
    - 191
    - 196
    - 196
    - 209
    - elaboration
  - !!python/tuple
    - 198
    - 209
    - 196
    - 198
    - attribution
  - !!python/tuple
    - 191
    - 209
    - 209
    - 1527
    - elaboration
  - !!python/tuple
    - 209
    - 226
    - 226
    - 240
    - elaboration
  - !!python/tuple
    - 209
    - 240
    - 240
    - 1527
    - elaboration
  - !!python/tuple
    - 244
    - 254
    - 240
    - 244
    - attribution
  - !!python/tuple
    - 240
    - 254
    - 254
    - 1527
    - explanation
  - !!python/tuple
    - 254
    - 258
    - 258
    - 276
    - purpose
  - !!python/tuple
    - 254
    - 276
    - 276
    - 1527
    - elaboration
  - !!python/tuple
    - 276
    - 300
    - 300
    - 306
    - elaboration
  - !!python/tuple
    - 276
    - 306
    - 306
    - 1527
    - elaboration
  - !!python/tuple
    - 306
    - 319
    - 319
    - 1527
    - elaboration
  - !!python/tuple
    - 319
    - 347
    - 347
    - 353
    - same_unit
  - !!python/tuple
    - 319
    - 321
    - 321
    - 347
    - elaboration
  - !!python/tuple
    - 321
    - 334
    - 334
    - 347
    - elaboration
  - !!python/tuple
    - 347
    - 353
    - 319
    - 347
    - same_unit
  - !!python/tuple
    - 319
    - 353
    - 353
    - 1527
    - elaboration
  - !!python/tuple
    - 353
    - 380
    - 380
    - 1527
    - topic
  - !!python/tuple
    - 353
    - 369
    - 369
    - 380
    - same_unit
  - !!python/tuple
    - 353
    - 361
    - 361
    - 369
    - purpose
  - !!python/tuple
    - 369
    - 380
    - 353
    - 369
    - same_unit
  - !!python/tuple
    - 369
    - 376
    - 376
    - 380
    - comparison
  - !!python/tuple
    - 380
    - 1527
    - 353
    - 380
    - topic
  - !!python/tuple
    - 380
    - 413
    - 413
    - 422
    - elaboration
  - !!python/tuple
    - 380
    - 422
    - 422
    - 432
    - elaboration
  - !!python/tuple
    - 422
    - 428
    - 428
    - 432
    - elaboration
  - !!python/tuple
    - 380
    - 432
    - 432
    - 1527
    - elaboration
  - !!python/tuple
    - 432
    - 456
    - 456
    - 1527
    - list
  - !!python/tuple
    - 432
    - 435
    - 435
    - 456
    - elaboration
  - !!python/tuple
    - 456
    - 1527
    - 432
    - 456
    - list
  - !!python/tuple
    - 456
    - 461
    - 461
    - 479
    - same_unit
  - !!python/tuple
    - 456
    - 458
    - 458
    - 461
    - elaboration
  - !!python/tuple
    - 461
    - 479
    - 456
    - 461
    - same_unit
  - !!python/tuple
    - 461
    - 471
    - 471
    - 479
    - circumstance
  - !!python/tuple
    - 456
    - 479
    - 479
    - 1527
    - elaboration
  - !!python/tuple
    - 479
    - 489
    - 489
    - 502
    - same_unit
  - !!python/tuple
    - 479
    - 485
    - 485
    - 489
    - elaboration
  - !!python/tuple
    - 489
    - 502
    - 479
    - 489
    - same_unit
  - !!python/tuple
    - 489
    - 492
    - 492
    - 502
    - elaboration
  - !!python/tuple
    - 479
    - 502
    - 502
    - 1527
    - elaboration
  - !!python/tuple
    - 502
    - 506
    - 506
    - 1527
    - elaboration
  - !!python/tuple
    - 506
    - 548
    - 548
    - 1527
    - circumstance
  - !!python/tuple
    - 548
    - 557
    - 557
    - 1527
    - textualorganization
  - !!python/tuple
    - 557
    - 1527
    - 548
    - 557
    - textualorganization
  - !!python/tuple
    - 557
    - 575
    - 575
    - 582
    - elaboration
  - !!python/tuple
    - 557
    - 582
    - 582
    - 596
    - elaboration
  - !!python/tuple
    - 582
    - 586
    - 586
    - 596
    - purpose
  - !!python/tuple
    - 557
    - 596
    - 596
    - 611
    - elaboration
  - !!python/tuple
    - 597
    - 611
    - 596
    - 597
    - attribution
  - !!python/tuple
    - 597
    - 601
    - 601
    - 611
    - purpose
  - !!python/tuple
    - 601
    - 606
    - 606
    - 611
    - circumstance
  - !!python/tuple
    - 557
    - 611
    - 611
    - 1527
    - elaboration
  - !!python/tuple
    - 611
    - 624
    - 624
    - 1527
    - elaboration
  - !!python/tuple
    - 624
    - 628
    - 628
    - 645
    - elaboration
  - !!python/tuple
    - 628
    - 637
    - 637
    - 645
    - condition
  - !!python/tuple
    - 624
    - 645
    - 645
    - 1527
    - elaboration
  - !!python/tuple
    - 645
    - 650
    - 650
    - 683
    - elaboration
  - !!python/tuple
    - 650
    - 675
    - 675
    - 683
    - elaboration
  - !!python/tuple
    - 645
    - 683
    - 683
    - 1527
    - elaboration
  - !!python/tuple
    - 683
    - 716
    - 716
    - 1527
    - topic
  - !!python/tuple
    - 683
    - 690
    - 690
    - 697
    - elaboration
  - !!python/tuple
    - 690
    - 691
    - 691
    - 697
    - elaboration
  - !!python/tuple
    - 683
    - 697
    - 697
    - 716
    - elaboration
  - !!python/tuple
    - 698
    - 716
    - 697
    - 698
    - attribution
  - !!python/tuple
    - 707
    - 716
    - 698
    - 707
    - antithesis
  - !!python/tuple
    - 716
    - 1527
    - 683
    - 716
    - topic
  - !!python/tuple
    - 735
    - 1527
    - 716
    - 735
    - antithesis
  - !!python/tuple
    - 719
    - 735
    - 716
    - 719
    - attribution
  - !!python/tuple
    - 741
    - 760
    - 735
    - 741
    - attribution
  - !!python/tuple
    - 741
    - 752
    - 752
    - 760
    - purpose
  - !!python/tuple
    - 735
    - 760
    - 760
    - 1527
    - elaboration
  - !!python/tuple
    - 760
    - 818
    - 818
    - 1527
    - topic
  - !!python/tuple
    - 760
    - 789
    - 789
    - 818
    - list
  - !!python/tuple
    - 760
    - 767
    - 767
    - 789
    - purpose
  - !!python/tuple
    - 767
    - 782
    - 782
    - 789
    - elaboration
  - !!python/tuple
    - 789
    - 818
    - 760
    - 789
    - list
  - !!python/tuple
    - 790
    - 818
    - 789
    - 790
    - attribution
  - !!python/tuple
    - 807
    - 818
    - 790
    - 807
    - antithesis
  - !!python/tuple
    - 790
    - 799
    - 799
    - 807
    - purpose
  - !!python/tuple
    - 807
    - 814
    - 814
    - 818
    - comparison
  - !!python/tuple
    - 818
    - 1527
    - 760
    - 818
    - topic
  - !!python/tuple
    - 818
    - 851
    - 851
    - 870
    - elaboration
  - !!python/tuple
    - 851
    - 860
    - 860
    - 870
    - elaboration
  - !!python/tuple
    - 860
    - 866
    - 866
    - 870
    - elaboration
  - !!python/tuple
    - 818
    - 870
    - 870
    - 871
    - elaboration
  - !!python/tuple
    - 818
    - 871
    - 871
    - 1527
    - elaboration
  - !!python/tuple
    - 871
    - 874
    - 874
    - 903
    - elaboration
  - !!python/tuple
    - 874
    - 896
    - 896
    - 903
    - elaboration
  - !!python/tuple
    - 871
    - 903
    - 903
    - 1527
    - elaboration
  - !!python/tuple
    - 903
    - 906
    - 906
    - 914
    - elaboration
  - !!python/tuple
    - 903
    - 914
    - 914
    - 926
    - elaboration
  - !!python/tuple
    - 903
    - 926
    - 926
    - 1527
    - elaboration
  - !!python/tuple
    - 926
    - 939
    - 939
    - 962
    - purpose
  - !!python/tuple
    - 939
    - 952
    - 952
    - 962
    - same_unit
  - !!python/tuple
    - 939
    - 944
    - 944
    - 952
    - purpose
  - !!python/tuple
    - 944
    - 949
    - 949
    - 952
    - elaboration
  - !!python/tuple
    - 952
    - 962
    - 939
    - 952
    - same_unit
  - !!python/tuple
    - 954
    - 962
    - 952
    - 954
    - attribution
  - !!python/tuple
    - 926
    - 962
    - 962
    - 1527
    - elaboration
  - !!python/tuple
    - 964
    - 987
    - 962
    - 964
    - attribution
  - !!python/tuple
    - 962
    - 987
    - 987
    - 1527
    - elaboration
  - !!python/tuple
    - 987
    - 996
    - 996
    - 1527
    - list
  - !!python/tuple
    - 996
    - 1527
    - 987
    - 996
    - list
  - !!python/tuple
    - 1008
    - 1018
    - 996
    - 1008
    - attribution
  - !!python/tuple
    - 996
    - 1004
    - 1004
    - 1008
    - same_unit
  - !!python/tuple
    - 996
    - 998
    - 998
    - 1004
    - elaboration
  - !!python/tuple
    - 1004
    - 1008
    - 996
    - 1004
    - same_unit
  - !!python/tuple
    - 996
    - 1018
    - 1018
    - 1527
    - elaboration
  - !!python/tuple
    - 1018
    - 1035
    - 1035
    - 1041
    - elaboration
  - !!python/tuple
    - 1018
    - 1041
    - 1041
    - 1527
    - example
  - !!python/tuple
    - 1041
    - 1049
    - 1049
    - 1069
    - elaboration
  - !!python/tuple
    - 1041
    - 1069
    - 1069
    - 1527
    - means
  - !!python/tuple
    - 1069
    - 1090
    - 1090
    - 1102
    - elaboration
  - !!python/tuple
    - 1094
    - 1102
    - 1090
    - 1094
    - attribution
  - !!python/tuple
    - 1069
    - 1102
    - 1102
    - 1129
    - elaboration
  - !!python/tuple
    - 1116
    - 1129
    - 1102
    - 1116
    - attribution
  - !!python/tuple
    - 1069
    - 1129
    - 1129
    - 1527
    - elaboration
  - !!python/tuple
    - 1130
    - 1150
    - 1129
    - 1130
    - attribution
  - !!python/tuple
    - 1130
    - 1134
    - 1134
    - 1150
    - elaboration
  - !!python/tuple
    - 1134
    - 1142
    - 1142
    - 1150
    - circumstance
  - !!python/tuple
    - 1129
    - 1150
    - 1150
    - 1527
    - elaboration
  - !!python/tuple
    - 1150
    - 1153
    - 1153
    - 1160
    - elaboration
  - !!python/tuple
    - 1150
    - 1160
    - 1160
    - 1171
    - elaboration
  - !!python/tuple
    - 1150
    - 1171
    - 1171
    - 1527
    - elaboration
  - !!python/tuple
    - 1171
    - 1185
    - 1185
    - 1218
    - elaboration
  - !!python/tuple
    - 1185
    - 1217
    - 1217
    - 1218
    - same_unit
  - !!python/tuple
    - 1185
    - 1199
    - 1199
    - 1217
    - elaboration
  - !!python/tuple
    - 1217
    - 1218
    - 1185
    - 1217
    - same_unit
  - !!python/tuple
    - 1171
    - 1218
    - 1218
    - 1527
    - elaboration
  - !!python/tuple
    - 1218
    - 1221
    - 1221
    - 1242
    - elaboration
  - !!python/tuple
    - 1221
    - 1225
    - 1225
    - 1242
    - purpose
  - !!python/tuple
    - 1225
    - 1231
    - 1231
    - 1242
    - elaboration
  - !!python/tuple
    - 1231
    - 1235
    - 1235
    - 1242
    - elaboration
  - !!python/tuple
    - 1218
    - 1242
    - 1242
    - 1527
    - elaboration
  - !!python/tuple
    - 1242
    - 1247
    - 1247
    - 1527
    - elaboration
  - !!python/tuple
    - 1247
    - 1287
    - 1287
    - 1527
    - circumstance
  - !!python/tuple
    - 1287
    - 1310
    - 1310
    - 1330
    - elaboration
  - !!python/tuple
    - 1287
    - 1330
    - 1330
    - 1377
    - elaboration
  - !!python/tuple
    - 1330
    - 1336
    - 1336
    - 1352
    - list
  - !!python/tuple
    - 1336
    - 1352
    - 1330
    - 1336
    - list
  - !!python/tuple
    - 1336
    - 1339
    - 1339
    - 1352
    - purpose
  - !!python/tuple
    - 1330
    - 1352
    - 1352
    - 1377
    - elaboration
  - !!python/tuple
    - 1352
    - 1358
    - 1358
    - 1377
    - elaboration
  - !!python/tuple
    - 1358
    - 1366
    - 1366
    - 1367
    - elaboration
  - !!python/tuple
    - 1358
    - 1367
    - 1367
    - 1377
    - means
  - !!python/tuple
    - 1287
    - 1377
    - 1377
    - 1527
    - elaboration
  - !!python/tuple
    - 1377
    - 1381
    - 1381
    - 1390
    - purpose
  - !!python/tuple
    - 1377
    - 1390
    - 1390
    - 1527
    - elaboration
  - !!python/tuple
    - 1390
    - 1392
    - 1392
    - 1406
    - elaboration
  - !!python/tuple
    - 1390
    - 1406
    - 1406
    - 1527
    - elaboration
  - !!python/tuple
    - 1406
    - 1407
    - 1407
    - 1527
    - textualorganization
  - !!python/tuple
    - 1407
    - 1527
    - 1406
    - 1407
    - textualorganization
  - !!python/tuple
    - 1407
    - 1411
    - 1411
    - 1514
    - elaboration
  - !!python/tuple
    - 1411
    - 1430
    - 1430
    - 1445
    - elaboration
  - !!python/tuple
    - 1430
    - 1434
    - 1434
    - 1445
    - purpose
  - !!python/tuple
    - 1411
    - 1445
    - 1445
    - 1514
    - elaboration
  - !!python/tuple
    - 1445
    - 1471
    - 1471
    - 1490
    - elaboration
  - !!python/tuple
    - 1471
    - 1480
    - 1480
    - 1490
    - elaboration
  - !!python/tuple
    - 1445
    - 1490
    - 1490
    - 1514
    - elaboration
  - !!python/tuple
    - 1407
    - 1514
    - 1514
    - 1527
    - elaboration
  - !!python/tuple
    - 1524
    - 1527
    - 1514
    - 1524
    - attribution
  - !!python/tuple
    - 1514
    - 1519
    - 1519
    - 1520
    - purpose
  - !!python/tuple
    - 1514
    - 1520
    - 1520
    - 1524
    - elaboration
  tokens:
  - The
  - paper
  - proposes
  - a
  - new
  - approach
  - to
  - compute
  - hyperbolic
  - embeddings
  - based
  - 'on'
  - the
  - squared
  - Lorentzian
  - distance
  - .
  - This
  - choice
  - of
  - distance
  - function
  - is
  - motivated
  - by
  - the
  - observation
  - that
  - the
  - ranking
  - of
  - these
  - distances
  - is
  - equivalent
  - to
  - the
  - ranking
  - of
  - the
  - 'true'
  - hyperbolic
  - distance
  - -LRB-
  - e.g.
  - ','
  - 'on'
  - the
  - hyperboloid
  - -RRB-
  - .
  - For
  - this
  - reason
  - ','
  - the
  - paper
  - proposes
  - to
  - use
  - this
  - distance
  - function
  - in
  - combination
  - with
  - ranking
  - losses
  - as
  - proposed
  - by
  - Nickel
  - '&'
  - Kiela
  - -LRB-
  - '2017'
  - -RRB-
  - ','
  - as
  - it
  - might
  - be
  - easier
  - to
  - optimize
  - .
  - Moreover
  - ','
  - the
  - paper
  - proposes
  - to
  - use
  - Weierstrass
  - coordinates
  - as
  - a
  - representation
  - for
  - points
  - 'on'
  - the
  - hyperboloid
  - .
  - Hyperbolic
  - embeddings
  - are
  - a
  - promising
  - new
  - research
  - area
  - that
  - fits
  - well
  - into
  - ICLR
  - .
  - Overall
  - ','
  - the
  - paper
  - is
  - written
  - well
  - and
  - good
  - to
  - understand
  - .
  - It
  - introduces
  - interesting
  - ideas
  - that
  - are
  - promising
  - to
  - advance
  - hyperbolic
  - embeddings
  - .
  - However
  - ','
  - in
  - the
  - current
  - version
  - of
  - the
  - paper
  - ','
  - these
  - ideas
  - are
  - not
  - fully
  - developed
  - or
  - their
  - impact
  - is
  - unclear
  - .
  - For
  - instance
  - ','
  - using
  - Weierstrass
  - coordinates
  - as
  - a
  - representations
  - seems
  - to
  - make
  - sense
  - ','
  - as
  - it
  - allows
  - to
  - use
  - standard
  - optimization
  - methods
  - without
  - leaving
  - the
  - manifold
  - .
  - However
  - ','
  - it
  - is
  - important
  - to
  - note
  - that
  - the
  - optimization
  - is
  - still
  - performed
  - 'on'
  - a
  - Riemannian
  - manifold
  - .
  - For
  - that
  - reason
  - ','
  - following
  - the
  - Riemannian
  - gradient
  - along
  - geodesics
  - would
  - still
  - require
  - the
  - exponential
  - map
  - .
  - Moreover
  - ','
  - optimization
  - methods
  - like
  - Adam
  - or
  - SVRG
  - are
  - still
  - not
  - directly
  - applicable
  - .
  - Therefore
  - ','
  - it
  - seems
  - that
  - the
  - practical
  - benefit
  - of
  - this
  - representation
  - is
  - limited
  - .
  - Similarly
  - ','
  - being
  - able
  - to
  - compute
  - the
  - centroid
  - efficiently
  - in
  - closed
  - form
  - is
  - indeed
  - an
  - interesting
  - aspect
  - of
  - the
  - proposed
  - approach
  - .
  - Moreover
  - ','
  - the
  - paper
  - explicitly
  - connects
  - the
  - centroid
  - to
  - the
  - least
  - common
  - ancestor
  - of
  - children
  - in
  - a
  - tree
  - ','
  - what
  - could
  - be
  - very
  - useful
  - to
  - derive
  - new
  - embedding
  - methods
  - .
  - Unfortunately
  - ','
  - this
  - is
  - advantage
  - is
  - n't
  - really
  - exploited
  - in
  - the
  - paper
  - .
  - The
  - approach
  - taken
  - in
  - the
  - paper
  - simply
  - uses
  - the
  - loss
  - function
  - of
  - Nickel
  - '&'
  - Kiela
  - -LRB-
  - '2017'
  - -RRB-
  - and
  - this
  - loss
  - does
  - n't
  - make
  - use
  - of
  - centroids
  - ','
  - as
  - the
  - paper
  - notes
  - itself
  - .
  - The
  - only
  - use
  - of
  - the
  - centroid
  - seems
  - then
  - to
  - justify
  - the
  - regularization
  - method
  - ','
  - i.e.
  - ','
  - that
  - parents
  - should
  - have
  - a
  - smaller
  - norm
  - than
  - their
  - children
  - .
  - However
  - ','
  - this
  - insight
  - alone
  - seems
  - not
  - particularly
  - novel
  - ','
  - as
  - the
  - same
  - insight
  - can
  - be
  - derived
  - for
  - standard
  - hyperbolic
  - method
  - and
  - has
  - ','
  - for
  - instance
  - ','
  - been
  - discussed
  - in
  - Nickel
  - '&'
  - Kiela
  - -LRB-
  - '2017'
  - ','
  - '2018'
  - -RRB-
  - ','
  - Ganea
  - et
  - al
  - -LRB-
  - '2018'
  - -RRB-
  - ','
  - De
  - Sa
  - -LRB-
  - '2018'
  - -RRB-
  - .
  - Using
  - the
  - centroid
  - to
  - derive
  - new
  - hyperbolic
  - embeddings
  - could
  - be
  - interesting
  - ','
  - but
  - ','
  - unfortunately
  - ','
  - is
  - currently
  - not
  - done
  - in
  - the
  - paper
  - .
  - Further
  - comments
  - '-'
  - p.
  - '3'
  - ':'
  - Projection
  - back
  - onto
  - the
  - Poincar
  - ball/manifold
  - is
  - only
  - necessary
  - when
  - the
  - exponential
  - map
  - is
  - n't
  - used
  - .
  - The
  - methods
  - of
  - Nickel
  - '&'
  - Kiela
  - -LRB-
  - '2018'
  - -RRB-
  - ','
  - Ganea
  - et
  - al
  - -LRB-
  - '2018'
  - -RRB-
  - therefore
  - do
  - n't
  - have
  - this
  - problem
  - .
  - '-'
  - p.
  - '7'
  - ':'
  - Since
  - MR
  - and
  - MAP
  - are
  - ranking
  - measures
  - ','
  - and
  - the
  - ranking
  - of
  - distances
  - between
  - H
  - ^
  - d
  - and
  - the
  - L
  - ^
  - '2'
  - distance
  - should
  - be
  - identical
  - ','
  - it
  - is
  - not
  - clear
  - to
  - me
  - why
  - the
  - experiments
  - show
  - significant
  - differences
  - for
  - these
  - methods
  - when
  - \
  - beta
  - '='
  - '1'
  - '-'
  - p.
  - '7'
  - ':'
  - Embeddings
  - in
  - the
  - Poincar
  - ball
  - and
  - the
  - Hyperboloid
  - are
  - both
  - compatible
  - with
  - the
  - regularization
  - method
  - in
  - eq
  - '.14'
  - -LRB-
  - using
  - their
  - respective
  - norms
  - -RRB-
  - .
  - It
  - would
  - be
  - interesting
  - to
  - also
  - see
  - results
  - for
  - these
  - methods
  - with
  - regularization
  - .
  - '-'
  - '``'
  - Using
  - Weierstrass
  - allows
  - to
  - use
  - standard
  - optimization
  - methods
  - without
  - leaving
  - the
  - manifold
  - .
  - However
  - ','
  - the
  - optimization
  - is
  - still
  - performed
  - 'on'
  - a
  - Riemannian
  - manifold
  - .
  - ''''''
  - We
  - understand
  - the
  - concern
  - that
  - a
  - Riemannian
  - optimizer
  - would
  - probably
  - be
  - more
  - appropriate
  - since
  - our
  - representations
  - lie
  - 'on'
  - a
  - manifold
  - .
  - However
  - ','
  - from
  - Eq
  - .
  - -LRB-
  - '9'
  - -RRB-
  - ','
  - our
  - distance
  - function
  - can
  - also
  - be
  - seen
  - as
  - the
  - sum
  - of
  - a
  - simple
  - bilinear
  - form
  - between
  - real
  - vectors
  - and
  - another
  - term
  - promoting
  - some
  - similarity
  - of
  - their
  - Euclidean
  - norms
  - .
  - This
  - formulation
  - is
  - then
  - very
  - similar
  - to
  - optimizing
  - -LRB-
  - squared
  - -RRB-
  - Euclidean
  - distances
  - .
  - '-'
  - '``'
  - Computing
  - the
  - centroid
  - in
  - closed
  - form
  - is
  - interesting
  - but
  - is
  - n't
  - really
  - exploited
  - in
  - the
  - paper
  - .
  - '``'
  - We
  - agree
  - that
  - we
  - do
  - not
  - explicitly
  - use
  - the
  - closed-form
  - solution
  - of
  - the
  - centroid
  - in
  - our
  - experiments
  - .
  - However
  - ','
  - our
  - last
  - theorem
  - explains
  - that
  - minimizing
  - the
  - distances
  - to
  - a
  - set
  - of
  - points
  - is
  - equivalent
  - to
  - minimizing
  - the
  - distance
  - to
  - its
  - centroid
  - .
  - Our
  - study
  - of
  - the
  - centroid
  - is
  - important
  - to
  - understand
  - the
  - behavior
  - of
  - our
  - distance
  - function
  - with
  - the
  - considered
  - set
  - of
  - similarity
  - constraints
  - -LRB-
  - based
  - 'on'
  - hierarchical
  - relationships
  - -RRB-
  - .
  - '-'
  - '``'
  - The
  - only
  - use
  - of
  - the
  - centroid
  - seems
  - then
  - to
  - justify
  - the
  - regularization
  - method
  - ','
  - i.e.
  - ','
  - that
  - parents
  - should
  - have
  - a
  - smaller
  - norm
  - than
  - their
  - children
  - .
  - However
  - ','
  - this
  - insight
  - alone
  - seems
  - not
  - particularly
  - novel
  - ','
  - as
  - the
  - same
  - insight
  - can
  - be
  - derived
  - for
  - standard
  - hyperbolic
  - method
  - and
  - has
  - ','
  - for
  - instance
  - ','
  - been
  - discussed
  - in
  - Nickel
  - '&'
  - Kiela
  - -LRB-
  - '2017'
  - ','
  - '2018'
  - -RRB-
  - ','
  - Ganea
  - et
  - al
  - -LRB-
  - '2018'
  - -RRB-
  - ','
  - De
  - Sa
  - -LRB-
  - '2018'
  - -RRB-
  - .
  - ''''''
  - Although
  - the
  - fact
  - that
  - the
  - representation
  - of
  - the
  - common
  - ancestor
  - should
  - have
  - lower
  - Euclidean
  - norm
  - is
  - mentioned
  - in
  - these
  - papers
  - ','
  - it
  - is
  - never
  - proven
  - that
  - it
  - has
  - lower
  - Euclidean
  - norm
  - .
  - The
  - closest
  - example
  - that
  - mentions
  - a
  - minimizer
  - of
  - an
  - expectation
  - of
  - -LRB-
  - squared
  - -RRB-
  - hyperbolic
  - distances
  - is
  - De
  - Sa
  - -LSB-
  - F
  - -RSB-
  - .
  - However
  - ','
  - they
  - do
  - not
  - exploit
  - a
  - closed-form
  - of
  - the
  - centroid
  - and
  - have
  - to
  - use
  - a
  - gradient-based
  - method
  - to
  - minimize
  - an
  - optimization
  - problem
  - based
  - 'on'
  - it
  - -LRB-
  - see
  - -LSB-
  - F
  - -RSB-
  - ','
  - Section
  - '4.2'
  - -RRB-
  - .
  - We
  - show
  - that
  - the
  - Euclidean
  - norm
  - of
  - the
  - centroid
  - of
  - a
  - set
  - of
  - point
  - can
  - be
  - controlled
  - with
  - the
  - curvature
  - of
  - the
  - hyperbolic
  - space
  - .
  - We
  - experimentally
  - show
  - its
  - impact
  - in
  - Table
  - '2'
  - .
  - Retrieval
  - performance
  - -LRB-
  - Mean
  - Rank
  - and
  - MAP
  - -RRB-
  - in
  - Table
  - '1'
  - shows
  - how
  - close
  - to
  - its
  - descendents
  - a
  - common
  - ancestor
  - is
  - .
  - The
  - Poincar
  - metric
  - is
  - defined
  - for
  - a
  - fixed
  - curvature
  - of
  - '-1'
  - and
  - can
  - not
  - have
  - smaller
  - curvature
  - given
  - its
  - formulation
  - exploiting
  - arcosh
  - .
  - Fig.
  - '1'
  - of
  - our
  - submission
  - shows
  - an
  - example
  - where
  - the
  - centroid
  - of
  - the
  - Poincar
  - metric
  - does
  - not
  - have
  - a
  - smaller
  - Euclidean
  - distance
  - than
  - the
  - set
  - of
  - points
  - .
  - By
  - manipulating
  - the
  - curvature
  - of
  - the
  - space
  - ','
  - the
  - centroid
  - of
  - the
  - Lorentzian
  - norm
  - can
  - produce
  - centroids
  - with
  - smaller
  - Euclidean
  - norm
  - -LRB-
  - as
  - we
  - demonstrate
  - that
  - they
  - depend
  - 'on'
  - each
  - other
  - -RRB-
  - .
  - We
  - can
  - also
  - plot
  - the
  - centroid
  - of
  - the
  - squared
  - Poincar
  - distance
  - ','
  - which
  - shows
  - that
  - the
  - corresponding
  - centroid
  - does
  - not
  - have
  - a
  - smaller
  - Euclidean
  - norm
  - either
  - .
  - '-'
  - '``'
  - p.
  - '3'
  - ':'
  - Projection
  - onto
  - the
  - Poincar
  - ball/manifold
  - is
  - only
  - necessary
  - when
  - the
  - exponential
  - map
  - is
  - n't
  - used
  - .
  - Nickel
  - '&'
  - Kiela
  - -LRB-
  - '2018'
  - -RRB-
  - ','
  - Ganea
  - et
  - al
  - -LRB-
  - '2018'
  - -RRB-
  - therefore
  - do
  - n't
  - have
  - this
  - problem
  - .
  - ''''''
  - That
  - is
  - exactly
  - what
  - we
  - explain
  - in
  - p.
  - '3'
  - ','
  - although
  - Ganea
  - et
  - al.
  - -LSB-
  - E
  - -RSB-
  - also
  - reproject
  - their
  - embeddings
  - onto
  - the
  - Poincar
  - ball
  - at
  - each
  - iteration
  - -LRB-
  - as
  - explained
  - in
  - the
  - '``'
  - Numerical
  - errors
  - ''''''
  - paragraph
  - of
  - Section
  - '4'
  - of
  - -LSB-
  - E
  - -RSB-
  - -RRB-
  - .
  - Nickel
  - '&'
  - Kiela
  - -LRB-
  - '2018'
  - -RRB-
  - propose
  - to
  - work
  - in
  - the
  - hyperboloid
  - space
  - to
  - avoid
  - this
  - reprojection
  - as
  - we
  - explain
  - in
  - p.
  - '3'
  - .
  - '-'
  - '``'
  - p.
  - '7'
  - ':'
  - Since
  - MR
  - and
  - MAP
  - are
  - ranking
  - measures
  - ','
  - and
  - the
  - ranking
  - of
  - distances
  - between
  - H
  - ^
  - d
  - and
  - the
  - L
  - ^
  - '2'
  - distance
  - should
  - be
  - identical
  - ','
  - it
  - is
  - not
  - clear
  - why
  - the
  - experiments
  - show
  - significant
  - differences
  - for
  - these
  - methods
  - when
  - \
  - beta
  - '='
  - '1'
  - ''''''
  - Although
  - the
  - order
  - of
  - distances
  - is
  - the
  - same
  - between
  - H
  - ^
  - d
  - and
  - the
  - L
  - ^
  - '2'
  - -LRB-
  - since
  - they
  - only
  - differ
  - by
  - an
  - arcosh
  - activation
  - function
  - -RRB-
  - ','
  - these
  - two
  - distance
  - functions
  - are
  - not
  - equivalent
  - .
  - The
  - arcosh
  - has
  - a
  - logarithmic
  - form
  - and
  - then
  - tends
  - to
  - penalize
  - differences
  - between
  - small
  - distances
  - more
  - than
  - differences
  - between
  - large
  - distances
  - .
  - This
  - generates
  - a
  - difference
  - during
  - training
  - that
  - is
  - for
  - instance
  - similar
  - to
  - the
  - difference
  - obtained
  - by
  - training
  - a
  - linear
  - loss
  - vs
  - a
  - quadratic
  - loss
  - .
  - The
  - quadratic
  - loss
  - tends
  - to
  - penalize
  - outliers
  - more
  - than
  - a
  - linear
  - loss
  - .
  - The
  - fact
  - that
  - these
  - distance
  - functions
  - are
  - not
  - equivalent
  - explains
  - the
  - difference
  - of
  - the
  - results
  - .
  - '-'
  - '``'
  - p.
  - '7'
  - ':'
  - Embeddings
  - in
  - the
  - Poincar
  - ball
  - and
  - the
  - Hyperboloid
  - are
  - both
  - compatible
  - with
  - the
  - regularization
  - method
  - in
  - eq
  - '.14'
  - .
  - It
  - would
  - be
  - interesting
  - to
  - also
  - see
  - results
  - for
  - these
  - methods
  - with
  - regularization
  - .
  - ''''''
  - We
  - agree
  - but
  - the
  - point
  - of
  - that
  - regularizer
  - was
  - to
  - show
  - that
  - the
  - global
  - structure
  - of
  - the
  - tree
  - could
  - be
  - easily
  - recovered
  - by
  - using
  - such
  - constraints
  - without
  - having
  - a
  - significant
  - impact
  - 'on'
  - the
  - retrieval
  - performances
  - -LRB-
  - i.e.
  - Mean
  - Rank
  - and
  - Mean
  - Average
  - Precision
  - -RRB-
  - .
  - We
  - have
  - added
  - for
  - instance
  - in
  - the
  - updated
  - version
  - a
  - study
  - of
  - the
  - impact
  - of
  - such
  - regularization
  - 'on'
  - classification
  - performance
  - in
  - Table
  - '3'
  - .
  - Removing
  - that
  - regularization
  - consistently
  - leads
  - to
  - -LRB-
  - slightly
  - -RRB-
  - better
  - classification
  - scores
  - .
- comment_id: B1eu6wFIyN
  rels:
  - !!python/tuple
    - 3
    - 13
    - 0
    - 3
    - attribution
  - !!python/tuple
    - 3
    - 6
    - 6
    - 13
    - elaboration
  - !!python/tuple
    - 0
    - 13
    - 13
    - 1242
    - elaboration
  - !!python/tuple
    - 13
    - 18
    - 18
    - 30
    - elaboration
  - !!python/tuple
    - 13
    - 30
    - 30
    - 1242
    - elaboration
  - !!python/tuple
    - 30
    - 48
    - 48
    - 60
    - elaboration
  - !!python/tuple
    - 48
    - 54
    - 54
    - 60
    - elaboration
  - !!python/tuple
    - 56
    - 60
    - 54
    - 56
    - attribution
  - !!python/tuple
    - 30
    - 60
    - 60
    - 1242
    - elaboration
  - !!python/tuple
    - 60
    - 86
    - 86
    - 1242
    - elaboration
  - !!python/tuple
    - 86
    - 101
    - 101
    - 1242
    - elaboration
  - !!python/tuple
    - 102
    - 110
    - 101
    - 102
    - attribution
  - !!python/tuple
    - 104
    - 110
    - 102
    - 104
    - attribution
  - !!python/tuple
    - 101
    - 110
    - 110
    - 1242
    - elaboration
  - !!python/tuple
    - 130
    - 1242
    - 110
    - 130
    - antithesis
  - !!python/tuple
    - 130
    - 138
    - 138
    - 147
    - elaboration
  - !!python/tuple
    - 130
    - 147
    - 147
    - 1242
    - elaboration
  - !!python/tuple
    - 150
    - 176
    - 147
    - 150
    - attribution
  - !!python/tuple
    - 150
    - 170
    - 170
    - 176
    - elaboration
  - !!python/tuple
    - 147
    - 176
    - 176
    - 1242
    - elaboration
  - !!python/tuple
    - 176
    - 183
    - 183
    - 191
    - elaboration
  - !!python/tuple
    - 176
    - 191
    - 191
    - 193
    - elaboration
  - !!python/tuple
    - 176
    - 193
    - 193
    - 1242
    - elaboration
  - !!python/tuple
    - 193
    - 213
    - 213
    - 1242
    - list
  - !!python/tuple
    - 193
    - 198
    - 198
    - 213
    - purpose
  - !!python/tuple
    - 198
    - 202
    - 202
    - 213
    - elaboration
  - !!python/tuple
    - 213
    - 1242
    - 193
    - 213
    - list
  - !!python/tuple
    - 213
    - 215
    - 215
    - 1242
    - elaboration
  - !!python/tuple
    - 215
    - 234
    - 234
    - 1242
    - elaboration
  - !!python/tuple
    - 245
    - 1242
    - 234
    - 245
    - attribution
  - !!python/tuple
    - 245
    - 249
    - 249
    - 261
    - purpose
  - !!python/tuple
    - 245
    - 261
    - 261
    - 1242
    - circumstance
  - !!python/tuple
    - 261
    - 288
    - 288
    - 1242
    - list
  - !!python/tuple
    - 261
    - 285
    - 285
    - 288
    - elaboration
  - !!python/tuple
    - 288
    - 1242
    - 261
    - 288
    - list
  - !!python/tuple
    - 288
    - 1028
    - 1028
    - 1242
    - topic
  - !!python/tuple
    - 288
    - 290
    - 290
    - 1028
    - elaboration
  - !!python/tuple
    - 290
    - 291
    - 291
    - 300
    - elaboration
  - !!python/tuple
    - 290
    - 300
    - 300
    - 334
    - manner
  - !!python/tuple
    - 300
    - 310
    - 310
    - 334
    - purpose
  - !!python/tuple
    - 310
    - 327
    - 327
    - 334
    - elaboration
  - !!python/tuple
    - 290
    - 334
    - 334
    - 1028
    - elaboration
  - !!python/tuple
    - 334
    - 336
    - 336
    - 1028
    - elaboration
  - !!python/tuple
    - 336
    - 348
    - 348
    - 365
    - elaboration
  - !!python/tuple
    - 348
    - 351
    - 351
    - 365
    - circumstance
  - !!python/tuple
    - 336
    - 365
    - 365
    - 433
    - elaboration
  - !!python/tuple
    - 385
    - 433
    - 365
    - 385
    - attribution
  - !!python/tuple
    - 376
    - 385
    - 365
    - 376
    - attribution
  - !!python/tuple
    - 381
    - 385
    - 376
    - 381
    - attribution
  - !!python/tuple
    - 385
    - 405
    - 405
    - 433
    - elaboration
  - !!python/tuple
    - 416
    - 433
    - 405
    - 416
    - attribution
  - !!python/tuple
    - 416
    - 420
    - 420
    - 433
    - elaboration
  - !!python/tuple
    - 336
    - 433
    - 433
    - 1028
    - elaboration
  - !!python/tuple
    - 436
    - 445
    - 433
    - 436
    - attribution
  - !!python/tuple
    - 433
    - 445
    - 445
    - 1028
    - elaboration
  - !!python/tuple
    - 445
    - 463
    - 463
    - 474
    - elaboration
  - !!python/tuple
    - 445
    - 474
    - 474
    - 1028
    - elaboration
  - !!python/tuple
    - 474
    - 491
    - 491
    - 534
    - elaboration
  - !!python/tuple
    - 491
    - 501
    - 501
    - 534
    - elaboration
  - !!python/tuple
    - 501
    - 507
    - 507
    - 534
    - purpose
  - !!python/tuple
    - 474
    - 534
    - 534
    - 1028
    - elaboration
  - !!python/tuple
    - 537
    - 569
    - 534
    - 537
    - attribution
  - !!python/tuple
    - 537
    - 568
    - 568
    - 569
    - same_unit
  - !!python/tuple
    - 537
    - 557
    - 557
    - 568
    - elaboration
  - !!python/tuple
    - 568
    - 569
    - 537
    - 568
    - same_unit
  - !!python/tuple
    - 534
    - 569
    - 569
    - 1028
    - elaboration
  - !!python/tuple
    - 569
    - 594
    - 594
    - 598
    - elaboration
  - !!python/tuple
    - 569
    - 598
    - 598
    - 1028
    - elaboration
  - !!python/tuple
    - 598
    - 604
    - 604
    - 627
    - elaboration
  - !!python/tuple
    - 598
    - 627
    - 627
    - 1028
    - elaboration
  - !!python/tuple
    - 627
    - 664
    - 664
    - 681
    - list
  - !!python/tuple
    - 627
    - 639
    - 639
    - 664
    - elaboration
  - !!python/tuple
    - 639
    - 653
    - 653
    - 664
    - elaboration
  - !!python/tuple
    - 664
    - 681
    - 627
    - 664
    - list
  - !!python/tuple
    - 664
    - 669
    - 669
    - 681
    - elaboration
  - !!python/tuple
    - 627
    - 681
    - 681
    - 1028
    - elaboration
  - !!python/tuple
    - 681
    - 706
    - 706
    - 1028
    - elaboration
  - !!python/tuple
    - 706
    - 711
    - 711
    - 1028
    - elaboration
  - !!python/tuple
    - 711
    - 721
    - 721
    - 759
    - list
  - !!python/tuple
    - 721
    - 759
    - 711
    - 721
    - list
  - !!python/tuple
    - 721
    - 734
    - 734
    - 759
    - list
  - !!python/tuple
    - 734
    - 759
    - 721
    - 734
    - list
  - !!python/tuple
    - 711
    - 759
    - 759
    - 1028
    - elaboration
  - !!python/tuple
    - 759
    - 763
    - 763
    - 779
    - purpose
  - !!python/tuple
    - 763
    - 770
    - 770
    - 779
    - purpose
  - !!python/tuple
    - 759
    - 779
    - 779
    - 1028
    - elaboration
  - !!python/tuple
    - 792
    - 814
    - 779
    - 792
    - attribution
  - !!python/tuple
    - 792
    - 803
    - 803
    - 814
    - purpose
  - !!python/tuple
    - 779
    - 814
    - 814
    - 1028
    - elaboration
  - !!python/tuple
    - 814
    - 837
    - 837
    - 853
    - elaboration
  - !!python/tuple
    - 837
    - 846
    - 846
    - 853
    - condition
  - !!python/tuple
    - 814
    - 853
    - 853
    - 1028
    - elaboration
  - !!python/tuple
    - 853
    - 855
    - 855
    - 864
    - elaboration
  - !!python/tuple
    - 853
    - 864
    - 864
    - 899
    - manner
  - !!python/tuple
    - 864
    - 874
    - 874
    - 899
    - purpose
  - !!python/tuple
    - 874
    - 891
    - 891
    - 899
    - elaboration
  - !!python/tuple
    - 891
    - 898
    - 898
    - 899
    - elaboration
  - !!python/tuple
    - 853
    - 899
    - 899
    - 1028
    - elaboration
  - !!python/tuple
    - 907
    - 921
    - 899
    - 907
    - attribution
  - !!python/tuple
    - 907
    - 913
    - 913
    - 921
    - elaboration
  - !!python/tuple
    - 899
    - 921
    - 921
    - 1028
    - elaboration
  - !!python/tuple
    - 921
    - 928
    - 928
    - 937
    - elaboration
  - !!python/tuple
    - 921
    - 937
    - 937
    - 1028
    - elaboration
  - !!python/tuple
    - 937
    - 941
    - 941
    - 944
    - same_unit
  - !!python/tuple
    - 937
    - 938
    - 938
    - 941
    - elaboration
  - !!python/tuple
    - 941
    - 944
    - 937
    - 941
    - same_unit
  - !!python/tuple
    - 937
    - 944
    - 944
    - 1028
    - elaboration
  - !!python/tuple
    - 944
    - 956
    - 956
    - 965
    - elaboration
  - !!python/tuple
    - 944
    - 965
    - 965
    - 1028
    - elaboration
  - !!python/tuple
    - 965
    - 977
    - 977
    - 1028
    - elaboration
  - !!python/tuple
    - 977
    - 988
    - 988
    - 991
    - elaboration
  - !!python/tuple
    - 977
    - 991
    - 991
    - 1028
    - elaboration
  - !!python/tuple
    - 991
    - 998
    - 998
    - 1028
    - elaboration
  - !!python/tuple
    - 998
    - 1011
    - 1011
    - 1014
    - elaboration
  - !!python/tuple
    - 998
    - 1014
    - 1014
    - 1028
    - elaboration
  - !!python/tuple
    - 1028
    - 1242
    - 288
    - 1028
    - topic
  - !!python/tuple
    - 1028
    - 1072
    - 1072
    - 1242
    - topic
  - !!python/tuple
    - 1049
    - 1072
    - 1028
    - 1049
    - attribution
  - !!python/tuple
    - 1029
    - 1049
    - 1028
    - 1029
    - attribution
  - !!python/tuple
    - 1029
    - 1031
    - 1031
    - 1049
    - purpose
  - !!python/tuple
    - 1031
    - 1039
    - 1039
    - 1049
    - elaboration
  - !!python/tuple
    - 1044
    - 1049
    - 1039
    - 1044
    - attribution
  - !!python/tuple
    - 1049
    - 1054
    - 1054
    - 1072
    - elaboration
  - !!python/tuple
    - 1072
    - 1242
    - 1028
    - 1072
    - topic
  - !!python/tuple
    - 1072
    - 1095
    - 1095
    - 1242
    - list
  - !!python/tuple
    - 1077
    - 1095
    - 1072
    - 1077
    - attribution
  - !!python/tuple
    - 1077
    - 1083
    - 1083
    - 1095
    - condition
  - !!python/tuple
    - 1095
    - 1242
    - 1072
    - 1095
    - list
  - !!python/tuple
    - 1095
    - 1139
    - 1139
    - 1242
    - list
  - !!python/tuple
    - 1095
    - 1098
    - 1098
    - 1117
    - purpose
  - !!python/tuple
    - 1105
    - 1117
    - 1098
    - 1105
    - attribution
  - !!python/tuple
    - 1095
    - 1117
    - 1117
    - 1139
    - elaboration
  - !!python/tuple
    - 1117
    - 1132
    - 1132
    - 1139
    - elaboration
  - !!python/tuple
    - 1132
    - 1135
    - 1135
    - 1139
    - elaboration
  - !!python/tuple
    - 1139
    - 1242
    - 1095
    - 1139
    - list
  - !!python/tuple
    - 1139
    - 1144
    - 1144
    - 1167
    - elaboration
  - !!python/tuple
    - 1139
    - 1167
    - 1167
    - 1242
    - elaboration
  - !!python/tuple
    - 1167
    - 1175
    - 1175
    - 1193
    - elaboration
  - !!python/tuple
    - 1175
    - 1179
    - 1179
    - 1193
    - purpose
  - !!python/tuple
    - 1167
    - 1193
    - 1193
    - 1196
    - elaboration
  - !!python/tuple
    - 1167
    - 1196
    - 1196
    - 1242
    - elaboration
  - !!python/tuple
    - 1196
    - 1201
    - 1201
    - 1208
    - elaboration
  - !!python/tuple
    - 1201
    - 1202
    - 1202
    - 1208
    - elaboration
  - !!python/tuple
    - 1196
    - 1208
    - 1208
    - 1216
    - elaboration
  - !!python/tuple
    - 1196
    - 1216
    - 1216
    - 1242
    - elaboration
  - !!python/tuple
    - 1216
    - 1218
    - 1218
    - 1242
    - elaboration
  - !!python/tuple
    - 1218
    - 1228
    - 1228
    - 1242
    - contrast
  - !!python/tuple
    - 1228
    - 1242
    - 1218
    - 1228
    - contrast
  tokens:
  - This
  - paper
  - shows
  - that
  - the
  - problem
  - of
  - defending
  - MNIST
  - is
  - still
  - unsuccessful
  - .
  - It
  - hereby
  - proposes
  - a
  - model
  - that
  - is
  - robust
  - by
  - design
  - specifically
  - for
  - the
  - MNIST
  - classification
  - task
  - .
  - Unlike
  - conventional
  - classifiers
  - ','
  - the
  - proposal
  - learns
  - a
  - class-dependent
  - data
  - distribution
  - using
  - VAEs
  - ','
  - and
  - conducts
  - variational
  - inference
  - by
  - optimizing
  - over
  - the
  - latent
  - space
  - to
  - estimate
  - the
  - classification
  - logits
  - .
  - Some
  - extensive
  - experiments
  - verify
  - the
  - model
  - robustness
  - with
  - respect
  - to
  - different
  - distance
  - measure
  - ','
  - with
  - most
  - state-of-the-art
  - attacking
  - schemes
  - ','
  - and
  - compared
  - against
  - several
  - baselines
  - .
  - The
  - added
  - experiments
  - with
  - rotation
  - and
  - translation
  - further
  - consolidate
  - the
  - value
  - of
  - the
  - work
  - .
  - Overall
  - I
  - think
  - this
  - is
  - a
  - nice
  - paper
  - .
  - Although
  - being
  - lack
  - of
  - some
  - good
  - intuition
  - ','
  - the
  - proposed
  - model
  - indeed
  - show
  - superior
  - robustness
  - to
  - previous
  - defending
  - approaches
  - .
  - Also
  - ','
  - the
  - model
  - has
  - some
  - other
  - benefits
  - that
  - are
  - shown
  - in
  - Figure
  - '3'
  - and
  - '4'
  - .
  - The
  - results
  - show
  - that
  - the
  - model
  - has
  - indeed
  - learned
  - the
  - data
  - distribution
  - rather
  - than
  - roughly
  - determining
  - the
  - decision
  - boundary
  - of
  - the
  - input
  - space
  - as
  - most
  - existing
  - models
  - do
  - .
  - However
  - ','
  - I
  - have
  - the
  - following
  - comments
  - that
  - might
  - help
  - to
  - improve
  - the
  - paper
  - ':'
  - '1'
  - .
  - It
  - would
  - be
  - more
  - interesting
  - to
  - add
  - more
  - intuition
  - 'on'
  - why
  - the
  - proposed
  - model
  - is
  - already
  - robust
  - by
  - design
  - .
  - '2'
  - .
  - Although
  - the
  - paper
  - is
  - designed
  - for
  - MNIST
  - specifically
  - ','
  - the
  - proposed
  - scheme
  - should
  - apply
  - to
  - other
  - classification
  - tasks
  - .
  - Have
  - you
  - tried
  - the
  - models
  - 'on'
  - other
  - datasets
  - like
  - CIFAR10/100
  - '?'
  - It
  - would
  - be
  - interesting
  - to
  - see
  - whether
  - the
  - proposal
  - would
  - work
  - for
  - more
  - complicated
  - tasks
  - .
  - When
  - the
  - training
  - data
  - for
  - each
  - label
  - is
  - unbalanced
  - ','
  - namely
  - ','
  - some
  - class
  - has
  - very
  - few
  - samples
  - ','
  - would
  - you
  - expect
  - the
  - model
  - to
  - fail
  - '?'
  - '3'
  - .
  - Equation
  - -LRB-
  - '8'
  - -RRB-
  - is
  - complicated
  - and
  - still
  - model-dependent
  - .
  - Without
  - further
  - relaxation
  - and
  - simplification
  - ','
  - it
  - '''s'
  - not
  - easy
  - to
  - see
  - if
  - this
  - value
  - is
  - small
  - or
  - large
  - ','
  - or
  - to
  - understand
  - what
  - kind
  - of
  - message
  - this
  - section
  - is
  - trying
  - to
  - pass
  - .
  - '4'
  - .
  - Although
  - the
  - main
  - contribution
  - of
  - the
  - paper
  - is
  - to
  - propose
  - a
  - model
  - that
  - is
  - robust
  - without
  - further
  - defending
  - ','
  - the
  - proposed
  - model
  - could
  - still
  - benefit
  - from
  - adversarial
  - training
  - .
  - Have
  - you
  - tried
  - to
  - retrain
  - your
  - model
  - using
  - the
  - adversarial
  - examples
  - you
  - have
  - got
  - and
  - see
  - if
  - it
  - helps
  - '?'
  - '``'
  - Although
  - the
  - paper
  - is
  - designed
  - for
  - MNIST
  - specifically
  - ','
  - the
  - proposed
  - scheme
  - should
  - apply
  - to
  - other
  - classification
  - tasks
  - .
  - Have
  - you
  - tried
  - the
  - models
  - 'on'
  - other
  - datasets
  - like
  - CIFAR10/100
  - '?'
  - It
  - would
  - be
  - interesting
  - to
  - see
  - whether
  - the
  - proposal
  - would
  - work
  - for
  - more
  - complicated
  - tasks
  - .
  - ''''''
  - First
  - experiments
  - suggest
  - that
  - our
  - robustness
  - is
  - not
  - limited
  - to
  - MNIST
  - .
  - To
  - show
  - this
  - ','
  - we
  - trained
  - the
  - proposed
  - ABS
  - model
  - and
  - a
  - vanilla
  - CNN
  - 'on'
  - two
  - class
  - CIFAR
  - and
  - achieve
  - a
  - robustness
  - '~'
  - 3x
  - larger
  - than
  - a
  - CNN
  - .
  - Robustness
  - results
  - 'on'
  - '2'
  - class
  - CIFAR
  - ':'
  - model
  - accuracy
  - '|'
  - L2
  - robustness
  - CNN
  - '97.1'
  - '%'
  - '|'
  - '0.8'
  - -LRB-
  - estimated
  - with
  - BIM
  - -RRB-
  - ABS
  - '89.7'
  - '%'
  - '|'
  - '2.5'
  - -LRB-
  - estimated
  - with
  - LatentDescent
  - attack
  - -RRB-
  - To
  - tackle
  - the
  - reduced
  - accuracy
  - of
  - ABS
  - 'on'
  - CIFAR-10
  - and
  - other
  - datasets
  - ','
  - we
  - are
  - currently
  - working
  - 'on'
  - extensions
  - of
  - our
  - architecture
  - and
  - the
  - training
  - procedure
  - .
  - First
  - experiments
  - show
  - that
  - this
  - can
  - improve
  - the
  - accuracy
  - substantially
  - over
  - baseline
  - ABS
  - and
  - still
  - comes
  - with
  - the
  - same
  - robustness
  - to
  - adversarial
  - perturbations
  - -LRB-
  - but
  - this
  - is
  - beyond
  - the
  - scope
  - of
  - this
  - paper
  - -RRB-
  - .
  - '``'
  - When
  - the
  - training
  - data
  - for
  - each
  - label
  - is
  - unbalanced
  - ','
  - namely
  - ','
  - some
  - class
  - has
  - very
  - few
  - samples
  - ','
  - would
  - you
  - expect
  - the
  - model
  - to
  - fail
  - '?'
  - ''''''
  - In
  - contrast
  - to
  - purely
  - discriminative
  - models
  - that
  - require
  - manual
  - rebalancing
  - of
  - the
  - training
  - data
  - ','
  - our
  - generative
  - architecture
  - can
  - cope
  - well
  - with
  - unbalanced
  - datasets
  - out
  - of
  - the
  - box
  - .
  - To
  - demonstrate
  - this
  - experimentally
  - ','
  - we
  - have
  - trained
  - a
  - two-class
  - MNIST
  - classifier
  - -LRB-
  - ones
  - vs.
  - sevens
  - -RRB-
  - both
  - 'on'
  - a
  - balanced
  - dataset
  - ','
  - an
  - unbalanced
  - datasets
  - -LRB-
  - '10'
  - times
  - as
  - many
  - sevens
  - than
  - ones
  - during
  - training
  - -RRB-
  - and
  - a
  - highly
  - unbalanced
  - dataset
  - -LRB-
  - '100'
  - times
  - as
  - many
  - ones
  - as
  - sevens
  - during
  - training
  - -RRB-
  - .
  - They
  - all
  - perform
  - similarly
  - well
  - ':'
  - accuracy
  - '|'
  - L_2
  - median
  - perturbation
  - size
  - with
  - Latent
  - Descent
  - attack
  - balanced
  - ABS
  - '99.6'
  - +
  - '-'
  - '0.1'
  - '%'
  - '|'
  - '3.5'
  - +
  - '-'
  - '0.1'
  - '10'
  - ':'
  - '1'
  - unbalanced
  - ABS
  - '99.3'
  - +
  - '-'
  - '0.2'
  - '%'
  - '|'
  - '3.4'
  - +
  - '-'
  - '0.2'
  - '100:1'
  - unbalanced
  - ABS
  - '98.5'
  - +
  - '-'
  - '0.2'
  - '%'
  - '|'
  - '3.2'
  - +
  - '-'
  - '0.2'
  - '``'
  - It
  - would
  - be
  - more
  - interesting
  - to
  - add
  - more
  - intuition
  - 'on'
  - why
  - the
  - proposed
  - model
  - is
  - already
  - robust
  - by
  - design
  - .
  - ''''''
  - Adversarial
  - training
  - is
  - used
  - to
  - prevent
  - small
  - changes
  - in
  - the
  - input
  - to
  - make
  - large
  - changes
  - in
  - the
  - model
  - decision
  - .
  - In
  - the
  - ABS
  - model
  - ','
  - the
  - Gaussian
  - posterior
  - in
  - the
  - reconstruction
  - term
  - ensures
  - that
  - small
  - changes
  - in
  - the
  - input
  - can
  - only
  - entail
  - small
  - changes
  - to
  - the
  - posterior
  - likelihood
  - and
  - thus
  - to
  - the
  - model
  - decision
  - .
  - In
  - other
  - words
  - ','
  - small
  - changes
  - in
  - the
  - input
  - can
  - only
  - lead
  - to
  - small
  - changes
  - in
  - the
  - reconstruction
  - error
  - and
  - so
  - the
  - logits
  - -LRB-
  - '='
  - reconstruction
  - error
  - +
  - KL
  - divergence
  - -RRB-
  - can
  - only
  - change
  - slowly
  - with
  - varying
  - inputs
  - .
  - '``'
  - Equation
  - -LRB-
  - '8'
  - -RRB-
  - is
  - complicated
  - and
  - still
  - model-dependent
  - .
  - Without
  - further
  - relaxation
  - and
  - simplification
  - ','
  - it
  - '''s'
  - not
  - easy
  - to
  - see
  - if
  - this
  - value
  - is
  - small
  - or
  - large
  - ','
  - or
  - to
  - understand
  - what
  - kind
  - of
  - message
  - this
  - section
  - is
  - trying
  - to
  - pass
  - .
  - ''''''
  - We
  - provide
  - quantitative
  - values
  - in
  - the
  - results
  - section
  - '``'
  - Lower
  - bounds
  - 'on'
  - Robustness
  - ''''''
  - -LRB-
  - we
  - '''ll'
  - add
  - a
  - pointer
  - -RRB-
  - .
  - For
  - ABS
  - ','
  - the
  - mean
  - L2
  - perturbation
  - -LRB-
  - i.e.
  - the
  - mean
  - of
  - epsilon
  - in
  - eq
  - .
  - '8'
  - across
  - samples
  - -RRB-
  - is
  - '0.69'
  - .
  - For
  - comparison
  - ','
  - Hein
  - et
  - al.
  - -LSB-
  - '1'
  - -RSB-
  - reaches
  - '0.48'
  - .
  - -LSB-
  - '1'
  - -RSB-
  - Matthias
  - Hein
  - and
  - Maksym
  - Andriushchenko
  - .
  - Formal
  - guarantees
  - 'on'
  - the
  - robustness
  - of
  - a
  - classifier
  - against
  - adversarial
  - manipulation
  - .
  - In
  - Advances
  - in
  - Neural
  - Information
  - Processing
  - Systems
  - '30'
  - ','
  - pp.
  - '2266'
  - --
  - '2276'
  - .
  - Curran
  - Associates
  - ','
  - Inc.
  - ','
  - '2017'
  - .
  - '``'
  - Although
  - the
  - main
  - contribution
  - of
  - the
  - paper
  - is
  - to
  - propose
  - a
  - model
  - that
  - is
  - robust
  - without
  - further
  - defending
  - ','
  - the
  - proposed
  - model
  - could
  - still
  - benefit
  - from
  - adversarial
  - training
  - .
  - Have
  - you
  - tried
  - to
  - retrain
  - your
  - model
  - using
  - the
  - adversarial
  - examples
  - you
  - have
  - got
  - and
  - see
  - if
  - it
  - helps
  - '?'
  - ''''''
  - It
  - '''s'
  - an
  - interesting
  - question
  - as
  - to
  - whether
  - a
  - combination
  - of
  - analysis
  - by
  - synthesis
  - and
  - adversarial
  - training
  - can
  - yield
  - even
  - better
  - results
  - .
  - One
  - potential
  - problem
  - could
  - be
  - that
  - adversarial
  - training
  - makes
  - little
  - sense
  - if
  - adversarials
  - are
  - already
  - at
  - the
  - perceptual
  - boundary
  - between
  - two
  - classes
  - .
  - This
  - would
  - need
  - to
  - be
  - evaluated
  - carefully
  - and
  - we
  - feel
  - that
  - such
  - an
  - analysis
  - goes
  - beyond
  - the
  - scope
  - of
  - this
  - paper
  - .
  - We
  - will
  - ','
  - however
  - ','
  - release
  - the
  - code
  - and
  - the
  - pretrained
  - model
  - for
  - the
  - community
  - to
  - play
  - around
  - with
  - such
  - ideas
  - .
  - Thanks
  - for
  - the
  - suggestion
  - '!'
  - As
  - we
  - know
  - ','
  - MNIST
  - is
  - too
  - easy
  - and
  - over
  - used
  - ','
  - more
  - importantly
  - ','
  - it
  - can
  - not
  - represent
  - modern
  - CV
  - tasks
  - .
  - Fashion-MNIST
  - is
  - an
  - alternative
  - dataset
  - for
  - MNIST
  - .
  - It
  - would
  - be
  - interesting
  - to
  - see
  - whether
  - the
  - proposal
  - would
  - work
  - for
  - more
  - complicated
  - tasks
  - like
  - Fashion-MNIST
  - .
  - I
  - concur
  - .
  - Fashion-MNIST
  - is
  - a
  - necessary
  - datasets
  - ','
  - which
  - is
  - similar
  - to
  - MNIST
  - .
  - Why
  - not
  - to
  - choose
  - Fashion-MNIST
  - for
  - analysis
  - .
  - The
  - fact
  - that
  - the
  - method
  - performs
  - well
  - 'on'
  - MNIST
  - is
  - nice
  - ','
  - but
  - MNIST
  - should
  - be
  - considered
  - for
  - what
  - it
  - is
  - ':'
  - a
  - toy
  - dataset
  - .
- comment_id: B1eo5rRORm
  rels:
  - !!python/tuple
    - 0
    - 6
    - 6
    - 22
    - elaboration
  - !!python/tuple
    - 6
    - 13
    - 13
    - 22
    - comparison
  - !!python/tuple
    - 0
    - 22
    - 22
    - 1048
    - elaboration
  - !!python/tuple
    - 22
    - 28
    - 28
    - 45
    - purpose
  - !!python/tuple
    - 28
    - 34
    - 34
    - 45
    - elaboration
  - !!python/tuple
    - 34
    - 37
    - 37
    - 45
    - condition
  - !!python/tuple
    - 22
    - 45
    - 45
    - 1048
    - elaboration
  - !!python/tuple
    - 45
    - 77
    - 77
    - 1048
    - topic
  - !!python/tuple
    - 45
    - 51
    - 51
    - 77
    - elaboration
  - !!python/tuple
    - 51
    - 58
    - 58
    - 77
    - purpose
  - !!python/tuple
    - 58
    - 62
    - 62
    - 77
    - elaboration
  - !!python/tuple
    - 62
    - 67
    - 67
    - 77
    - list
  - !!python/tuple
    - 67
    - 77
    - 62
    - 67
    - list
  - !!python/tuple
    - 67
    - 73
    - 73
    - 77
    - elaboration
  - !!python/tuple
    - 77
    - 1048
    - 45
    - 77
    - topic
  - !!python/tuple
    - 77
    - 93
    - 93
    - 1048
    - contrast
  - !!python/tuple
    - 77
    - 79
    - 79
    - 93
    - elaboration
  - !!python/tuple
    - 79
    - 89
    - 89
    - 93
    - elaboration
  - !!python/tuple
    - 93
    - 1048
    - 77
    - 93
    - contrast
  - !!python/tuple
    - 93
    - 107
    - 107
    - 1048
    - elaboration
  - !!python/tuple
    - 107
    - 122
    - 122
    - 1048
    - list
  - !!python/tuple
    - 107
    - 115
    - 115
    - 122
    - elaboration
  - !!python/tuple
    - 122
    - 1048
    - 107
    - 122
    - list
  - !!python/tuple
    - 122
    - 124
    - 124
    - 137
    - elaboration
  - !!python/tuple
    - 122
    - 137
    - 137
    - 1048
    - elaboration
  - !!python/tuple
    - 137
    - 140
    - 140
    - 1048
    - textualorganization
  - !!python/tuple
    - 140
    - 1048
    - 137
    - 140
    - textualorganization
  - !!python/tuple
    - 140
    - 256
    - 256
    - 1048
    - list
  - !!python/tuple
    - 140
    - 143
    - 143
    - 256
    - elaboration
  - !!python/tuple
    - 143
    - 147
    - 147
    - 151
    - elaboration
  - !!python/tuple
    - 143
    - 151
    - 151
    - 167
    - elaboration
  - !!python/tuple
    - 143
    - 167
    - 167
    - 256
    - elaboration
  - !!python/tuple
    - 167
    - 176
    - 176
    - 182
    - elaboration
  - !!python/tuple
    - 167
    - 182
    - 182
    - 256
    - elaboration
  - !!python/tuple
    - 182
    - 183
    - 183
    - 184
    - elaboration
  - !!python/tuple
    - 182
    - 184
    - 184
    - 200
    - elaboration
  - !!python/tuple
    - 182
    - 200
    - 200
    - 256
    - elaboration
  - !!python/tuple
    - 200
    - 222
    - 222
    - 240
    - list
  - !!python/tuple
    - 200
    - 202
    - 202
    - 222
    - elaboration
  - !!python/tuple
    - 222
    - 240
    - 200
    - 222
    - list
  - !!python/tuple
    - 222
    - 231
    - 231
    - 240
    - elaboration
  - !!python/tuple
    - 200
    - 240
    - 240
    - 256
    - elaboration
  - !!python/tuple
    - 256
    - 1048
    - 140
    - 256
    - list
  - !!python/tuple
    - 256
    - 287
    - 287
    - 1048
    - list
  - !!python/tuple
    - 256
    - 265
    - 265
    - 287
    - elaboration
  - !!python/tuple
    - 265
    - 266
    - 266
    - 280
    - elaboration
  - !!python/tuple
    - 265
    - 280
    - 280
    - 287
    - elaboration
  - !!python/tuple
    - 287
    - 1048
    - 256
    - 287
    - list
  - !!python/tuple
    - 287
    - 298
    - 298
    - 1048
    - condition
  - !!python/tuple
    - 298
    - 308
    - 308
    - 1048
    - condition
  - !!python/tuple
    - 308
    - 321
    - 321
    - 329
    - elaboration
  - !!python/tuple
    - 308
    - 329
    - 329
    - 1048
    - elaboration
  - !!python/tuple
    - 329
    - 331
    - 331
    - 343
    - elaboration
  - !!python/tuple
    - 329
    - 343
    - 343
    - 1048
    - elaboration
  - !!python/tuple
    - 343
    - 357
    - 357
    - 363
    - elaboration
  - !!python/tuple
    - 343
    - 363
    - 363
    - 1048
    - elaboration
  - !!python/tuple
    - 363
    - 391
    - 391
    - 1048
    - list
  - !!python/tuple
    - 363
    - 369
    - 369
    - 391
    - purpose
  - !!python/tuple
    - 391
    - 1048
    - 363
    - 391
    - list
  - !!python/tuple
    - 391
    - 404
    - 404
    - 441
    - elaboration
  - !!python/tuple
    - 404
    - 412
    - 412
    - 441
    - elaboration
  - !!python/tuple
    - 412
    - 420
    - 420
    - 441
    - elaboration
  - !!python/tuple
    - 420
    - 426
    - 426
    - 441
    - elaboration
  - !!python/tuple
    - 391
    - 441
    - 441
    - 1048
    - elaboration
  - !!python/tuple
    - 444
    - 457
    - 441
    - 444
    - attribution
  - !!python/tuple
    - 441
    - 457
    - 457
    - 1048
    - elaboration
  - !!python/tuple
    - 457
    - 474
    - 474
    - 1048
    - elaboration
  - !!python/tuple
    - 474
    - 502
    - 502
    - 1048
    - elaboration
  - !!python/tuple
    - 502
    - 526
    - 526
    - 1048
    - elaboration
  - !!python/tuple
    - 526
    - 533
    - 533
    - 1048
    - explanation
  - !!python/tuple
    - 533
    - 542
    - 542
    - 558
    - elaboration
  - !!python/tuple
    - 533
    - 558
    - 558
    - 573
    - elaboration
  - !!python/tuple
    - 533
    - 573
    - 573
    - 1048
    - elaboration
  - !!python/tuple
    - 573
    - 575
    - 575
    - 606
    - elaboration
  - !!python/tuple
    - 575
    - 588
    - 588
    - 606
    - same_unit
  - !!python/tuple
    - 575
    - 583
    - 583
    - 588
    - elaboration
  - !!python/tuple
    - 588
    - 606
    - 575
    - 588
    - same_unit
  - !!python/tuple
    - 588
    - 600
    - 600
    - 606
    - same_unit
  - !!python/tuple
    - 588
    - 595
    - 595
    - 600
    - elaboration
  - !!python/tuple
    - 600
    - 606
    - 588
    - 600
    - same_unit
  - !!python/tuple
    - 573
    - 606
    - 606
    - 1048
    - elaboration
  - !!python/tuple
    - 606
    - 625
    - 625
    - 660
    - elaboration
  - !!python/tuple
    - 625
    - 633
    - 633
    - 660
    - elaboration
  - !!python/tuple
    - 606
    - 660
    - 660
    - 688
    - elaboration
  - !!python/tuple
    - 660
    - 661
    - 661
    - 662
    - elaboration
  - !!python/tuple
    - 660
    - 662
    - 662
    - 688
    - elaboration
  - !!python/tuple
    - 662
    - 675
    - 675
    - 688
    - same_unit
  - !!python/tuple
    - 662
    - 670
    - 670
    - 675
    - elaboration
  - !!python/tuple
    - 675
    - 688
    - 662
    - 675
    - same_unit
  - !!python/tuple
    - 675
    - 684
    - 684
    - 688
    - same_unit
  - !!python/tuple
    - 675
    - 681
    - 681
    - 684
    - elaboration
  - !!python/tuple
    - 684
    - 688
    - 675
    - 684
    - same_unit
  - !!python/tuple
    - 606
    - 688
    - 688
    - 1048
    - elaboration
  - !!python/tuple
    - 688
    - 704
    - 704
    - 712
    - elaboration
  - !!python/tuple
    - 688
    - 712
    - 712
    - 1048
    - elaboration
  - !!python/tuple
    - 712
    - 741
    - 741
    - 1048
    - list
  - !!python/tuple
    - 712
    - 714
    - 714
    - 741
    - elaboration
  - !!python/tuple
    - 714
    - 725
    - 725
    - 741
    - same_unit
  - !!python/tuple
    - 714
    - 722
    - 722
    - 725
    - elaboration
  - !!python/tuple
    - 725
    - 741
    - 714
    - 725
    - same_unit
  - !!python/tuple
    - 725
    - 727
    - 727
    - 741
    - list
  - !!python/tuple
    - 727
    - 741
    - 725
    - 727
    - list
  - !!python/tuple
    - 727
    - 735
    - 735
    - 741
    - same_unit
  - !!python/tuple
    - 727
    - 730
    - 730
    - 735
    - elaboration
  - !!python/tuple
    - 735
    - 741
    - 727
    - 735
    - same_unit
  - !!python/tuple
    - 741
    - 1048
    - 712
    - 741
    - list
  - !!python/tuple
    - 741
    - 750
    - 750
    - 758
    - elaboration
  - !!python/tuple
    - 741
    - 758
    - 758
    - 1048
    - elaboration
  - !!python/tuple
    - 758
    - 807
    - 807
    - 819
    - elaboration
  - !!python/tuple
    - 814
    - 819
    - 807
    - 814
    - attribution
  - !!python/tuple
    - 758
    - 819
    - 819
    - 1048
    - elaboration
  - !!python/tuple
    - 819
    - 929
    - 929
    - 1048
    - list
  - !!python/tuple
    - 819
    - 828
    - 828
    - 847
    - elaboration
  - !!python/tuple
    - 828
    - 832
    - 832
    - 847
    - purpose
  - !!python/tuple
    - 832
    - 843
    - 843
    - 847
    - elaboration
  - !!python/tuple
    - 819
    - 847
    - 847
    - 864
    - elaboration
  - !!python/tuple
    - 819
    - 864
    - 864
    - 929
    - elaboration
  - !!python/tuple
    - 864
    - 882
    - 882
    - 929
    - elaboration
  - !!python/tuple
    - 882
    - 900
    - 900
    - 929
    - list
  - !!python/tuple
    - 882
    - 893
    - 893
    - 900
    - elaboration
  - !!python/tuple
    - 900
    - 929
    - 882
    - 900
    - list
  - !!python/tuple
    - 908
    - 929
    - 900
    - 908
    - attribution
  - !!python/tuple
    - 908
    - 919
    - 919
    - 929
    - same_unit
  - !!python/tuple
    - 908
    - 912
    - 912
    - 919
    - elaboration
  - !!python/tuple
    - 919
    - 929
    - 908
    - 919
    - same_unit
  - !!python/tuple
    - 919
    - 921
    - 921
    - 929
    - elaboration
  - !!python/tuple
    - 929
    - 1048
    - 819
    - 929
    - list
  - !!python/tuple
    - 929
    - 960
    - 960
    - 979
    - elaboration
  - !!python/tuple
    - 929
    - 979
    - 979
    - 1048
    - elaboration
  - !!python/tuple
    - 979
    - 985
    - 985
    - 986
    - elaboration
  - !!python/tuple
    - 979
    - 986
    - 986
    - 1048
    - elaboration
  - !!python/tuple
    - 986
    - 1001
    - 1001
    - 1015
    - list
  - !!python/tuple
    - 1001
    - 1015
    - 986
    - 1001
    - list
  - !!python/tuple
    - 1001
    - 1008
    - 1008
    - 1015
    - elaboration
  - !!python/tuple
    - 986
    - 1015
    - 1015
    - 1037
    - elaboration
  - !!python/tuple
    - 1015
    - 1030
    - 1030
    - 1035
    - elaboration
  - !!python/tuple
    - 1015
    - 1035
    - 1035
    - 1037
    - elaboration
  - !!python/tuple
    - 986
    - 1037
    - 1037
    - 1048
    - elaboration
  - !!python/tuple
    - 1040
    - 1048
    - 1037
    - 1040
    - attribution
  tokens:
  - This
  - paper
  - investigates
  - multi-target
  - domain
  - adaptation
  - which
  - is
  - an
  - unexplored
  - domain
  - adaptation
  - scenario
  - compared
  - with
  - adapting
  - single/multiple
  - source
  - to
  - single
  - target
  - .
  - A
  - mutual
  - information-based
  - loss
  - is
  - proposed
  - to
  - encourage
  - part
  - of
  - the
  - features
  - to
  - be
  - domain-specific
  - while
  - the
  - other
  - part
  - to
  - be
  - domain-invariant
  - .
  - Instead
  - of
  - optimizing
  - the
  - proposed
  - loss
  - which
  - is
  - intractable
  - ','
  - this
  - work
  - proposes
  - to
  - use
  - neural
  - network
  - to
  - model
  - the
  - relative
  - functions
  - and
  - optimize
  - proposed
  - loss
  - ''''
  - lower
  - bound
  - by
  - SGD
  - .
  - Method
  - ':'
  - The
  - proposed
  - loss
  - has
  - an
  - explanation
  - from
  - information
  - theory
  - ','
  - which
  - is
  - nice
  - .
  - However
  - ','
  - the
  - proposed
  - loss
  - is
  - a
  - combination
  - of
  - '4'
  - different
  - mutual
  - information
  - .
  - The
  - effectiveness
  - of
  - each
  - one
  - is
  - unclear
  - .
  - An
  - ablation
  - study
  - should
  - be
  - provided
  - .
  - Clarity
  - ':'
  - The
  - presentation
  - should
  - be
  - improved
  - ','
  - especially
  - in
  - the
  - descriptions
  - for
  - experiments
  - .
  - '-'
  - Typo
  - ':'
  - Section
  - '4'
  - ':'
  - TanH
  - should
  - be
  - Tanh
  - '-'
  - Duplicated
  - reference
  - ':'
  - Konstantinos
  - Bousmalis
  - ','
  - Nathan
  - Silberman
  - ','
  - David
  - Dohan
  - ','
  - Dumitru
  - Erhan
  - ','
  - and
  - Dilip
  - Krishnan
  - .
  - Unsupervised
  - pixel-level
  - domain
  - adaptation
  - with
  - generative
  - adversarial
  - networks
  - .
  - In
  - CVPR
  - ','
  - July
  - 2017a
  - .
  - Results
  - ':'
  - '-'
  - I
  - am
  - confused
  - by
  - the
  - experimental
  - settings
  - of
  - MTDA-ITA
  - ','
  - c-MTDA-ITA
  - ','
  - and
  - c-MTDA-ITA
  - .
  - s-MTDA-ITA
  - .
  - I
  - understand
  - c-MTDA-ITA
  - is
  - to
  - combine
  - all
  - the
  - target
  - domains
  - into
  - a
  - single
  - one
  - and
  - train
  - it
  - using
  - MTDA-ITA
  - .
  - And
  - s-MTDA-ITA
  - is
  - to
  - train
  - multiple
  - MTDA-ITA
  - separately
  - ','
  - where
  - each
  - one
  - corresponds
  - to
  - a
  - source-target
  - pair
  - .
  - But
  - I
  - am
  - confused
  - by
  - the
  - MTDA-ITA
  - results
  - in
  - both
  - table
  - '1'
  - and
  - table
  - '2'
  - .
  - Could
  - the
  - authors
  - provide
  - some
  - explanation
  - for
  - MTDA-ITA
  - '?'
  - '-'
  - For
  - the
  - metric
  - in
  - digits
  - adaptation
  - ','
  - the
  - standard
  - metric
  - is
  - classification
  - accuracy
  - .
  - The
  - authors
  - use
  - mean
  - classification
  - accuracy
  - .
  - Is
  - this
  - the
  - mean
  - of
  - classification
  - accuracy
  - of
  - multiple
  - runs
  - '?'
  - If
  - so
  - ','
  - authors
  - should
  - provide
  - the
  - standard
  - deviation
  - .
  - If
  - this
  - is
  - the
  - average
  - per-class
  - accuracy
  - ','
  - this
  - is
  - different
  - from
  - standard
  - routine
  - in
  - ADDA
  - ','
  - CORAL
  - ','
  - etc.
  - .
  - Concerns
  - ':'
  - The
  - effectiveness
  - of
  - MDTA-ITA
  - ','
  - s-MDTA-ITA
  - and
  - c-MDTA-ITA
  - are
  - not
  - convincing
  - .
  - From
  - the
  - experiments
  - ','
  - it
  - seems
  - the
  - c-MDTA-ITA
  - can
  - not
  - provide
  - convincing
  - superior
  - performance
  - compared
  - to
  - c-ADDA
  - and
  - c-DTN
  - .
  - Thank
  - you
  - for
  - the
  - review
  - '!'
  - To
  - improve
  - the
  - quality
  - of
  - the
  - paper
  - ','
  - we
  - have
  - made
  - several
  - adjustments
  - to
  - our
  - paper
  - in
  - accordance
  - with
  - your
  - review
  - .
  - '``'
  - The
  - proposed
  - loss
  - is
  - a
  - combination
  - of
  - '4'
  - different
  - mutual
  - information
  - .
  - The
  - effectiveness
  - of
  - each
  - one
  - is
  - unclear
  - .
  - An
  - ablation
  - study
  - should
  - be
  - provided
  - ''''''
  - .
  - We
  - provided
  - a
  - detailed
  - ablation
  - study
  - analyzing
  - the
  - effectiveness
  - of
  - each
  - term
  - in
  - our
  - proposed
  - loss
  - function
  - in
  - Sec.
  - '4.3'
  - .
  - The
  - conclusion
  - is
  - that
  - disabling
  - each
  - of
  - the
  - model
  - '''s'
  - components
  - leads
  - to
  - degraded
  - performance
  - .
  - More
  - precisely
  - ','
  - the
  - average
  - drop
  - by
  - disabling
  - the
  - classifier
  - entropy
  - loss
  - is
  - about
  - '3.5'
  - '%'
  - .
  - Similarly
  - ','
  - by
  - disabling
  - the
  - reconstruction
  - loss
  - and
  - the
  - multi-domain
  - separation
  - loss
  - ','
  - we
  - have
  - about
  - '4.5'
  - '%'
  - and
  - '22'
  - '%'
  - average
  - drop
  - in
  - performance
  - ','
  - respectively
  - .
  - Clearly
  - ','
  - by
  - disabling
  - the
  - multi-domain
  - separation
  - loss
  - ','
  - the
  - accuracy
  - drops
  - significantly
  - due
  - to
  - the
  - severe
  - data
  - distribution
  - mismatch
  - between
  - different
  - domains
  - .
  - See
  - Sec.
  - '4.3'
  - for
  - more
  - details
  - .
  - '``'
  - The
  - descriptions
  - for
  - experiments
  - should
  - be
  - improved
  - .
  - I
  - am
  - confused
  - by
  - the
  - experimental
  - settings
  - of
  - MTDA-ITA
  - ','
  - c-MTDA-ITA
  - ','
  - and
  - s-MTDA-ITA
  - .
  - ''''''
  - The
  - experimental
  - setups
  - for
  - the
  - c-MTDA-ITA
  - ','
  - s-MTDA-ITA
  - and
  - MTDA-ITA
  - results
  - are
  - as
  - follows
  - .
  - c-MTDA-ITA
  - ':'
  - for
  - this
  - case
  - ','
  - we
  - consider
  - a
  - dataset
  - -LRB-
  - for
  - example
  - SVHN
  - -RRB-
  - as
  - the
  - source
  - and
  - combine
  - the
  - others
  - -LRB-
  - MNIST,MNIST-M
  - ','
  - USPS
  - -RRB-
  - into
  - a
  - single
  - target
  - dataset
  - .
  - Hence
  - ','
  - this
  - is
  - a
  - standard
  - single
  - source
  - single
  - target
  - domain
  - adaptation
  - ','
  - where
  - the
  - target
  - contains
  - multiple
  - datasets
  - without
  - knowing
  - which
  - sample
  - belongs
  - to
  - which
  - dataset
  - -LRB-
  - the
  - domain
  - label
  - of
  - the
  - source
  - samples
  - are
  - set
  - to
  - '0'
  - ','
  - and
  - the
  - domain
  - label
  - of
  - all
  - target
  - samples
  - are
  - set
  - to
  - '1'
  - -RRB-
  - .
  - s-MTDA-ITA
  - ':'
  - in
  - this
  - case
  - ','
  - we
  - consider
  - a
  - dataset
  - -LRB-
  - for
  - example
  - SVHN
  - -RRB-
  - as
  - the
  - source
  - and
  - another
  - one
  - -LRB-
  - MNIST
  - -RRB-
  - as
  - the
  - target
  - .
  - Thus
  - ','
  - this
  - setup
  - also
  - corresponds
  - to
  - a
  - standard
  - single
  - source
  - single
  - target
  - domain
  - adaptation
  - ','
  - where
  - the
  - target
  - contains
  - only
  - one
  - dataset
  - .
  - MTDA-ITA
  - ':'
  - for
  - this
  - case
  - ','
  - we
  - consider
  - a
  - dataset
  - -LRB-
  - SVHN
  - -RRB-
  - as
  - source
  - and
  - consider
  - others
  - -LRB-
  - MNIST,MNIST-M
  - ','
  - USPS
  - -RRB-
  - as
  - multiple
  - disjoint
  - target
  - domains
  - .
  - Therefore
  - ','
  - this
  - setup
  - corresponds
  - to
  - a
  - novel
  - setting
  - where
  - we
  - adapt
  - jointly
  - multiple
  - target
  - domains
  - .
  - It
  - should
  - be
  - noted
  - that
  - although
  - for
  - both
  - MTDA-ITA
  - and
  - c-MTDA-ITA
  - ','
  - we
  - do
  - domain
  - adaptation
  - for
  - multiple
  - target
  - dataset
  - ','
  - for
  - c-MTDA-ITA
  - ','
  - we
  - do
  - not
  - have
  - access
  - to
  - the
  - domain
  - labels
  - of
  - the
  - target
  - datasets
  - while
  - for
  - MTDA-ITA
  - ','
  - we
  - have
  - access
  - to
  - the
  - target
  - domain
  - labels
  - -LRB-
  - we
  - know
  - which
  - target
  - sample
  - belong
  - to
  - which
  - domain
  - -RRB-
  - .
  - '``'
  - The
  - meaning
  - of
  - mean
  - classification
  - accuracy
  - ''''''
  - .
  - We
  - use
  - this
  - term
  - to
  - indicate
  - the
  - mean
  - of
  - classification
  - accuracy
  - of
  - five
  - different
  - runs
  - using
  - random
  - initialization
  - .
  - We
  - have
  - included
  - the
  - standard
  - deviation
  - of
  - the
  - reported
  - accuracies
  - to
  - the
  - tables
  - in
  - the
  - paper
  - .
  - Based
  - 'on'
  - the
  - standard
  - deviation
  - results
  - ','
  - our
  - model
  - has
  - lower
  - variances
  - than
  - the
  - other
  - competing
  - methods
  - .
  - '``'
  - It
  - seems
  - the
  - c-MDTA-ITA
  - can
  - not
  - provide
  - convincing
  - superior
  - performance
  - compared
  - to
  - c-ADDA
  - and
  - c-DTN
  - ''''''
  - .
  - The
  - performance
  - scores
  - reported
  - in
  - the
  - tables
  - indicate
  - that
  - c-MDTA-ITA
  - outperforms
  - c-ADDA
  - -LRB-
  - '27'
  - out
  - of
  - '32'
  - cases
  - -RRB-
  - and
  - c-DTN
  - -LRB-
  - '22'
  - out
  - of
  - '32'
  - cases
  - -RRB-
  - .
  - More
  - importantly
  - ','
  - one
  - of
  - the
  - contributions
  - of
  - our
  - work
  - is
  - to
  - demonstrate
  - our
  - specific
  - ','
  - novel
  - way
  - of
  - simultaneously
  - adapting
  - to
  - multiple
  - target
  - domains
  - offers
  - empirical
  - benefit
  - over
  - naive
  - solutions
  - -LRB-
  - combining
  - all
  - target
  - datasets
  - into
  - single
  - domain
  - or
  - adapting
  - each
  - source-target
  - separately
  - in
  - a
  - pair-wise
  - fashion
  - -RRB-
  - .
  - Our
  - experimental
  - results
  - support
  - this
  - claim
  - ':'
  - 'On'
  - digit
  - experiments
  - ','
  - our
  - approach
  - ranks
  - '1'
  - in
  - '9'
  - out
  - of
  - '12'
  - cases
  - ','
  - 'On'
  - Multi-PIE
  - dataset
  - ','
  - it
  - ranks
  - '1'
  - in
  - '17'
  - out
  - of
  - '20'
  - cases
  - .
  - We
  - also
  - included
  - the
  - average
  - rank
  - of
  - each
  - method
  - over
  - all
  - adaptation
  - pairs
  - to
  - the
  - -LRB-
  - last
  - column
  - of
  - -RRB-
  - tables
  - .
  - The
  - scores
  - indicate
  - that
  - MDTA-ITA
  - significantly
  - outperforms
  - other
  - competing
  - methods
  - .
- comment_id: B1eC2L8J0m
  rels:
  - !!python/tuple
    - 0
    - 5
    - 5
    - 14
    - elaboration
  - !!python/tuple
    - 0
    - 14
    - 14
    - 28
    - elaboration
  - !!python/tuple
    - 0
    - 28
    - 28
    - 1028
    - elaboration
  - !!python/tuple
    - 28
    - 47
    - 47
    - 57
    - elaboration
  - !!python/tuple
    - 28
    - 57
    - 57
    - 1028
    - elaboration
  - !!python/tuple
    - 57
    - 73
    - 73
    - 1028
    - elaboration
  - !!python/tuple
    - 73
    - 100
    - 100
    - 1028
    - question
  - !!python/tuple
    - 73
    - 84
    - 84
    - 100
    - elaboration
  - !!python/tuple
    - 100
    - 1028
    - 73
    - 100
    - question
  - !!python/tuple
    - 100
    - 109
    - 109
    - 122
    - list
  - !!python/tuple
    - 109
    - 122
    - 100
    - 109
    - list
  - !!python/tuple
    - 109
    - 115
    - 115
    - 122
    - list
  - !!python/tuple
    - 115
    - 122
    - 109
    - 115
    - list
  - !!python/tuple
    - 100
    - 122
    - 122
    - 1028
    - elaboration
  - !!python/tuple
    - 122
    - 135
    - 135
    - 161
    - same_unit
  - !!python/tuple
    - 122
    - 134
    - 134
    - 135
    - elaboration
  - !!python/tuple
    - 135
    - 161
    - 122
    - 135
    - same_unit
  - !!python/tuple
    - 135
    - 138
    - 138
    - 161
    - elaboration
  - !!python/tuple
    - 138
    - 140
    - 140
    - 161
    - circumstance
  - !!python/tuple
    - 122
    - 161
    - 161
    - 1028
    - elaboration
  - !!python/tuple
    - 161
    - 177
    - 177
    - 178
    - same_unit
  - !!python/tuple
    - 161
    - 172
    - 172
    - 177
    - elaboration
  - !!python/tuple
    - 177
    - 178
    - 161
    - 177
    - same_unit
  - !!python/tuple
    - 161
    - 178
    - 178
    - 1028
    - elaboration
  - !!python/tuple
    - 178
    - 180
    - 180
    - 186
    - attribution
  - !!python/tuple
    - 178
    - 186
    - 186
    - 1028
    - elaboration
  - !!python/tuple
    - 186
    - 209
    - 209
    - 1028
    - topic
  - !!python/tuple
    - 186
    - 191
    - 191
    - 209
    - elaboration
  - !!python/tuple
    - 209
    - 1028
    - 186
    - 209
    - topic
  - !!python/tuple
    - 209
    - 223
    - 223
    - 1028
    - list
  - !!python/tuple
    - 223
    - 1028
    - 209
    - 223
    - list
  - !!python/tuple
    - 224
    - 233
    - 223
    - 224
    - attribution
  - !!python/tuple
    - 224
    - 229
    - 229
    - 233
    - list
  - !!python/tuple
    - 229
    - 233
    - 224
    - 229
    - list
  - !!python/tuple
    - 223
    - 233
    - 233
    - 1028
    - condition
  - !!python/tuple
    - 233
    - 240
    - 240
    - 241
    - elaboration
  - !!python/tuple
    - 233
    - 241
    - 241
    - 1028
    - elaboration
  - !!python/tuple
    - 241
    - 254
    - 254
    - 264
    - elaboration
  - !!python/tuple
    - 255
    - 264
    - 254
    - 255
    - attribution
  - !!python/tuple
    - 241
    - 264
    - 264
    - 1028
    - elaboration
  - !!python/tuple
    - 264
    - 266
    - 266
    - 276
    - elaboration
  - !!python/tuple
    - 266
    - 271
    - 271
    - 276
    - purpose
  - !!python/tuple
    - 264
    - 276
    - 276
    - 283
    - elaboration
  - !!python/tuple
    - 264
    - 283
    - 283
    - 310
    - elaboration
  - !!python/tuple
    - 283
    - 288
    - 288
    - 310
    - elaboration
  - !!python/tuple
    - 264
    - 310
    - 310
    - 1028
    - elaboration
  - !!python/tuple
    - 310
    - 324
    - 324
    - 1028
    - elaboration
  - !!python/tuple
    - 324
    - 345
    - 345
    - 1028
    - elaboration
  - !!python/tuple
    - 345
    - 392
    - 392
    - 1028
    - list
  - !!python/tuple
    - 345
    - 366
    - 366
    - 392
    - elaboration
  - !!python/tuple
    - 366
    - 386
    - 386
    - 392
    - same_unit
  - !!python/tuple
    - 366
    - 369
    - 369
    - 383
    - elaboration
  - !!python/tuple
    - 366
    - 383
    - 383
    - 386
    - restatement
  - !!python/tuple
    - 386
    - 392
    - 366
    - 386
    - same_unit
  - !!python/tuple
    - 392
    - 1028
    - 345
    - 392
    - list
  - !!python/tuple
    - 392
    - 412
    - 412
    - 1028
    - list
  - !!python/tuple
    - 392
    - 404
    - 404
    - 412
    - elaboration
  - !!python/tuple
    - 412
    - 1028
    - 392
    - 412
    - list
  - !!python/tuple
    - 412
    - 422
    - 422
    - 434
    - elaboration
  - !!python/tuple
    - 422
    - 429
    - 429
    - 434
    - same_unit
  - !!python/tuple
    - 422
    - 423
    - 423
    - 429
    - elaboration
  - !!python/tuple
    - 429
    - 434
    - 422
    - 429
    - same_unit
  - !!python/tuple
    - 412
    - 434
    - 434
    - 1028
    - elaboration
  - !!python/tuple
    - 434
    - 442
    - 442
    - 452
    - elaboration
  - !!python/tuple
    - 442
    - 449
    - 449
    - 452
    - elaboration
  - !!python/tuple
    - 434
    - 452
    - 452
    - 1028
    - example
  - !!python/tuple
    - 452
    - 458
    - 458
    - 499
    - elaboration
  - !!python/tuple
    - 458
    - 459
    - 459
    - 498
    - elaboration
  - !!python/tuple
    - 459
    - 482
    - 482
    - 498
    - elaboration
  - !!python/tuple
    - 482
    - 487
    - 487
    - 498
    - elaboration
  - !!python/tuple
    - 458
    - 498
    - 498
    - 499
    - elaboration
  - !!python/tuple
    - 452
    - 499
    - 499
    - 550
    - elaboration
  - !!python/tuple
    - 499
    - 513
    - 513
    - 526
    - same_unit
  - !!python/tuple
    - 499
    - 506
    - 506
    - 513
    - elaboration
  - !!python/tuple
    - 513
    - 526
    - 499
    - 513
    - same_unit
  - !!python/tuple
    - 499
    - 526
    - 526
    - 550
    - elaboration
  - !!python/tuple
    - 452
    - 550
    - 550
    - 1028
    - elaboration
  - !!python/tuple
    - 550
    - 567
    - 567
    - 1028
    - elaboration
  - !!python/tuple
    - 567
    - 594
    - 594
    - 1028
    - question
  - !!python/tuple
    - 567
    - 578
    - 578
    - 594
    - elaboration
  - !!python/tuple
    - 594
    - 1028
    - 567
    - 594
    - question
  - !!python/tuple
    - 594
    - 617
    - 617
    - 641
    - same_unit
  - !!python/tuple
    - 594
    - 603
    - 603
    - 617
    - list
  - !!python/tuple
    - 603
    - 617
    - 594
    - 603
    - list
  - !!python/tuple
    - 603
    - 609
    - 609
    - 617
    - purpose
  - !!python/tuple
    - 617
    - 641
    - 594
    - 617
    - same_unit
  - !!python/tuple
    - 617
    - 627
    - 627
    - 641
    - same_unit
  - !!python/tuple
    - 617
    - 622
    - 622
    - 627
    - elaboration
  - !!python/tuple
    - 627
    - 641
    - 617
    - 627
    - same_unit
  - !!python/tuple
    - 627
    - 635
    - 635
    - 641
    - elaboration
  - !!python/tuple
    - 594
    - 641
    - 641
    - 1028
    - elaboration
  - !!python/tuple
    - 641
    - 660
    - 660
    - 667
    - purpose
  - !!python/tuple
    - 641
    - 667
    - 667
    - 668
    - elaboration
  - !!python/tuple
    - 641
    - 668
    - 668
    - 1028
    - elaboration
  - !!python/tuple
    - 668
    - 684
    - 684
    - 685
    - same_unit
  - !!python/tuple
    - 668
    - 679
    - 679
    - 684
    - elaboration
  - !!python/tuple
    - 684
    - 685
    - 668
    - 684
    - same_unit
  - !!python/tuple
    - 668
    - 685
    - 685
    - 1028
    - elaboration
  - !!python/tuple
    - 685
    - 687
    - 687
    - 693
    - attribution
  - !!python/tuple
    - 685
    - 693
    - 693
    - 721
    - elaboration
  - !!python/tuple
    - 693
    - 694
    - 694
    - 721
    - attribution
  - !!python/tuple
    - 694
    - 712
    - 712
    - 713
    - same_unit
  - !!python/tuple
    - 694
    - 705
    - 705
    - 712
    - elaboration
  - !!python/tuple
    - 712
    - 713
    - 694
    - 712
    - same_unit
  - !!python/tuple
    - 694
    - 713
    - 713
    - 721
    - elaboration
  - !!python/tuple
    - 685
    - 721
    - 721
    - 748
    - elaboration
  - !!python/tuple
    - 721
    - 735
    - 735
    - 747
    - elaboration
  - !!python/tuple
    - 721
    - 747
    - 747
    - 748
    - elaboration
  - !!python/tuple
    - 685
    - 748
    - 748
    - 1028
    - elaboration
  - !!python/tuple
    - 753
    - 772
    - 748
    - 753
    - attribution
  - !!python/tuple
    - 748
    - 772
    - 772
    - 1028
    - elaboration
  - !!python/tuple
    - 772
    - 779
    - 779
    - 784
    - purpose
  - !!python/tuple
    - 772
    - 784
    - 784
    - 1028
    - elaboration
  - !!python/tuple
    - 784
    - 822
    - 822
    - 848
    - elaboration
  - !!python/tuple
    - 822
    - 826
    - 826
    - 848
    - circumstance
  - !!python/tuple
    - 826
    - 839
    - 839
    - 848
    - list
  - !!python/tuple
    - 839
    - 848
    - 826
    - 839
    - list
  - !!python/tuple
    - 784
    - 848
    - 848
    - 1028
    - elaboration
  - !!python/tuple
    - 848
    - 876
    - 876
    - 1028
    - list
  - !!python/tuple
    - 848
    - 855
    - 855
    - 876
    - elaboration
  - !!python/tuple
    - 855
    - 857
    - 857
    - 876
    - elaboration
  - !!python/tuple
    - 876
    - 1028
    - 848
    - 876
    - list
  - !!python/tuple
    - 876
    - 891
    - 891
    - 954
    - list
  - !!python/tuple
    - 891
    - 954
    - 876
    - 891
    - list
  - !!python/tuple
    - 901
    - 954
    - 891
    - 901
    - attribution
  - !!python/tuple
    - 892
    - 901
    - 891
    - 892
    - attribution
  - !!python/tuple
    - 892
    - 897
    - 897
    - 901
    - list
  - !!python/tuple
    - 897
    - 901
    - 892
    - 897
    - list
  - !!python/tuple
    - 901
    - 910
    - 910
    - 924
    - elaboration
  - !!python/tuple
    - 910
    - 916
    - 916
    - 924
    - purpose
  - !!python/tuple
    - 901
    - 924
    - 924
    - 954
    - elaboration
  - !!python/tuple
    - 926
    - 954
    - 924
    - 926
    - attribution
  - !!python/tuple
    - 926
    - 931
    - 931
    - 954
    - elaboration
  - !!python/tuple
    - 931
    - 932
    - 932
    - 954
    - elaboration
  - !!python/tuple
    - 937
    - 954
    - 932
    - 937
    - attribution
  - !!python/tuple
    - 937
    - 941
    - 941
    - 953
    - elaboration
  - !!python/tuple
    - 941
    - 948
    - 948
    - 953
    - purpose
  - !!python/tuple
    - 937
    - 953
    - 953
    - 954
    - elaboration
  - !!python/tuple
    - 876
    - 954
    - 954
    - 1028
    - elaboration
  - !!python/tuple
    - 954
    - 969
    - 969
    - 982
    - elaboration
  - !!python/tuple
    - 954
    - 982
    - 982
    - 1028
    - elaboration
  - !!python/tuple
    - 982
    - 994
    - 994
    - 1028
    - list
  - !!python/tuple
    - 984
    - 985
    - 982
    - 984
    - attribution
  - !!python/tuple
    - 982
    - 985
    - 985
    - 994
    - elaboration
  - !!python/tuple
    - 994
    - 1028
    - 982
    - 994
    - list
  - !!python/tuple
    - 994
    - 996
    - 996
    - 1002
    - elaboration
  - !!python/tuple
    - 994
    - 1002
    - 1002
    - 1028
    - elaboration
  - !!python/tuple
    - 1002
    - 1009
    - 1009
    - 1028
    - elaboration
  - !!python/tuple
    - 1009
    - 1013
    - 1013
    - 1023
    - elaboration
  - !!python/tuple
    - 1009
    - 1023
    - 1023
    - 1028
    - elaboration
  tokens:
  - I
  - have
  - found
  - the
  - ideas
  - proposed
  - in
  - the
  - paper
  - very
  - insightful
  - and
  - interesting
  - .
  - The
  - paper
  - ','
  - in
  - general
  - ','
  - is
  - written
  - very
  - well
  - and
  - is
  - accessible
  - .
  - My
  - most
  - important
  - concern
  - is
  - The
  - whole
  - development
  - seems
  - not
  - as
  - effective
  - as
  - k
  - '='
  - '1'
  - in
  - Table
  - '.2'
  - -LRB-
  - BTW
  - ','
  - there
  - is
  - a
  - typo
  - there
  - -RRB-
  - .
  - One
  - wonders
  - ','
  - why
  - for
  - k
  - '='
  - '2'
  - ','
  - k
  - '='
  - '1'
  - is
  - not
  - included
  - '?'
  - That
  - is
  - ','
  - can
  - the
  - formulation
  - be
  - changed
  - in
  - a
  - way
  - that
  - \
  - downarrow
  - operator
  - represents
  - l
  - \
  - in
  - -LCB-
  - '1'
  - \
  - cdots
  - k
  - -RCB-
  - projections
  - '?'
  - In
  - the
  - end
  - ','
  - the
  - method
  - creates
  - k
  - tuples
  - and
  - feed
  - them
  - through
  - specific
  - fs
  - so
  - why
  - not
  - having
  - smaller
  - tuples
  - '?'
  - The
  - rest
  - of
  - my
  - review
  - below
  - hopefully
  - can
  - help
  - improving
  - the
  - paper
  - ;
  - '-'
  - Is
  - there
  - any
  - reason
  - as
  - to
  - why
  - higher
  - order
  - Janossy
  - poolings
  - do
  - not
  - perform
  - as
  - good
  - as
  - k
  - '='
  - '1'
  - for
  - the
  - sum
  - experiment
  - '?'
  - '-'
  - Can
  - you
  - report
  - the
  - number
  - of
  - parameters
  - for
  - the
  - developments
  - -LRB-
  - Janossy
  - '-'
  - k
  - -RRB-
  - '?'
  - Some
  - examples
  - according
  - to
  - the
  - experiments
  - help
  - .
  - '-'
  - I
  - am
  - a
  - bit
  - lost
  - to
  - grasp
  - the
  - paragraph
  - below
  - Eq
  - '.4'
  - ','
  - can
  - you
  - rephrase
  - it
  - and
  - possibly
  - provide
  - references
  - '?'
  - '-'
  - When
  - it
  - comes
  - to
  - testing
  - ','
  - how
  - do
  - you
  - use
  - Eq
  - '.13'
  - '?'
  - Do
  - you
  - sample
  - a
  - few
  - permutation
  - and
  - compute
  - '13'
  - '?'
  - If
  - 'yes'
  - ','
  - how
  - many
  - in
  - practice
  - '?'
  - '-'
  - In
  - preposition
  - '2.1'
  - ','
  - n
  - seems
  - confusing
  - ','
  - why
  - not
  - '|'
  - h
  - '|'
  - '-'
  - In
  - P6
  - ','
  - x_i
  - is
  - a
  - sequence
  - .
  - this
  - needs
  - to
  - be
  - mentioned
  - Thank
  - you
  - for
  - your
  - positive
  - comments
  - .
  - We
  - address
  - your
  - concerns
  - below
  - .
  - ''''''
  - '-'
  - Is
  - there
  - any
  - reason
  - as
  - to
  - why
  - higher
  - order
  - Janossy
  - poolings
  - do
  - not
  - perform
  - as
  - good
  - as
  - k
  - '='
  - '1'
  - for
  - the
  - sum
  - experiment
  - '?'
  - ''''''
  - The
  - sum
  - task
  - is
  - an
  - easy
  - task
  - ','
  - designed
  - for
  - k
  - '='
  - '1'
  - .
  - Our
  - revised
  - manuscript
  - shows
  - sum
  - task
  - results
  - with
  - more
  - runs
  - and
  - more
  - epochs
  - and
  - the
  - difference
  - is
  - not
  - statistically
  - significant
  - .
  - '``'
  - '-'
  - The
  - whole
  - development
  - seems
  - not
  - as
  - effective
  - as
  - k
  - '='
  - '1'
  - in
  - Table
  - '.2'
  - '...'
  - ''''''
  - Theorem
  - '2.1'
  - shows
  - that
  - Janossy
  - Pooling
  - -LRB-
  - JP
  - -RRB-
  - with
  - k-ary
  - dependencies
  - includes
  - and
  - is
  - more
  - expressive
  - than
  - JP
  - with
  - -LRB-
  - k-1
  - -RRB-
  - '-'
  - ary
  - dependencies
  - ','
  - but
  - there
  - will
  - be
  - tasks
  - where
  - it
  - is
  - sufficient
  - to
  - let
  - k
  - '='
  - '1'
  - -LRB-
  - and
  - also
  - easier
  - to
  - optimize
  - -RRB-
  - .
  - This
  - is
  - especially
  - 'true'
  - for
  - easy
  - tasks
  - like
  - the
  - sum
  - task
  - which
  - do
  - not
  - require
  - exploiting
  - dependencies
  - within
  - the
  - input
  - sequence
  - .
  - Our
  - revised
  - manuscript
  - now
  - considers
  - the
  - harder
  - task
  - of
  - computing
  - the
  - variance
  - of
  - a
  - sequence
  - of
  - numbers
  - .
  - For
  - this
  - harder
  - task
  - ','
  - full-sequence
  - Janossy
  - -LRB-
  - k
  - '='
  - '|'
  - h
  - '|'
  - -RRB-
  - is
  - significantly
  - more
  - accurate
  - than
  - k
  - '='
  - 1,2,3
  - ','
  - by
  - using
  - pi-SGD
  - to
  - train
  - the
  - model
  - -LRB-
  - which
  - optimizes
  - \
  - doublebar
  - -LCB-
  - J
  - -RCB-
  - rather
  - than
  - \
  - doublebar
  - -LCB-
  - L
  - -RCB-
  - -RRB-
  - .
  - In
  - the
  - range
  - task
  - ','
  - full
  - Janossy
  - -LRB-
  - k
  - '='
  - '|'
  - h
  - '|'
  - -RRB-
  - +
  - GRU
  - +
  - pi-SGD
  - also
  - shows
  - significant
  - gains
  - over
  - k
  - '='
  - 1,2,3
  - .
  - For
  - all
  - other
  - tasks
  - ','
  - Janossy
  - k
  - '='
  - '|'
  - h
  - '|'
  - +
  - GRU
  - +
  - pi-SGD
  - performs
  - as
  - well
  - as
  - the
  - other
  - approaches
  - .
  - ''''''
  - '-'
  - One
  - wonders
  - ','
  - why
  - for
  - k
  - '='
  - '2'
  - ','
  - k
  - '='
  - '1'
  - is
  - not
  - included
  - '?'
  - That
  - is
  - ','
  - can
  - the
  - formulation
  - be
  - changed
  - in
  - a
  - way
  - that
  - \
  - downarrow
  - operator
  - represents
  - l
  - \
  - in
  - -LCB-
  - '1'
  - \
  - cdots
  - k
  - -RCB-
  - projections
  - '?'
  - In
  - the
  - end
  - ','
  - the
  - method
  - creates
  - k
  - tuples
  - and
  - feed
  - them
  - through
  - specific
  - fs
  - so
  - why
  - not
  - having
  - smaller
  - tuples
  - '?'
  - ''''''
  - Theoretically
  - it
  - is
  - not
  - necessary
  - -LRB-
  - by
  - Theorem
  - '2.1'
  - -RRB-
  - but
  - is
  - an
  - interesting
  - direction
  - for
  - future
  - work
  - that
  - could
  - help
  - in
  - practice
  - .
  - It
  - is
  - clear
  - ','
  - however
  - ','
  - that
  - Janossy
  - k
  - '='
  - '|'
  - h
  - '|'
  - with
  - GRU
  - +
  - pi-SGD
  - is
  - hard
  - to
  - beat
  - in
  - more
  - challenging
  - tasks
  - .
  - ''''''
  - '-'
  - Can
  - you
  - report
  - the
  - number
  - of
  - parameters
  - for
  - the
  - developments
  - -LRB-
  - Janossy
  - '-'
  - k
  - -RRB-
  - '?'
  - Some
  - examples
  - according
  - to
  - the
  - experiments
  - help
  - .
  - ''''''
  - We
  - have
  - added
  - the
  - number
  - of
  - parameters
  - in
  - the
  - Supplementary
  - Material
  - -LRB-
  - Table
  - '7'
  - and
  - Table
  - '9'
  - -RRB-
  - together
  - with
  - more
  - details
  - about
  - our
  - experimental
  - setting
  - .
  - We
  - have
  - also
  - tested
  - k
  - '='
  - 2,3
  - with
  - more
  - complex
  - models
  - for
  - \
  - arrow
  - -LCB-
  - f
  - -RCB-
  - ','
  - the
  - Supplementary
  - Material
  - shows
  - the
  - improved
  - results
  - .
  - ''''''
  - '-'
  - I
  - am
  - a
  - bit
  - lost
  - to
  - grasp
  - the
  - paragraph
  - below
  - Eq
  - '.4'
  - ','
  - can
  - you
  - rephrase
  - it
  - and
  - possibly
  - provide
  - references
  - '?'
  - ''''''
  - Thank
  - you
  - ','
  - we
  - rephrased
  - our
  - observations
  - to
  - simplify
  - the
  - exposition
  - .
  - We
  - also
  - considered
  - the
  - pros
  - and
  - cons
  - of
  - including
  - a
  - proof
  - that
  - Eq
  - '.4'
  - captures
  - any
  - permutation-invariant
  - function
  - with
  - an
  - expressive-enough
  - set
  - of
  - permutation-sensitive
  - functions
  - ':'
  - the
  - proof
  - is
  - straightforward
  - as
  - one
  - can
  - simply
  - add
  - all
  - possible
  - asymmetries
  - -LRB-
  - that
  - cancel
  - out
  - when
  - summing
  - over
  - all
  - permutations
  - -RRB-
  - to
  - the
  - set
  - of
  - all
  - permutation-invariant
  - functions
  - and
  - make
  - this
  - a
  - set
  - of
  - permutation-sensitive
  - functions
  - .
  - It
  - could
  - be
  - useful
  - as
  - a
  - Proposition
  - but
  - ','
  - given
  - the
  - page
  - limit
  - ','
  - we
  - have
  - chosen
  - to
  - omit
  - this
  - straightforward
  - proof
  - in
  - favor
  - of
  - other
  - observations
  - .
  - '``'
  - '-'
  - When
  - it
  - comes
  - to
  - testing
  - ','
  - how
  - do
  - you
  - use
  - Eq
  - '.13'
  - '?'
  - Do
  - you
  - sample
  - a
  - few
  - permutation
  - and
  - compute
  - '13'
  - '?'
  - If
  - 'yes'
  - ','
  - how
  - many
  - in
  - practice
  - '?'
  - ''''''
  - We
  - have
  - rewritten
  - our
  - experimental
  - section
  - to
  - clarify
  - how
  - Eq
  - '.13'
  - is
  - used
  - .
  - We
  - recommend
  - looking
  - at
  - the
  - new
  - Table
  - '1'
  - which
  - now
  - more
  - clearly
  - defines
  - '``'
  - infr
  - samples
  - ''''''
  - to
  - describe
  - how
  - many
  - samples
  - we
  - use
  - to
  - estimate
  - Eq
  - '.13'
  - .
  - ''''''
  - '-'
  - In
  - preposition
  - '2.1'
  - ','
  - n
  - seems
  - confusing
  - ','
  - why
  - not
  - '|'
  - h
  - '|'
  - ''''''
  - That
  - was
  - a
  - typo
  - ','
  - we
  - have
  - changed
  - to
  - '|'
  - h
  - '|'
  - .
  - Thank
  - you
  - '!'
  - '-'
  - In
  - P6
  - ','
  - x_i
  - is
  - a
  - sequence
  - .
  - this
  - needs
  - to
  - be
  - mentioned
  - Thank
  - you
  - .
  - We
  - have
  - made
  - changes
  - in
  - the
  - notation
  - to
  - clarify
  - that
  - x
  - -LRB-
  - i
  - -RRB-
  - is
  - the
  - i-th
  - sequence
  - from
  - the
  - training
  - -LRB-
  - test
  - -RRB-
  - data
  - .
- comment_id: B1eEd30jpQ
  rels:
  - !!python/tuple
    - 0
    - 13
    - 13
    - 36
    - elaboration
  - !!python/tuple
    - 0
    - 36
    - 36
    - 66
    - elaboration
  - !!python/tuple
    - 36
    - 48
    - 48
    - 66
    - elaboration
  - !!python/tuple
    - 0
    - 66
    - 66
    - 682
    - elaboration
  - !!python/tuple
    - 66
    - 74
    - 74
    - 682
    - elaboration
  - !!python/tuple
    - 74
    - 105
    - 105
    - 682
    - elaboration
  - !!python/tuple
    - 105
    - 113
    - 113
    - 119
    - elaboration
  - !!python/tuple
    - 105
    - 119
    - 119
    - 682
    - elaboration
  - !!python/tuple
    - 119
    - 153
    - 153
    - 183
    - elaboration
  - !!python/tuple
    - 153
    - 156
    - 156
    - 183
    - elaboration
  - !!python/tuple
    - 119
    - 183
    - 183
    - 682
    - elaboration
  - !!python/tuple
    - 183
    - 199
    - 199
    - 682
    - list
  - !!python/tuple
    - 183
    - 184
    - 184
    - 199
    - temporal
  - !!python/tuple
    - 184
    - 192
    - 192
    - 199
    - purpose
  - !!python/tuple
    - 199
    - 682
    - 183
    - 199
    - list
  - !!python/tuple
    - 199
    - 206
    - 206
    - 212
    - elaboration
  - !!python/tuple
    - 199
    - 212
    - 212
    - 682
    - elaboration
  - !!python/tuple
    - 212
    - 220
    - 220
    - 682
    - elaboration
  - !!python/tuple
    - 220
    - 231
    - 231
    - 682
    - elaboration
  - !!python/tuple
    - 231
    - 258
    - 258
    - 682
    - list
  - !!python/tuple
    - 231
    - 234
    - 234
    - 258
    - elaboration
  - !!python/tuple
    - 234
    - 249
    - 249
    - 258
    - elaboration
  - !!python/tuple
    - 249
    - 253
    - 253
    - 258
    - elaboration
  - !!python/tuple
    - 258
    - 682
    - 231
    - 258
    - list
  - !!python/tuple
    - 258
    - 272
    - 272
    - 318
    - elaboration
  - !!python/tuple
    - 272
    - 294
    - 294
    - 318
    - contrast
  - !!python/tuple
    - 294
    - 318
    - 272
    - 294
    - contrast
  - !!python/tuple
    - 294
    - 298
    - 298
    - 318
    - elaboration
  - !!python/tuple
    - 258
    - 318
    - 318
    - 682
    - elaboration
  - !!python/tuple
    - 318
    - 326
    - 326
    - 352
    - same_unit
  - !!python/tuple
    - 318
    - 321
    - 321
    - 326
    - elaboration
  - !!python/tuple
    - 326
    - 352
    - 318
    - 326
    - same_unit
  - !!python/tuple
    - 326
    - 335
    - 335
    - 352
    - elaboration
  - !!python/tuple
    - 335
    - 341
    - 341
    - 352
    - same_unit
  - !!python/tuple
    - 335
    - 336
    - 336
    - 341
    - elaboration
  - !!python/tuple
    - 341
    - 352
    - 335
    - 341
    - same_unit
  - !!python/tuple
    - 318
    - 352
    - 352
    - 682
    - elaboration
  - !!python/tuple
    - 354
    - 375
    - 352
    - 354
    - attribution
  - !!python/tuple
    - 354
    - 365
    - 365
    - 375
    - means
  - !!python/tuple
    - 365
    - 369
    - 369
    - 375
    - elaboration
  - !!python/tuple
    - 352
    - 375
    - 375
    - 405
    - elaboration
  - !!python/tuple
    - 375
    - 379
    - 379
    - 405
    - purpose
  - !!python/tuple
    - 381
    - 405
    - 379
    - 381
    - attribution
  - !!python/tuple
    - 381
    - 398
    - 398
    - 405
    - same_unit
  - !!python/tuple
    - 381
    - 390
    - 390
    - 398
    - same_unit
  - !!python/tuple
    - 381
    - 383
    - 383
    - 390
    - elaboration
  - !!python/tuple
    - 390
    - 398
    - 381
    - 390
    - same_unit
  - !!python/tuple
    - 398
    - 405
    - 381
    - 398
    - same_unit
  - !!python/tuple
    - 398
    - 399
    - 399
    - 405
    - elaboration
  - !!python/tuple
    - 352
    - 405
    - 405
    - 682
    - circumstance
  - !!python/tuple
    - 405
    - 426
    - 426
    - 488
    - elaboration
  - !!python/tuple
    - 426
    - 432
    - 432
    - 455
    - concession
  - !!python/tuple
    - 432
    - 450
    - 450
    - 455
    - elaboration
  - !!python/tuple
    - 426
    - 455
    - 455
    - 488
    - elaboration
  - !!python/tuple
    - 459
    - 488
    - 455
    - 459
    - attribution
  - !!python/tuple
    - 459
    - 471
    - 471
    - 488
    - elaboration
  - !!python/tuple
    - 471
    - 477
    - 477
    - 488
    - list
  - !!python/tuple
    - 477
    - 488
    - 471
    - 477
    - list
  - !!python/tuple
    - 405
    - 488
    - 488
    - 682
    - elaboration
  - !!python/tuple
    - 488
    - 504
    - 504
    - 682
    - list
  - !!python/tuple
    - 488
    - 490
    - 490
    - 504
    - elaboration
  - !!python/tuple
    - 501
    - 504
    - 490
    - 501
    - attribution
  - !!python/tuple
    - 490
    - 496
    - 496
    - 501
    - elaboration
  - !!python/tuple
    - 496
    - 497
    - 497
    - 501
    - elaboration
  - !!python/tuple
    - 497
    - 500
    - 500
    - 501
    - elaboration
  - !!python/tuple
    - 501
    - 502
    - 502
    - 504
    - elaboration
  - !!python/tuple
    - 504
    - 682
    - 488
    - 504
    - list
  - !!python/tuple
    - 504
    - 532
    - 532
    - 682
    - list
  - !!python/tuple
    - 504
    - 506
    - 506
    - 532
    - elaboration
  - !!python/tuple
    - 506
    - 520
    - 520
    - 532
    - elaboration
  - !!python/tuple
    - 532
    - 682
    - 504
    - 532
    - list
  - !!python/tuple
    - 532
    - 534
    - 534
    - 566
    - elaboration
  - !!python/tuple
    - 534
    - 549
    - 549
    - 566
    - elaboration
  - !!python/tuple
    - 549
    - 555
    - 555
    - 566
    - elaboration
  - !!python/tuple
    - 532
    - 566
    - 566
    - 682
    - elaboration
  - !!python/tuple
    - 566
    - 571
    - 571
    - 592
    - purpose
  - !!python/tuple
    - 566
    - 592
    - 592
    - 682
    - elaboration
  - !!python/tuple
    - 592
    - 597
    - 597
    - 607
    - elaboration
  - !!python/tuple
    - 597
    - 599
    - 599
    - 607
    - elaboration
  - !!python/tuple
    - 592
    - 607
    - 607
    - 635
    - elaboration
  - !!python/tuple
    - 592
    - 635
    - 635
    - 653
    - elaboration
  - !!python/tuple
    - 592
    - 653
    - 653
    - 682
    - elaboration
  - !!python/tuple
    - 653
    - 676
    - 676
    - 682
    - elaboration
  tokens:
  - In
  - the
  - submitted
  - manuscript
  - ','
  - the
  - authors
  - introduce
  - a
  - novel
  - deep
  - learning
  - architecture
  - to
  - solve
  - the
  - problem
  - of
  - supervised
  - learning
  - with
  - sparse
  - and
  - irregularly
  - sampled
  - multivariate
  - time
  - series
  - ','
  - with
  - a
  - specific
  - interest
  - in
  - EHRs
  - .
  - The
  - architecture
  - is
  - based
  - 'on'
  - the
  - use
  - of
  - a
  - semi-parametric
  - interpolation
  - network
  - followed
  - by
  - the
  - application
  - of
  - a
  - prediction
  - network
  - ','
  - and
  - it
  - is
  - tested
  - 'on'
  - two
  - classification/regression
  - tasks
  - .
  - The
  - manuscript
  - is
  - interesting
  - and
  - well
  - written
  - ':'
  - the
  - problem
  - is
  - properly
  - located
  - into
  - context
  - with
  - extensive
  - bibliography
  - ','
  - the
  - method
  - is
  - sufficiently
  - detailed
  - and
  - the
  - experimental
  - comparative
  - section
  - is
  - rich
  - and
  - supportive
  - of
  - the
  - authors
  - ''''
  - claim
  - .
  - However
  - ','
  - there
  - are
  - a
  - couple
  - of
  - issues
  - that
  - need
  - to
  - be
  - discussed
  - ':'
  - VB
  - the
  - reported
  - performances
  - represent
  - only
  - a
  - limited
  - improvement
  - over
  - the
  - comparing
  - baselines
  - ','
  - indicating
  - that
  - the
  - proposed
  - model
  - is
  - promising
  - but
  - it
  - is
  - still
  - immature
  - IN
  - the
  - model
  - is
  - sharing
  - many
  - characteristics
  - with
  - -LRB-
  - referenced
  - -RRB-
  - published
  - methods
  - ','
  - which
  - the
  - proposed
  - algorithm
  - is
  - a
  - smart
  - combination
  - of
  - '-'
  - thus
  - ','
  - overall
  - ','
  - the
  - novelty
  - of
  - the
  - introduced
  - method
  - is
  - somewhat
  - limited
  - .
  - '#########'
  - After
  - considering
  - the
  - proposed
  - improvements
  - ','
  - I
  - decided
  - to
  - raise
  - my
  - mark
  - to
  - '6'
  - .
  - Thanks
  - for
  - the
  - good
  - job
  - done
  - '!'
  - Thank
  - you
  - for
  - your
  - comments
  - .
  - We
  - address
  - the
  - issues
  - below
  - ':'
  - Q
  - ':'
  - the
  - reported
  - performances
  - represent
  - only
  - a
  - limited
  - improvement
  - '...'
  - A
  - ':'
  - In
  - Table
  - '1'
  - -LRB-
  - UWave
  - dataset
  - -RRB-
  - ','
  - the
  - proposed
  - model
  - achieves
  - similar
  - accuracy
  - to
  - the
  - Gaussian
  - Process
  - -LRB-
  - GP
  - -RRB-
  - baseline
  - while
  - running
  - 50x
  - faster
  - .
  - At
  - the
  - same
  - time
  - ','
  - it
  - outperforms
  - the
  - other
  - strong
  - deep
  - learning
  - baselines
  - .
  - Our
  - model
  - allows
  - for
  - incorporating
  - all
  - of
  - the
  - information
  - from
  - all
  - available
  - time
  - points
  - into
  - a
  - global
  - interpolation
  - model
  - just
  - like
  - GP
  - but
  - removes
  - the
  - restrictions
  - associated
  - with
  - the
  - need
  - for
  - a
  - positive
  - definite
  - covariance
  - matrix
  - and
  - at
  - the
  - same
  - time
  - reduces
  - the
  - computational
  - complexity
  - .
  - In
  - Table
  - '2'
  - -LRB-
  - MIMIC-III
  - dataset
  - -RRB-
  - ','
  - our
  - model
  - achieves
  - statistically
  - significant
  - improvements
  - over
  - the
  - baseline
  - models
  - -LRB-
  - p
  - <
  - '0.01'
  - -RRB-
  - with
  - respect
  - to
  - all
  - the
  - metrics
  - except
  - median
  - absolute
  - error
  - .
  - We
  - show
  - that
  - the
  - performance
  - 'on'
  - the
  - regression
  - task
  - can
  - be
  - further
  - improved
  - using
  - only
  - two
  - interpolants
  - -LRB-
  - Appendix
  - A.
  - '3'
  - -RRB-
  - .
  - We
  - would
  - also
  - like
  - to
  - note
  - that
  - AUPRC
  - -LRB-
  - Davis
  - '&'
  - Goadrich
  - ','
  - '2006'
  - -RRB-
  - is
  - a
  - better
  - metric
  - for
  - a
  - highly
  - imbalanced
  - dataset
  - which
  - is
  - the
  - case
  - here
  - .
  - When
  - considering
  - AUPRC
  - ','
  - the
  - difference
  - between
  - the
  - performance
  - of
  - the
  - proposed
  - model
  - with
  - respect
  - to
  - the
  - other
  - baselines
  - increases
  - .
  - Similarly
  - for
  - the
  - regression
  - task
  - ','
  - even
  - though
  - the
  - median
  - absolute
  - error
  - is
  - similar
  - to
  - the
  - baselines
  - the
  - explained
  - variance
  - score
  - shows
  - large
  - improvements
  - compared
  - to
  - the
  - baselines
  - .
  - Thus
  - ','
  - we
  - feel
  - that
  - the
  - improved
  - accuracy
  - relative
  - to
  - the
  - existing
  - GRU-based
  - methods
  - 'on'
  - MIMIC-III
  - coupled
  - with
  - the
  - increased
  - modeling
  - flexibility
  - and
  - significant
  - speed-ups
  - relative
  - to
  - the
  - GP-GRU
  - are
  - important
  - contributions
  - .
  - Q
  - ':'
  - the
  - model
  - is
  - sharing
  - many
  - characteristics
  - with
  - -LRB-
  - referenced
  - -RRB-
  - published
  - methods
  - '...'
  - .
  - A
  - ':'
  - The
  - proposed
  - model
  - is
  - designed
  - to
  - allow
  - the
  - flexible
  - selection
  - of
  - prediction
  - networks
  - ','
  - which
  - is
  - characteristic
  - that
  - it
  - shares
  - with
  - the
  - prior
  - GP-based
  - methods
  - .
  - Here
  - ','
  - the
  - primary
  - contribution
  - of
  - our
  - approach
  - is
  - a
  - highly
  - significant
  - reduction
  - in
  - the
  - compute
  - time
  - relative
  - to
  - using
  - GP-based
  - methods
  - ','
  - which
  - makes
  - the
  - method
  - much
  - more
  - suitable
  - for
  - practical
  - use
  - .
  - In
  - addition
  - ','
  - our
  - approach
  - to
  - decomposing
  - the
  - continuous
  - time
  - data
  - to
  - directly
  - expose
  - smooth
  - trends
  - and
  - transient
  - components
  - is
  - absent
  - from
  - prior
  - GP-based
  - methods
  - .
  - Relative
  - to
  - prior
  - neural
  - network
  - based
  - approaches
  - -LRB-
  - the
  - GRU
  - '-'
  - '*'
  - family
  - -RRB-
  - ','
  - our
  - method
  - focuses
  - 'on'
  - enabling
  - global
  - interpolation
  - and
  - direct
  - use
  - of
  - continuous
  - time
  - data
  - with
  - 'no'
  - ad-hoc
  - decisions
  - about
  - how
  - to
  - assign
  - values
  - to
  - discrete
  - time
  - intervals
  - .
  - These
  - are
  - significant
  - differences
  - relative
  - to
  - the
  - prior
  - approaches
  - ','
  - particularly
  - in
  - terms
  - of
  - the
  - interpolation
  - process
  - .
  - Indeed
  - ','
  - these
  - differences
  - between
  - global
  - learned
  - interpolation
  - and
  - local
  - imputation
  - directly
  - account
  - for
  - the
  - improved
  - performance
  - of
  - our
  - approach
  - over
  - the
  - GRU
  - '-'
  - '*'
  - family
  - of
  - methods
  - .
- comment_id: B1eu-5JYR7
  rels:
  - !!python/tuple
    - 0
    - 5
    - 5
    - 22
    - elaboration
  - !!python/tuple
    - 0
    - 22
    - 22
    - 768
    - elaboration
  - !!python/tuple
    - 22
    - 29
    - 29
    - 48
    - elaboration
  - !!python/tuple
    - 29
    - 39
    - 39
    - 48
    - elaboration
  - !!python/tuple
    - 22
    - 48
    - 48
    - 768
    - elaboration
  - !!python/tuple
    - 48
    - 54
    - 54
    - 60
    - contrast
  - !!python/tuple
    - 54
    - 60
    - 48
    - 54
    - contrast
  - !!python/tuple
    - 48
    - 60
    - 60
    - 768
    - elaboration
  - !!python/tuple
    - 60
    - 64
    - 64
    - 82
    - elaboration
  - !!python/tuple
    - 60
    - 82
    - 82
    - 768
    - elaboration
  - !!python/tuple
    - 82
    - 121
    - 121
    - 768
    - contrast
  - !!python/tuple
    - 82
    - 93
    - 93
    - 121
    - elaboration
  - !!python/tuple
    - 93
    - 98
    - 98
    - 121
    - elaboration
  - !!python/tuple
    - 98
    - 105
    - 105
    - 121
    - elaboration
  - !!python/tuple
    - 121
    - 768
    - 82
    - 121
    - contrast
  - !!python/tuple
    - 121
    - 140
    - 140
    - 145
    - elaboration
  - !!python/tuple
    - 121
    - 145
    - 145
    - 768
    - elaboration
  - !!python/tuple
    - 145
    - 152
    - 152
    - 174
    - elaboration
  - !!python/tuple
    - 152
    - 156
    - 156
    - 174
    - elaboration
  - !!python/tuple
    - 156
    - 167
    - 167
    - 174
    - elaboration
  - !!python/tuple
    - 145
    - 174
    - 174
    - 768
    - elaboration
  - !!python/tuple
    - 174
    - 199
    - 199
    - 265
    - list
  - !!python/tuple
    - 174
    - 175
    - 175
    - 176
    - elaboration
  - !!python/tuple
    - 174
    - 176
    - 176
    - 186
    - elaboration
  - !!python/tuple
    - 174
    - 186
    - 186
    - 195
    - elaboration
  - !!python/tuple
    - 174
    - 195
    - 195
    - 199
    - elaboration
  - !!python/tuple
    - 199
    - 265
    - 174
    - 199
    - list
  - !!python/tuple
    - 199
    - 215
    - 215
    - 265
    - list
  - !!python/tuple
    - 199
    - 200
    - 200
    - 201
    - elaboration
  - !!python/tuple
    - 199
    - 201
    - 201
    - 211
    - elaboration
  - !!python/tuple
    - 199
    - 211
    - 211
    - 215
    - elaboration
  - !!python/tuple
    - 215
    - 265
    - 199
    - 215
    - list
  - !!python/tuple
    - 215
    - 242
    - 242
    - 265
    - list
  - !!python/tuple
    - 227
    - 242
    - 215
    - 227
    - attribution
  - !!python/tuple
    - 227
    - 232
    - 232
    - 242
    - purpose
  - !!python/tuple
    - 232
    - 236
    - 236
    - 242
    - circumstance
  - !!python/tuple
    - 242
    - 265
    - 215
    - 242
    - list
  - !!python/tuple
    - 242
    - 257
    - 257
    - 265
    - elaboration
  - !!python/tuple
    - 174
    - 265
    - 265
    - 768
    - elaboration
  - !!python/tuple
    - 265
    - 285
    - 285
    - 768
    - elaboration
  - !!python/tuple
    - 285
    - 302
    - 302
    - 768
    - textualorganization
  - !!python/tuple
    - 302
    - 768
    - 285
    - 302
    - textualorganization
  - !!python/tuple
    - 302
    - 311
    - 311
    - 372
    - elaboration
  - !!python/tuple
    - 311
    - 314
    - 314
    - 372
    - elaboration
  - !!python/tuple
    - 314
    - 332
    - 332
    - 349
    - elaboration
  - !!python/tuple
    - 335
    - 349
    - 332
    - 335
    - attribution
  - !!python/tuple
    - 314
    - 349
    - 349
    - 372
    - elaboration
  - !!python/tuple
    - 351
    - 372
    - 349
    - 351
    - attribution
  - !!python/tuple
    - 302
    - 372
    - 372
    - 768
    - elaboration
  - !!python/tuple
    - 372
    - 411
    - 411
    - 768
    - list
  - !!python/tuple
    - 372
    - 381
    - 381
    - 411
    - elaboration
  - !!python/tuple
    - 381
    - 405
    - 405
    - 411
    - elaboration
  - !!python/tuple
    - 411
    - 768
    - 372
    - 411
    - list
  - !!python/tuple
    - 411
    - 417
    - 417
    - 768
    - list
  - !!python/tuple
    - 411
    - 414
    - 414
    - 417
    - elaboration
  - !!python/tuple
    - 417
    - 768
    - 411
    - 417
    - list
  - !!python/tuple
    - 417
    - 444
    - 444
    - 768
    - list
  - !!python/tuple
    - 429
    - 444
    - 417
    - 429
    - attribution
  - !!python/tuple
    - 429
    - 434
    - 434
    - 444
    - purpose
  - !!python/tuple
    - 434
    - 438
    - 438
    - 444
    - circumstance
  - !!python/tuple
    - 444
    - 768
    - 417
    - 444
    - list
  - !!python/tuple
    - 444
    - 447
    - 447
    - 768
    - elaboration
  - !!python/tuple
    - 447
    - 448
    - 448
    - 457
    - elaboration
  - !!python/tuple
    - 448
    - 453
    - 453
    - 457
    - purpose
  - !!python/tuple
    - 447
    - 457
    - 457
    - 768
    - elaboration
  - !!python/tuple
    - 457
    - 463
    - 463
    - 478
    - elaboration
  - !!python/tuple
    - 463
    - 465
    - 465
    - 478
    - purpose
  - !!python/tuple
    - 465
    - 474
    - 474
    - 478
    - elaboration
  - !!python/tuple
    - 457
    - 478
    - 478
    - 768
    - elaboration
  - !!python/tuple
    - 478
    - 503
    - 503
    - 514
    - elaboration
  - !!python/tuple
    - 503
    - 509
    - 509
    - 514
    - purpose
  - !!python/tuple
    - 478
    - 514
    - 514
    - 768
    - elaboration
  - !!python/tuple
    - 514
    - 523
    - 523
    - 768
    - elaboration
  - !!python/tuple
    - 523
    - 548
    - 548
    - 768
    - list
  - !!python/tuple
    - 523
    - 526
    - 526
    - 548
    - elaboration
  - !!python/tuple
    - 526
    - 539
    - 539
    - 548
    - elaboration
  - !!python/tuple
    - 548
    - 768
    - 523
    - 548
    - list
  - !!python/tuple
    - 548
    - 568
    - 568
    - 655
    - elaboration
  - !!python/tuple
    - 568
    - 571
    - 571
    - 655
    - elaboration
  - !!python/tuple
    - 571
    - 587
    - 587
    - 599
    - same_unit
  - !!python/tuple
    - 571
    - 584
    - 584
    - 587
    - elaboration
  - !!python/tuple
    - 587
    - 599
    - 571
    - 587
    - same_unit
  - !!python/tuple
    - 587
    - 590
    - 590
    - 599
    - elaboration
  - !!python/tuple
    - 571
    - 599
    - 599
    - 655
    - elaboration
  - !!python/tuple
    - 599
    - 607
    - 607
    - 612
    - same_unit
  - !!python/tuple
    - 599
    - 602
    - 602
    - 607
    - elaboration
  - !!python/tuple
    - 607
    - 612
    - 599
    - 607
    - same_unit
  - !!python/tuple
    - 599
    - 612
    - 612
    - 655
    - elaboration
  - !!python/tuple
    - 612
    - 624
    - 624
    - 635
    - elaboration
  - !!python/tuple
    - 624
    - 625
    - 625
    - 635
    - elaboration
  - !!python/tuple
    - 612
    - 635
    - 635
    - 655
    - elaboration
  - !!python/tuple
    - 638
    - 655
    - 635
    - 638
    - attribution
  - !!python/tuple
    - 638
    - 648
    - 648
    - 655
    - elaboration
  - !!python/tuple
    - 548
    - 655
    - 655
    - 768
    - elaboration
  - !!python/tuple
    - 655
    - 731
    - 731
    - 768
    - list
  - !!python/tuple
    - 659
    - 682
    - 655
    - 659
    - attribution
  - !!python/tuple
    - 659
    - 667
    - 667
    - 682
    - elaboration
  - !!python/tuple
    - 655
    - 682
    - 682
    - 731
    - example
  - !!python/tuple
    - 682
    - 687
    - 687
    - 696
    - elaboration
  - !!python/tuple
    - 682
    - 696
    - 696
    - 731
    - elaboration
  - !!python/tuple
    - 696
    - 702
    - 702
    - 731
    - elaboration
  - !!python/tuple
    - 702
    - 709
    - 709
    - 710
    - list
  - !!python/tuple
    - 709
    - 710
    - 702
    - 709
    - list
  - !!python/tuple
    - 702
    - 710
    - 710
    - 731
    - elaboration
  - !!python/tuple
    - 710
    - 716
    - 716
    - 731
    - elaboration
  - !!python/tuple
    - 716
    - 723
    - 723
    - 731
    - list
  - !!python/tuple
    - 723
    - 731
    - 716
    - 723
    - list
  - !!python/tuple
    - 731
    - 768
    - 655
    - 731
    - list
  - !!python/tuple
    - 731
    - 750
    - 750
    - 768
    - elaboration
  - !!python/tuple
    - 750
    - 760
    - 760
    - 768
    - elaboration
  - !!python/tuple
    - 760
    - 761
    - 761
    - 768
    - elaboration
  tokens:
  - This
  - paper
  - explores
  - the
  - idea
  - of
  - utilizing
  - a
  - secret
  - random
  - permutation
  - in
  - the
  - Fourier
  - phase
  - domain
  - to
  - defense
  - against
  - adversarial
  - examples
  - .
  - The
  - idea
  - is
  - drawn
  - from
  - cryptography
  - ','
  - where
  - the
  - random
  - permutation
  - is
  - treated
  - as
  - a
  - secret
  - key
  - that
  - the
  - adversarial
  - does
  - not
  - have
  - access
  - to
  - .
  - This
  - setting
  - has
  - practical
  - limitations
  - ','
  - but
  - is
  - plausible
  - in
  - theory
  - .
  - While
  - the
  - defense
  - technique
  - is
  - certainly
  - novel
  - and
  - inspired
  - ','
  - its
  - use
  - case
  - seems
  - limited
  - to
  - simple
  - datasets
  - such
  - as
  - MNIST
  - .
  - The
  - permuted
  - phase
  - component
  - does
  - not
  - admit
  - weight
  - sharing
  - and
  - invariances
  - exploited
  - by
  - convolutional
  - networks
  - ','
  - which
  - results
  - in
  - severely
  - hindered
  - clean
  - accuracy
  - --
  - only
  - '96'
  - '%'
  - 'on'
  - MNIST
  - and
  - '45'
  - '%'
  - 'on'
  - CIFAR-10
  - for
  - a
  - single
  - model
  - .
  - While
  - the
  - security
  - of
  - a
  - model
  - against
  - adversarial
  - attacks
  - is
  - important
  - ','
  - a
  - defense
  - should
  - not
  - sacrifice
  - clean
  - accuracy
  - to
  - such
  - an
  - extent
  - .
  - For
  - this
  - weakness
  - ','
  - I
  - recommend
  - rejection
  - but
  - encourage
  - the
  - authors
  - to
  - continue
  - exploring
  - in
  - this
  - direction
  - for
  - a
  - more
  - suitable
  - scheme
  - that
  - does
  - not
  - compromise
  - clean
  - accuracy
  - .
  - Pros
  - ':'
  - '-'
  - Novel
  - defense
  - technique
  - against
  - very
  - challenging
  - white-box
  - attacks
  - .
  - '-'
  - Sound
  - threat
  - model
  - drawn
  - from
  - traditional
  - security
  - .
  - '-'
  - Clearly
  - written
  - .
  - Cons
  - ':'
  - '-'
  - Poor
  - clean
  - accuracy
  - makes
  - the
  - technique
  - very
  - impractical
  - .
  - '-'
  - Insufficient
  - baselines
  - .
  - While
  - the
  - permutation
  - is
  - kept
  - as
  - a
  - secret
  - ','
  - it
  - is
  - plausible
  - that
  - the
  - adversary
  - may
  - attempt
  - to
  - learn
  - the
  - transformation
  - when
  - given
  - enough
  - input-output
  - pairs
  - .
  - Also
  - ','
  - the
  - adversary
  - may
  - attack
  - an
  - ensemble
  - of
  - PPD
  - models
  - for
  - different
  - random
  - permutations
  - -LRB-
  - i.e.
  - expectation
  - over
  - random
  - permutations
  - -RRB-
  - .
  - The
  - authors
  - should
  - introduce
  - an
  - appropriate
  - threat
  - model
  - and
  - evaluate
  - this
  - defense
  - against
  - plausible
  - attacks
  - under
  - that
  - threat
  - model
  - .
  - We
  - thank
  - the
  - reviewer
  - for
  - the
  - detailed
  - feedback
  - and
  - address
  - the
  - comments
  - below
  - ':'
  - Reviewer
  - comment
  - ':'
  - Poor
  - clean
  - accuracy
  - makes
  - the
  - technique
  - very
  - impractical
  - .
  - Our
  - response
  - ':'
  - The
  - '48'
  - '%'
  - accuracy
  - 'on'
  - CIFAR-10
  - is
  - for
  - a
  - simple
  - '3'
  - layer
  - dense
  - neural
  - network
  - and
  - our
  - goal
  - was
  - to
  - show
  - that
  - even
  - with
  - such
  - a
  - simple
  - network
  - ','
  - SOTA
  - robustness
  - can
  - be
  - achieved
  - .
  - We
  - believe
  - that
  - high
  - accuracy
  - combined
  - with
  - adversarial
  - robustness
  - is
  - possible
  - for
  - CIFAR-10
  - ','
  - and
  - transfer
  - learning
  - shows
  - promise
  - in
  - this
  - direction
  - .
  - What
  - we
  - plan
  - to
  - do
  - as
  - future
  - work
  - is
  - to
  - replace
  - the
  - neural
  - network
  - in
  - the
  - PPD
  - pipeline
  - with
  - a
  - pre-trained
  - model
  - 'on'
  - massive
  - datasets
  - such
  - as
  - ImageNet
  - and
  - retrain
  - the
  - final
  - layers
  - to
  - fit
  - the
  - permutation-phase
  - domain
  - .
  - Reviewer
  - comment
  - ':'
  - Insufficient
  - baselines
  - .
  - While
  - the
  - permutation
  - is
  - kept
  - as
  - a
  - secret
  - ','
  - it
  - is
  - plausible
  - that
  - the
  - adversary
  - may
  - attempt
  - to
  - learn
  - the
  - transformation
  - when
  - given
  - enough
  - input-output
  - pairs
  - .
  - Our
  - response
  - ':'
  - Thanks
  - for
  - bringing
  - this
  - attack
  - scenario
  - to
  - our
  - attention
  - .
  - To
  - test
  - PPD
  - against
  - an
  - adversary
  - that
  - tries
  - to
  - learn
  - the
  - transformation
  - ','
  - we
  - used
  - Blackbox
  - attack
  - -LRB-
  - https://arxiv.org/abs/1602.02697
  - -RRB-
  - .
  - In
  - this
  - attack
  - ','
  - adversary
  - probes
  - an
  - ensemble
  - of
  - PPD
  - models
  - as
  - a
  - black
  - box
  - by
  - enough
  - input-output
  - pairs
  - and
  - trains
  - a
  - substitute
  - model
  - .
  - The
  - substitute
  - model
  - is
  - then
  - used
  - to
  - craft
  - adversarial
  - examples
  - .
  - Table
  - '1'
  - is
  - updated
  - with
  - the
  - Blackbox
  - results
  - .
  - Reviewer
  - comment
  - ':'
  - The
  - adversary
  - may
  - attack
  - an
  - ensemble
  - of
  - PPD
  - models
  - for
  - different
  - random
  - permutations
  - -LRB-
  - i.e.
  - ','
  - expectation
  - over
  - random
  - permutations
  - -RRB-
  - .
  - The
  - authors
  - should
  - introduce
  - an
  - appropriate
  - threat
  - model
  - and
  - evaluate
  - this
  - defense
  - against
  - plausible
  - attacks
  - under
  - that
  - threat
  - model
  - .
  - Our
  - response
  - ':'
  - Per
  - the
  - reviewer
  - '''s'
  - request
  - ','
  - we
  - tested
  - PPD
  - against
  - expectation
  - over
  - transformation
  - -LRB-
  - EoT
  - -RRB-
  - -LRB-
  - https://arxiv.org/abs/1707.07397
  - -RRB-
  - where
  - the
  - permutation
  - is
  - considered
  - as
  - the
  - transformation
  - .
  - '30'
  - PPD
  - models
  - -LRB-
  - with
  - different
  - permutations
  - -RRB-
  - are
  - used
  - for
  - EoT
  - .
  - The
  - adversarial
  - examples
  - are
  - then
  - fed
  - to
  - an
  - ensemble
  - of
  - '10'
  - PPD
  - models
  - -LRB-
  - with
  - different
  - permutations
  - from
  - the
  - '30'
  - models
  - -RRB-
  - .
  - Our
  - experiments
  - show
  - that
  - EoT
  - can
  - not
  - decrease
  - accuracy
  - more
  - than
  - an
  - adversary
  - that
  - attacks
  - with
  - a
  - single
  - model
  - .
  - One
  - possible
  - explanation
  - is
  - that
  - EoT
  - is
  - mostly
  - useful
  - in
  - the
  - case
  - that
  - sampling
  - a
  - few
  - transformations
  - provides
  - a
  - good
  - approximation
  - of
  - the
  - expectation
  - over
  - transformation
  - .
  - For
  - example
  - ','
  - two
  - scenarios
  - that
  - EoT
  - is
  - shown
  - to
  - be
  - successful
  - are
  - ':'
  - -LRB-
  - '1'
  - -RRB-
  - synthesizing
  - adversarial
  - examples
  - that
  - are
  - robust
  - to
  - camera
  - viewpoint
  - shift
  - and
  - -LRB-
  - '2'
  - -RRB-
  - breaking
  - a
  - defense
  - that
  - randomly
  - drops
  - pixels
  - of
  - the
  - image
  - and
  - replaces
  - them
  - with
  - total
  - variance
  - minimization
  - .
  - In
  - both
  - of
  - these
  - two
  - scenarios
  - ','
  - sampling
  - a
  - few
  - transformations
  - gives
  - a
  - good
  - idea
  - of
  - the
  - expectation
  - .
  - However
  - ','
  - in
  - PPD
  - ','
  - each
  - transformation
  - has
  - its
  - own
  - fingerprint
  - which
  - is
  - totally
  - different
  - from
  - others
  - .
- comment_id: B1eCntlsRQ
  rels:
  - !!python/tuple
    - 0
    - 16
    - 16
    - 943
    - elaboration
  - !!python/tuple
    - 16
    - 22
    - 22
    - 23
    - elaboration
  - !!python/tuple
    - 16
    - 23
    - 23
    - 44
    - elaboration
  - !!python/tuple
    - 23
    - 27
    - 27
    - 44
    - elaboration
  - !!python/tuple
    - 16
    - 44
    - 44
    - 943
    - elaboration
  - !!python/tuple
    - 44
    - 56
    - 56
    - 943
    - elaboration
  - !!python/tuple
    - 56
    - 74
    - 74
    - 943
    - example
  - !!python/tuple
    - 74
    - 117
    - 117
    - 943
    - elaboration
  - !!python/tuple
    - 117
    - 135
    - 135
    - 141
    - elaboration
  - !!python/tuple
    - 117
    - 141
    - 141
    - 943
    - elaboration
  - !!python/tuple
    - 141
    - 174
    - 174
    - 192
    - sequence
  - !!python/tuple
    - 141
    - 157
    - 157
    - 174
    - elaboration
  - !!python/tuple
    - 157
    - 166
    - 166
    - 174
    - elaboration
  - !!python/tuple
    - 174
    - 192
    - 141
    - 174
    - sequence
  - !!python/tuple
    - 174
    - 188
    - 188
    - 192
    - elaboration
  - !!python/tuple
    - 141
    - 192
    - 192
    - 943
    - elaboration
  - !!python/tuple
    - 192
    - 222
    - 222
    - 943
    - list
  - !!python/tuple
    - 192
    - 199
    - 199
    - 222
    - elaboration
  - !!python/tuple
    - 199
    - 205
    - 205
    - 222
    - reason
  - !!python/tuple
    - 205
    - 212
    - 212
    - 222
    - contrast
  - !!python/tuple
    - 212
    - 222
    - 205
    - 212
    - contrast
  - !!python/tuple
    - 222
    - 943
    - 192
    - 222
    - list
  - !!python/tuple
    - 222
    - 227
    - 227
    - 232
    - list
  - !!python/tuple
    - 227
    - 232
    - 222
    - 227
    - list
  - !!python/tuple
    - 222
    - 232
    - 232
    - 943
    - elaboration
  - !!python/tuple
    - 232
    - 239
    - 239
    - 264
    - elaboration
  - !!python/tuple
    - 239
    - 244
    - 244
    - 264
    - list
  - !!python/tuple
    - 244
    - 264
    - 239
    - 244
    - list
  - !!python/tuple
    - 244
    - 253
    - 253
    - 264
    - purpose
  - !!python/tuple
    - 232
    - 264
    - 264
    - 943
    - elaboration
  - !!python/tuple
    - 264
    - 274
    - 274
    - 943
    - elaboration
  - !!python/tuple
    - 276
    - 296
    - 274
    - 276
    - attribution
  - !!python/tuple
    - 274
    - 296
    - 296
    - 943
    - elaboration
  - !!python/tuple
    - 296
    - 314
    - 314
    - 322
    - purpose
  - !!python/tuple
    - 296
    - 322
    - 322
    - 943
    - elaboration
  - !!python/tuple
    - 322
    - 330
    - 330
    - 943
    - list
  - !!python/tuple
    - 330
    - 943
    - 322
    - 330
    - list
  - !!python/tuple
    - 330
    - 337
    - 337
    - 343
    - elaboration
  - !!python/tuple
    - 330
    - 343
    - 343
    - 943
    - elaboration
  - !!python/tuple
    - 343
    - 389
    - 389
    - 943
    - elaboration
  - !!python/tuple
    - 389
    - 404
    - 404
    - 418
    - elaboration
  - !!python/tuple
    - 404
    - 408
    - 408
    - 418
    - list
  - !!python/tuple
    - 408
    - 418
    - 404
    - 408
    - list
  - !!python/tuple
    - 408
    - 413
    - 413
    - 418
    - list
  - !!python/tuple
    - 413
    - 418
    - 408
    - 413
    - list
  - !!python/tuple
    - 389
    - 418
    - 418
    - 943
    - elaboration
  - !!python/tuple
    - 421
    - 437
    - 418
    - 421
    - attribution
  - !!python/tuple
    - 421
    - 427
    - 427
    - 437
    - elaboration
  - !!python/tuple
    - 418
    - 437
    - 437
    - 943
    - elaboration
  - !!python/tuple
    - 437
    - 449
    - 449
    - 943
    - elaboration
  - !!python/tuple
    - 449
    - 459
    - 459
    - 465
    - list
  - !!python/tuple
    - 459
    - 465
    - 449
    - 459
    - list
  - !!python/tuple
    - 449
    - 465
    - 465
    - 943
    - elaboration
  - !!python/tuple
    - 465
    - 479
    - 479
    - 943
    - elaboration
  - !!python/tuple
    - 479
    - 485
    - 485
    - 491
    - list
  - !!python/tuple
    - 485
    - 491
    - 479
    - 485
    - list
  - !!python/tuple
    - 479
    - 491
    - 491
    - 943
    - manner
  - !!python/tuple
    - 491
    - 498
    - 498
    - 506
    - elaboration
  - !!python/tuple
    - 491
    - 506
    - 506
    - 943
    - explanation
  - !!python/tuple
    - 506
    - 513
    - 513
    - 943
    - elaboration
  - !!python/tuple
    - 513
    - 550
    - 550
    - 943
    - topic
  - !!python/tuple
    - 513
    - 530
    - 530
    - 550
    - elaboration
  - !!python/tuple
    - 530
    - 547
    - 547
    - 550
    - same_unit
  - !!python/tuple
    - 530
    - 531
    - 531
    - 547
    - elaboration
  - !!python/tuple
    - 531
    - 543
    - 543
    - 547
    - elaboration
  - !!python/tuple
    - 547
    - 550
    - 530
    - 547
    - same_unit
  - !!python/tuple
    - 550
    - 943
    - 513
    - 550
    - topic
  - !!python/tuple
    - 550
    - 604
    - 604
    - 943
    - list
  - !!python/tuple
    - 550
    - 566
    - 566
    - 583
    - contrast
  - !!python/tuple
    - 566
    - 583
    - 550
    - 566
    - contrast
  - !!python/tuple
    - 550
    - 583
    - 583
    - 604
    - elaboration
  - !!python/tuple
    - 583
    - 588
    - 588
    - 604
    - elaboration
  - !!python/tuple
    - 588
    - 596
    - 596
    - 604
    - purpose
  - !!python/tuple
    - 596
    - 601
    - 601
    - 604
    - purpose
  - !!python/tuple
    - 604
    - 943
    - 550
    - 604
    - list
  - !!python/tuple
    - 604
    - 619
    - 619
    - 943
    - elaboration
  - !!python/tuple
    - 619
    - 624
    - 624
    - 943
    - textualorganization
  - !!python/tuple
    - 624
    - 943
    - 619
    - 624
    - textualorganization
  - !!python/tuple
    - 624
    - 632
    - 632
    - 943
    - elaboration
  - !!python/tuple
    - 632
    - 642
    - 642
    - 652
    - elaboration
  - !!python/tuple
    - 632
    - 652
    - 652
    - 943
    - elaboration
  - !!python/tuple
    - 652
    - 663
    - 663
    - 685
    - elaboration
  - !!python/tuple
    - 663
    - 674
    - 674
    - 685
    - elaboration
  - !!python/tuple
    - 674
    - 675
    - 675
    - 685
    - elaboration
  - !!python/tuple
    - 675
    - 679
    - 679
    - 685
    - elaboration
  - !!python/tuple
    - 652
    - 685
    - 685
    - 943
    - elaboration
  - !!python/tuple
    - 685
    - 710
    - 710
    - 727
    - elaboration
  - !!python/tuple
    - 710
    - 717
    - 717
    - 727
    - elaboration
  - !!python/tuple
    - 685
    - 727
    - 727
    - 943
    - elaboration
  - !!python/tuple
    - 731
    - 750
    - 727
    - 731
    - attribution
  - !!python/tuple
    - 731
    - 736
    - 736
    - 750
    - attribution
  - !!python/tuple
    - 736
    - 740
    - 740
    - 750
    - purpose
  - !!python/tuple
    - 742
    - 750
    - 740
    - 742
    - attribution
  - !!python/tuple
    - 727
    - 750
    - 750
    - 876
    - elaboration
  - !!python/tuple
    - 750
    - 759
    - 759
    - 778
    - purpose
  - !!python/tuple
    - 759
    - 765
    - 765
    - 778
    - reason
  - !!python/tuple
    - 765
    - 769
    - 769
    - 778
    - purpose
  - !!python/tuple
    - 750
    - 778
    - 778
    - 804
    - elaboration
  - !!python/tuple
    - 778
    - 782
    - 782
    - 804
    - elaboration
  - !!python/tuple
    - 782
    - 786
    - 786
    - 804
    - purpose
  - !!python/tuple
    - 786
    - 796
    - 796
    - 804
    - comparison
  - !!python/tuple
    - 750
    - 804
    - 804
    - 876
    - elaboration
  - !!python/tuple
    - 804
    - 809
    - 809
    - 876
    - textualorganization
  - !!python/tuple
    - 809
    - 876
    - 804
    - 809
    - textualorganization
  - !!python/tuple
    - 809
    - 826
    - 826
    - 847
    - same_unit
  - !!python/tuple
    - 809
    - 816
    - 816
    - 826
    - elaboration
  - !!python/tuple
    - 826
    - 847
    - 809
    - 826
    - same_unit
  - !!python/tuple
    - 826
    - 830
    - 830
    - 847
    - elaboration
  - !!python/tuple
    - 830
    - 835
    - 835
    - 847
    - elaboration
  - !!python/tuple
    - 809
    - 847
    - 847
    - 876
    - elaboration
  - !!python/tuple
    - 847
    - 859
    - 859
    - 876
    - same_unit
  - !!python/tuple
    - 847
    - 856
    - 856
    - 859
    - elaboration
  - !!python/tuple
    - 859
    - 876
    - 847
    - 859
    - same_unit
  - !!python/tuple
    - 859
    - 861
    - 861
    - 876
    - elaboration
  - !!python/tuple
    - 727
    - 876
    - 876
    - 943
    - elaboration
  - !!python/tuple
    - 876
    - 898
    - 898
    - 943
    - list
  - !!python/tuple
    - 876
    - 881
    - 881
    - 898
    - elaboration
  - !!python/tuple
    - 898
    - 943
    - 876
    - 898
    - list
  - !!python/tuple
    - 898
    - 907
    - 907
    - 917
    - elaboration
  - !!python/tuple
    - 898
    - 917
    - 917
    - 943
    - circumstance
  - !!python/tuple
    - 917
    - 921
    - 921
    - 943
    - elaboration
  - !!python/tuple
    - 921
    - 932
    - 932
    - 943
    - purpose
  - !!python/tuple
    - 932
    - 938
    - 938
    - 943
    - elaboration
  tokens:
  - This
  - paper
  - proposes
  - TopicGAN
  - ','
  - a
  - generative
  - adversarial
  - approach
  - to
  - topic
  - modeling
  - and
  - text
  - generation
  - .
  - The
  - model
  - basically
  - combines
  - two
  - steps
  - ':'
  - first
  - to
  - generate
  - words
  - -LRB-
  - bag-of-words
  - -RRB-
  - for
  - a
  - topic
  - ','
  - then
  - second
  - to
  - generate
  - the
  - sequence
  - of
  - the
  - words
  - .
  - While
  - the
  - idea
  - is
  - interesting
  - ','
  - there
  - are
  - several
  - important
  - limitations
  - .
  - First
  - ','
  - the
  - paper
  - is
  - difficult
  - to
  - understand
  - ','
  - and
  - some
  - of
  - the
  - explanations
  - are
  - not
  - convincing
  - .
  - For
  - example
  - ','
  - in
  - section
  - 4.1.1
  - ','
  - it
  - says
  - ''''''
  - '...'
  - our
  - method
  - assumes
  - that
  - the
  - documents
  - are
  - produced
  - from
  - a
  - single
  - topic
  - '...'
  - Our
  - assumption
  - aligns
  - well
  - with
  - human
  - intuition
  - that
  - most
  - documents
  - are
  - generated
  - from
  - a
  - single
  - main
  - topic
  - .
  - ''''''
  - This
  - goes
  - very
  - much
  - against
  - the
  - common
  - assumption
  - of
  - a
  - generative
  - topic
  - model
  - ','
  - such
  - as
  - LDA
  - ','
  - which
  - the
  - model
  - compares
  - against
  - .
  - I
  - do
  - n't
  - mean
  - to
  - argue
  - either
  - way
  - ','
  - but
  - if
  - the
  - paper
  - presents
  - a
  - viewpoint
  - which
  - is
  - quite
  - different
  - from
  - the
  - commonly
  - accepted
  - viewpoint
  - -LRB-
  - within
  - the
  - specific
  - research
  - field
  - -RRB-
  - ','
  - then
  - there
  - needs
  - to
  - be
  - a
  - much
  - deeper
  - explanation
  - ','
  - ideally
  - with
  - concrete
  - evidence
  - to
  - support
  - it
  - .
  - Another
  - sentence
  - from
  - the
  - same
  - paragraph
  - states
  - that
  - their
  - '``'
  - model
  - outperforms
  - LDA
  - because
  - LDA
  - is
  - a
  - statistical
  - model
  - ','
  - while
  - our
  - generator
  - is
  - a
  - deep
  - generative
  - model
  - .
  - ''''''
  - This
  - argument
  - also
  - seems
  - flawed
  - and
  - without
  - concrete
  - evidence
  - .
  - There
  - are
  - other
  - parts
  - in
  - the
  - paper
  - where
  - the
  - logic
  - seems
  - strange
  - and
  - without
  - evidence
  - ','
  - and
  - they
  - make
  - it
  - difficult
  - to
  - understand
  - and
  - accept
  - the
  - major
  - claims
  - of
  - the
  - paper
  - .
  - Second
  - ','
  - the
  - model
  - does
  - not
  - offer
  - much
  - novelty
  - .
  - It
  - seems
  - that
  - the
  - two-stage
  - model
  - simply
  - puts
  - the
  - two
  - pieces
  - ','
  - a
  - GAN-style
  - generator
  - and
  - an
  - LSTM
  - sequence
  - model
  - together
  - .
  - Perhaps
  - I
  - am
  - not
  - understanding
  - the
  - model
  - ','
  - but
  - the
  - model
  - description
  - was
  - also
  - not
  - clear
  - nor
  - easy
  - to
  - understand
  - with
  - respect
  - to
  - its
  - novelty
  - .
  - Third
  - ','
  - the
  - evaluation
  - is
  - somewhat
  - weak
  - .
  - There
  - are
  - two
  - main
  - evaluations
  - tasks
  - ':'
  - text
  - classification
  - and
  - text
  - generation
  - .
  - For
  - the
  - first
  - task
  - ','
  - classification
  - is
  - not
  - the
  - main
  - purpose
  - of
  - topic
  - models
  - ','
  - and
  - while
  - text
  - classification
  - _
  - is
  - _
  - used
  - in
  - many
  - topic
  - modeling
  - papers
  - ','
  - it
  - is
  - almost
  - always
  - accompanied
  - by
  - other
  - evaluation
  - metrics
  - such
  - as
  - held-out
  - perplexity
  - and
  - topic
  - coherence
  - .
  - This
  - is
  - because
  - the
  - main
  - purpose
  - of
  - topic
  - modeling
  - is
  - to
  - actually
  - infer
  - the
  - topics
  - -LRB-
  - per-topic
  - word
  - distribution
  - and
  - per-document
  - topic
  - distribution
  - -RRB-
  - and
  - model
  - the
  - corpus
  - .
  - Thus
  - I
  - feel
  - it
  - is
  - not
  - a
  - fair
  - evaluation
  - to
  - just
  - compare
  - the
  - models
  - using
  - text
  - classification
  - tasks
  - .
  - The
  - second
  - evaluation
  - task
  - of
  - text
  - generation
  - is
  - not
  - explained
  - enough
  - .
  - For
  - the
  - human
  - evaluation
  - ','
  - who
  - were
  - the
  - annotators
  - ','
  - and
  - how
  - were
  - they
  - trained
  - '?'
  - How
  - many
  - people
  - annotated
  - each
  - output
  - ','
  - and
  - what
  - was
  - the
  - inter-rater
  - agreement
  - '?'
  - How
  - many
  - sentences
  - were
  - evaluated
  - ','
  - and
  - how
  - were
  - they
  - chosen
  - '?'
  - Without
  - these
  - details
  - ','
  - it
  - is
  - difficult
  - to
  - judge
  - whether
  - this
  - evaluation
  - was
  - valid
  - .
  - Lastly
  - ','
  - the
  - results
  - are
  - mediocre
  - .
  - Besides
  - the
  - classification
  - task
  - ','
  - the
  - others
  - do
  - not
  - show
  - significant
  - improvements
  - over
  - the
  - baseline
  - models
  - .
  - Perplexity
  - -LRB-
  - table
  - '3'
  - -RRB-
  - shows
  - similar
  - results
  - for
  - DBPedia
  - and
  - worse
  - results
  - -LRB-
  - than
  - WGAN-gp
  - -RRB-
  - for
  - Gigaword
  - .
  - Table
  - '4'
  - shows
  - slightly
  - better
  - results
  - for
  - '``'
  - Preference
  - ''''''
  - for
  - TopicGAN
  - with
  - joint
  - training
  - ','
  - but
  - '``'
  - Accuracy
  - ''''''
  - is
  - measured
  - only
  - for
  - the
  - proposed
  - model
  - and
  - not
  - the
  - baseline
  - model
  - .
  - -LRB-
  - '1'
  - -RRB-
  - Writing
  - ':'
  - We
  - have
  - rewritten
  - many
  - parts
  - of
  - the
  - article
  - to
  - make
  - the
  - paper
  - easier
  - to
  - understand
  - .
  - In
  - addition
  - ','
  - some
  - not
  - convincing
  - explanations
  - mentioned
  - in
  - the
  - review
  - are
  - also
  - revised
  - .
  - -LRB-
  - '2'
  - -RRB-
  - Assuming
  - documents
  - are
  - generated
  - from
  - one
  - single
  - main
  - topic
  - ':'
  - In
  - our
  - experiments
  - ','
  - we
  - conduct
  - unsupervised
  - document
  - classification
  - ','
  - in
  - which
  - the
  - documents
  - have
  - only
  - one
  - single
  - class
  - .
  - Therefore
  - ','
  - for
  - those
  - unsupervised
  - classification
  - experiments
  - ','
  - assuming
  - each
  - documents
  - coming
  - from
  - a
  - single
  - main
  - topic
  - is
  - a
  - more
  - appropriate
  - assumption
  - ','
  - which
  - allows
  - our
  - model
  - to
  - learn
  - more
  - distinct
  - topics
  - .
  - In
  - addition
  - ','
  - as
  - the
  - length
  - of
  - our
  - training
  - documents
  - is
  - short
  - ','
  - it
  - '''s'
  - hard
  - to
  - break
  - the
  - short
  - text
  - into
  - several
  - topics
  - ','
  - which
  - is
  - one
  - of
  - the
  - possible
  - reason
  - that
  - makes
  - LDA
  - works
  - not
  - well
  - 'on'
  - short
  - text
  - .
  - However
  - ','
  - we
  - acknowledge
  - that
  - for
  - long
  - documents
  - ','
  - it
  - '''s'
  - more
  - appropriate
  - to
  - assume
  - they
  - come
  - from
  - the
  - mixture
  - of
  - topics
  - .
  - In
  - fact
  - ','
  - it
  - '''s'
  - feasible
  - for
  - our
  - method
  - to
  - generate
  - documents
  - from
  - several
  - topics
  - because
  - info-GAN
  - allows
  - us
  - to
  - decide
  - the
  - distribution
  - of
  - the
  - predicted
  - code
  - .
  - We
  - are
  - conducting
  - experiments
  - 'on'
  - using
  - several
  - topics
  - to
  - generate
  - longer
  - documents
  - and
  - the
  - current
  - result
  - seems
  - better
  - than
  - generating
  - from
  - one
  - single
  - main
  - topic
  - .
  - -LRB-
  - '3'
  - -RRB-
  - Novelty
  - ':'
  - The
  - novelty
  - of
  - our
  - work
  - is
  - that
  - -LRB-
  - a
  - -RRB-
  - as
  - far
  - as
  - we
  - know
  - ','
  - there
  - is
  - 'no'
  - previous
  - work
  - which
  - tries
  - to
  - use
  - GAN
  - to
  - achieve
  - topic
  - modeling
  - ','
  - which
  - is
  - a
  - worth
  - exploring
  - direction
  - .
  - -LRB-
  - b
  - -RRB-
  - Some
  - extra
  - tricks
  - for
  - Info-GAN
  - training
  - -LRB-
  - c
  - -RRB-
  - Two
  - steps
  - generation
  - of
  - text
  - may
  - also
  - be
  - a
  - better
  - and
  - easier
  - method
  - for
  - generating
  - text
  - .
  - -LRB-
  - '4'
  - -RRB-
  - Evaluation
  - ':'
  - We
  - have
  - evaluated
  - the
  - topic
  - coherence
  - score
  - and
  - reported
  - the
  - score
  - 'on'
  - revised
  - paper
  - Table
  - '3'
  - .
  - Our
  - method
  - outperformed
  - baseline
  - method
  - 'on'
  - all
  - datasets
  - ','
  - which
  - implies
  - the
  - effectiveness
  - of
  - our
  - proposed
  - topic
  - model
  - .
  - When
  - conducting
  - human
  - evaluation
  - to
  - evaluate
  - the
  - quality
  - of
  - sentences
  - ','
  - we
  - asked
  - '17'
  - annotators
  - to
  - compare
  - '13'
  - sets
  - of
  - sentences
  - generated
  - by
  - different
  - methods
  - .
- comment_id: B1e3U8GQJ4
  rels:
  - !!python/tuple
    - 0
    - 6
    - 6
    - 26
    - purpose
  - !!python/tuple
    - 6
    - 14
    - 14
    - 26
    - same_unit
  - !!python/tuple
    - 6
    - 10
    - 10
    - 14
    - elaboration
  - !!python/tuple
    - 14
    - 26
    - 6
    - 14
    - same_unit
  - !!python/tuple
    - 14
    - 21
    - 21
    - 26
    - purpose
  - !!python/tuple
    - 21
    - 25
    - 25
    - 26
    - elaboration
  - !!python/tuple
    - 0
    - 26
    - 26
    - 1583
    - elaboration
  - !!python/tuple
    - 26
    - 31
    - 31
    - 54
    - elaboration
  - !!python/tuple
    - 31
    - 39
    - 39
    - 54
    - elaboration
  - !!python/tuple
    - 26
    - 54
    - 54
    - 78
    - means
  - !!python/tuple
    - 26
    - 78
    - 78
    - 1583
    - elaboration
  - !!python/tuple
    - 78
    - 89
    - 89
    - 1583
    - elaboration
  - !!python/tuple
    - 89
    - 92
    - 92
    - 101
    - elaboration
  - !!python/tuple
    - 92
    - 96
    - 96
    - 101
    - purpose
  - !!python/tuple
    - 89
    - 101
    - 101
    - 1583
    - elaboration
  - !!python/tuple
    - 101
    - 121
    - 121
    - 1583
    - elaboration
  - !!python/tuple
    - 121
    - 126
    - 126
    - 131
    - purpose
  - !!python/tuple
    - 121
    - 131
    - 131
    - 144
    - elaboration
  - !!python/tuple
    - 121
    - 144
    - 144
    - 1583
    - elaboration
  - !!python/tuple
    - 144
    - 148
    - 148
    - 1583
    - elaboration
  - !!python/tuple
    - 150
    - 188
    - 148
    - 150
    - attribution
  - !!python/tuple
    - 150
    - 166
    - 166
    - 188
    - contrast
  - !!python/tuple
    - 150
    - 163
    - 163
    - 166
    - elaboration
  - !!python/tuple
    - 166
    - 188
    - 150
    - 166
    - contrast
  - !!python/tuple
    - 166
    - 171
    - 171
    - 188
    - elaboration
  - !!python/tuple
    - 148
    - 188
    - 188
    - 198
    - elaboration
  - !!python/tuple
    - 148
    - 198
    - 198
    - 1583
    - elaboration
  - !!python/tuple
    - 198
    - 214
    - 214
    - 230
    - same_unit
  - !!python/tuple
    - 198
    - 211
    - 211
    - 214
    - same_unit
  - !!python/tuple
    - 198
    - 206
    - 206
    - 211
    - elaboration
  - !!python/tuple
    - 211
    - 214
    - 198
    - 211
    - same_unit
  - !!python/tuple
    - 214
    - 230
    - 198
    - 214
    - same_unit
  - !!python/tuple
    - 214
    - 220
    - 220
    - 230
    - elaboration
  - !!python/tuple
    - 220
    - 229
    - 229
    - 230
    - same_unit
  - !!python/tuple
    - 220
    - 221
    - 221
    - 223
    - elaboration
  - !!python/tuple
    - 220
    - 223
    - 223
    - 229
    - elaboration
  - !!python/tuple
    - 229
    - 230
    - 220
    - 229
    - same_unit
  - !!python/tuple
    - 198
    - 230
    - 230
    - 1583
    - elaboration
  - !!python/tuple
    - 230
    - 261
    - 261
    - 1583
    - elaboration
  - !!python/tuple
    - 261
    - 266
    - 266
    - 282
    - purpose
  - !!python/tuple
    - 261
    - 282
    - 282
    - 1583
    - explanation
  - !!python/tuple
    - 282
    - 288
    - 288
    - 301
    - purpose
  - !!python/tuple
    - 288
    - 292
    - 292
    - 301
    - elaboration
  - !!python/tuple
    - 282
    - 301
    - 301
    - 313
    - elaboration
  - !!python/tuple
    - 282
    - 313
    - 313
    - 1583
    - elaboration
  - !!python/tuple
    - 313
    - 318
    - 318
    - 331
    - means
  - !!python/tuple
    - 318
    - 324
    - 324
    - 331
    - elaboration
  - !!python/tuple
    - 313
    - 331
    - 331
    - 1583
    - elaboration
  - !!python/tuple
    - 338
    - 346
    - 331
    - 338
    - attribution
  - !!python/tuple
    - 331
    - 346
    - 346
    - 1583
    - elaboration
  - !!python/tuple
    - 346
    - 368
    - 368
    - 1583
    - elaboration
  - !!python/tuple
    - 368
    - 386
    - 386
    - 1583
    - list
  - !!python/tuple
    - 386
    - 1583
    - 368
    - 386
    - list
  - !!python/tuple
    - 386
    - 391
    - 391
    - 405
    - purpose
  - !!python/tuple
    - 386
    - 405
    - 405
    - 1583
    - elaboration
  - !!python/tuple
    - 405
    - 418
    - 418
    - 434
    - purpose
  - !!python/tuple
    - 405
    - 434
    - 434
    - 1583
    - elaboration
  - !!python/tuple
    - 438
    - 449
    - 434
    - 438
    - attribution
  - !!python/tuple
    - 434
    - 449
    - 449
    - 1583
    - elaboration
  - !!python/tuple
    - 449
    - 486
    - 486
    - 493
    - elaboration
  - !!python/tuple
    - 449
    - 493
    - 493
    - 494
    - elaboration
  - !!python/tuple
    - 449
    - 494
    - 494
    - 1583
    - elaboration
  - !!python/tuple
    - 496
    - 538
    - 494
    - 496
    - attribution
  - !!python/tuple
    - 496
    - 499
    - 499
    - 538
    - elaboration
  - !!python/tuple
    - 499
    - 512
    - 512
    - 538
    - elaboration
  - !!python/tuple
    - 512
    - 523
    - 523
    - 538
    - same_unit
  - !!python/tuple
    - 512
    - 516
    - 516
    - 523
    - elaboration
  - !!python/tuple
    - 523
    - 538
    - 512
    - 523
    - same_unit
  - !!python/tuple
    - 523
    - 525
    - 525
    - 538
    - elaboration
  - !!python/tuple
    - 525
    - 532
    - 532
    - 538
    - elaboration
  - !!python/tuple
    - 494
    - 538
    - 538
    - 1583
    - elaboration
  - !!python/tuple
    - 538
    - 539
    - 539
    - 540
    - elaboration
  - !!python/tuple
    - 538
    - 540
    - 540
    - 547
    - elaboration
  - !!python/tuple
    - 538
    - 547
    - 547
    - 1583
    - elaboration
  - !!python/tuple
    - 547
    - 548
    - 548
    - 549
    - elaboration
  - !!python/tuple
    - 547
    - 549
    - 549
    - 573
    - elaboration
  - !!python/tuple
    - 549
    - 557
    - 557
    - 573
    - elaboration
  - !!python/tuple
    - 557
    - 566
    - 566
    - 573
    - list
  - !!python/tuple
    - 566
    - 573
    - 557
    - 566
    - list
  - !!python/tuple
    - 547
    - 573
    - 573
    - 1583
    - example
  - !!python/tuple
    - 573
    - 585
    - 585
    - 1583
    - same_unit
  - !!python/tuple
    - 573
    - 575
    - 575
    - 585
    - elaboration
  - !!python/tuple
    - 585
    - 1583
    - 573
    - 585
    - same_unit
  - !!python/tuple
    - 585
    - 601
    - 601
    - 1583
    - list
  - !!python/tuple
    - 585
    - 600
    - 600
    - 601
    - elaboration
  - !!python/tuple
    - 601
    - 1583
    - 585
    - 601
    - list
  - !!python/tuple
    - 601
    - 614
    - 614
    - 1583
    - textualorganization
  - !!python/tuple
    - 614
    - 1583
    - 601
    - 614
    - textualorganization
  - !!python/tuple
    - 614
    - 616
    - 616
    - 1583
    - textualorganization
  - !!python/tuple
    - 616
    - 1583
    - 614
    - 616
    - textualorganization
  - !!python/tuple
    - 616
    - 643
    - 643
    - 1583
    - elaboration
  - !!python/tuple
    - 643
    - 655
    - 655
    - 1583
    - list
  - !!python/tuple
    - 655
    - 1583
    - 643
    - 655
    - list
  - !!python/tuple
    - 655
    - 662
    - 662
    - 1583
    - list
  - !!python/tuple
    - 662
    - 1583
    - 655
    - 662
    - list
  - !!python/tuple
    - 662
    - 713
    - 713
    - 1583
    - topic
  - !!python/tuple
    - 662
    - 674
    - 674
    - 713
    - question
  - !!python/tuple
    - 674
    - 713
    - 662
    - 674
    - question
  - !!python/tuple
    - 674
    - 675
    - 675
    - 713
    - elaboration
  - !!python/tuple
    - 675
    - 679
    - 679
    - 713
    - elaboration
  - !!python/tuple
    - 679
    - 691
    - 691
    - 713
    - elaboration
  - !!python/tuple
    - 691
    - 696
    - 696
    - 713
    - elaboration
  - !!python/tuple
    - 696
    - 700
    - 700
    - 713
    - elaboration
  - !!python/tuple
    - 713
    - 1583
    - 662
    - 713
    - topic
  - !!python/tuple
    - 713
    - 1386
    - 1386
    - 1583
    - list
  - !!python/tuple
    - 713
    - 715
    - 715
    - 1386
    - elaboration
  - !!python/tuple
    - 715
    - 730
    - 730
    - 1386
    - elaboration
  - !!python/tuple
    - 732
    - 754
    - 730
    - 732
    - attribution
  - !!python/tuple
    - 732
    - 737
    - 737
    - 754
    - elaboration
  - !!python/tuple
    - 737
    - 747
    - 747
    - 754
    - list
  - !!python/tuple
    - 747
    - 754
    - 737
    - 747
    - list
  - !!python/tuple
    - 730
    - 754
    - 754
    - 1386
    - elaboration
  - !!python/tuple
    - 762
    - 771
    - 754
    - 762
    - attribution
  - !!python/tuple
    - 762
    - 767
    - 767
    - 771
    - elaboration
  - !!python/tuple
    - 754
    - 771
    - 771
    - 1386
    - elaboration
  - !!python/tuple
    - 771
    - 773
    - 773
    - 1386
    - elaboration
  - !!python/tuple
    - 773
    - 788
    - 788
    - 792
    - elaboration
  - !!python/tuple
    - 773
    - 792
    - 792
    - 1386
    - elaboration
  - !!python/tuple
    - 792
    - 799
    - 799
    - 1386
    - textualorganization
  - !!python/tuple
    - 799
    - 1386
    - 792
    - 799
    - textualorganization
  - !!python/tuple
    - 799
    - 801
    - 801
    - 803
    - elaboration
  - !!python/tuple
    - 799
    - 803
    - 803
    - 1386
    - elaboration
  - !!python/tuple
    - 803
    - 831
    - 831
    - 837
    - elaboration
  - !!python/tuple
    - 803
    - 837
    - 837
    - 1386
    - elaboration
  - !!python/tuple
    - 837
    - 845
    - 845
    - 867
    - elaboration
  - !!python/tuple
    - 845
    - 846
    - 846
    - 867
    - elaboration
  - !!python/tuple
    - 846
    - 861
    - 861
    - 867
    - elaboration
  - !!python/tuple
    - 837
    - 867
    - 867
    - 1386
    - elaboration
  - !!python/tuple
    - 867
    - 871
    - 871
    - 879
    - elaboration
  - !!python/tuple
    - 867
    - 879
    - 879
    - 1386
    - elaboration
  - !!python/tuple
    - 879
    - 880
    - 880
    - 890
    - elaboration
  - !!python/tuple
    - 880
    - 886
    - 886
    - 890
    - elaboration
  - !!python/tuple
    - 879
    - 890
    - 890
    - 896
    - elaboration
  - !!python/tuple
    - 890
    - 892
    - 892
    - 896
    - elaboration
  - !!python/tuple
    - 879
    - 896
    - 896
    - 1386
    - elaboration
  - !!python/tuple
    - 896
    - 898
    - 898
    - 954
    - elaboration
  - !!python/tuple
    - 898
    - 900
    - 900
    - 922
    - list
  - !!python/tuple
    - 900
    - 922
    - 898
    - 900
    - list
  - !!python/tuple
    - 900
    - 905
    - 905
    - 922
    - list
  - !!python/tuple
    - 905
    - 922
    - 900
    - 905
    - list
  - !!python/tuple
    - 905
    - 917
    - 917
    - 922
    - elaboration
  - !!python/tuple
    - 898
    - 922
    - 922
    - 954
    - elaboration
  - !!python/tuple
    - 922
    - 932
    - 932
    - 954
    - elaboration
  - !!python/tuple
    - 936
    - 954
    - 932
    - 936
    - attribution
  - !!python/tuple
    - 936
    - 944
    - 944
    - 954
    - list
  - !!python/tuple
    - 944
    - 954
    - 936
    - 944
    - list
  - !!python/tuple
    - 944
    - 950
    - 950
    - 954
    - list
  - !!python/tuple
    - 950
    - 954
    - 944
    - 950
    - list
  - !!python/tuple
    - 896
    - 954
    - 954
    - 1386
    - elaboration
  - !!python/tuple
    - 954
    - 961
    - 961
    - 969
    - purpose
  - !!python/tuple
    - 961
    - 965
    - 965
    - 969
    - elaboration
  - !!python/tuple
    - 954
    - 969
    - 969
    - 1386
    - elaboration
  - !!python/tuple
    - 969
    - 971
    - 971
    - 1007
    - elaboration
  - !!python/tuple
    - 971
    - 987
    - 987
    - 1007
    - elaboration
  - !!python/tuple
    - 987
    - 991
    - 991
    - 1007
    - purpose
  - !!python/tuple
    - 969
    - 1007
    - 1007
    - 1386
    - elaboration
  - !!python/tuple
    - 1007
    - 1027
    - 1027
    - 1098
    - list
  - !!python/tuple
    - 1007
    - 1014
    - 1014
    - 1027
    - purpose
  - !!python/tuple
    - 1014
    - 1018
    - 1018
    - 1027
    - elaboration
  - !!python/tuple
    - 1027
    - 1098
    - 1007
    - 1027
    - list
  - !!python/tuple
    - 1027
    - 1029
    - 1029
    - 1098
    - elaboration
  - !!python/tuple
    - 1029
    - 1058
    - 1058
    - 1069
    - elaboration
  - !!python/tuple
    - 1058
    - 1062
    - 1062
    - 1069
    - elaboration
  - !!python/tuple
    - 1029
    - 1069
    - 1069
    - 1098
    - reason
  - !!python/tuple
    - 1069
    - 1077
    - 1077
    - 1082
    - elaboration
  - !!python/tuple
    - 1069
    - 1082
    - 1082
    - 1098
    - elaboration
  - !!python/tuple
    - 1082
    - 1089
    - 1089
    - 1098
    - same_unit
  - !!python/tuple
    - 1082
    - 1086
    - 1086
    - 1089
    - elaboration
  - !!python/tuple
    - 1089
    - 1098
    - 1082
    - 1089
    - same_unit
  - !!python/tuple
    - 1007
    - 1098
    - 1098
    - 1386
    - elaboration
  - !!python/tuple
    - 1098
    - 1109
    - 1109
    - 1386
    - elaboration
  - !!python/tuple
    - 1133
    - 1137
    - 1109
    - 1133
    - attribution
  - !!python/tuple
    - 1109
    - 1114
    - 1114
    - 1133
    - elaboration
  - !!python/tuple
    - 1114
    - 1129
    - 1129
    - 1132
    - elaboration
  - !!python/tuple
    - 1114
    - 1132
    - 1132
    - 1133
    - means
  - !!python/tuple
    - 1109
    - 1137
    - 1137
    - 1386
    - elaboration
  - !!python/tuple
    - 1137
    - 1150
    - 1150
    - 1180
    - elaboration
  - !!python/tuple
    - 1152
    - 1180
    - 1150
    - 1152
    - attribution
  - !!python/tuple
    - 1152
    - 1163
    - 1163
    - 1180
    - elaboration
  - !!python/tuple
    - 1163
    - 1171
    - 1171
    - 1180
    - purpose
  - !!python/tuple
    - 1171
    - 1174
    - 1174
    - 1180
    - purpose
  - !!python/tuple
    - 1137
    - 1180
    - 1180
    - 1386
    - elaboration
  - !!python/tuple
    - 1180
    - 1191
    - 1191
    - 1203
    - list
  - !!python/tuple
    - 1191
    - 1203
    - 1180
    - 1191
    - list
  - !!python/tuple
    - 1193
    - 1203
    - 1191
    - 1193
    - attribution
  - !!python/tuple
    - 1180
    - 1203
    - 1203
    - 1386
    - elaboration
  - !!python/tuple
    - 1203
    - 1205
    - 1205
    - 1386
    - textualorganization
  - !!python/tuple
    - 1203
    - 1204
    - 1204
    - 1205
    - elaboration
  - !!python/tuple
    - 1205
    - 1386
    - 1203
    - 1205
    - textualorganization
  - !!python/tuple
    - 1205
    - 1216
    - 1216
    - 1386
    - question
  - !!python/tuple
    - 1216
    - 1386
    - 1205
    - 1216
    - question
  - !!python/tuple
    - 1216
    - 1217
    - 1217
    - 1218
    - elaboration
  - !!python/tuple
    - 1216
    - 1218
    - 1218
    - 1280
    - elaboration
  - !!python/tuple
    - 1218
    - 1220
    - 1220
    - 1245
    - elaboration
  - !!python/tuple
    - 1220
    - 1234
    - 1234
    - 1245
    - elaboration
  - !!python/tuple
    - 1218
    - 1245
    - 1245
    - 1280
    - elaboration
  - !!python/tuple
    - 1245
    - 1250
    - 1250
    - 1280
    - elaboration
  - !!python/tuple
    - 1250
    - 1266
    - 1266
    - 1280
    - same_unit
  - !!python/tuple
    - 1250
    - 1253
    - 1253
    - 1266
    - elaboration
  - !!python/tuple
    - 1266
    - 1280
    - 1250
    - 1266
    - same_unit
  - !!python/tuple
    - 1216
    - 1280
    - 1280
    - 1386
    - elaboration
  - !!python/tuple
    - 1280
    - 1282
    - 1282
    - 1303
    - elaboration
  - !!python/tuple
    - 1282
    - 1286
    - 1286
    - 1303
    - purpose
  - !!python/tuple
    - 1286
    - 1290
    - 1290
    - 1303
    - elaboration
  - !!python/tuple
    - 1280
    - 1303
    - 1303
    - 1386
    - elaboration
  - !!python/tuple
    - 1303
    - 1305
    - 1305
    - 1318
    - elaboration
  - !!python/tuple
    - 1310
    - 1318
    - 1305
    - 1310
    - attribution
  - !!python/tuple
    - 1310
    - 1311
    - 1311
    - 1318
    - elaboration
  - !!python/tuple
    - 1303
    - 1318
    - 1318
    - 1386
    - elaboration
  - !!python/tuple
    - 1318
    - 1341
    - 1341
    - 1386
    - elaboration
  - !!python/tuple
    - 1341
    - 1343
    - 1343
    - 1368
    - elaboration
  - !!python/tuple
    - 1343
    - 1352
    - 1352
    - 1368
    - elaboration
  - !!python/tuple
    - 1354
    - 1368
    - 1352
    - 1354
    - attribution
  - !!python/tuple
    - 1341
    - 1368
    - 1368
    - 1386
    - elaboration
  - !!python/tuple
    - 1377
    - 1386
    - 1368
    - 1377
    - attribution
  - !!python/tuple
    - 1386
    - 1583
    - 713
    - 1386
    - list
  - !!python/tuple
    - 1386
    - 1390
    - 1390
    - 1395
    - elaboration
  - !!python/tuple
    - 1390
    - 1391
    - 1391
    - 1395
    - elaboration
  - !!python/tuple
    - 1386
    - 1395
    - 1395
    - 1583
    - elaboration
  - !!python/tuple
    - 1395
    - 1415
    - 1415
    - 1423
    - same_unit
  - !!python/tuple
    - 1395
    - 1407
    - 1407
    - 1415
    - elaboration
  - !!python/tuple
    - 1415
    - 1423
    - 1395
    - 1415
    - same_unit
  - !!python/tuple
    - 1395
    - 1423
    - 1423
    - 1583
    - elaboration
  - !!python/tuple
    - 1423
    - 1425
    - 1425
    - 1583
    - elaboration
  - !!python/tuple
    - 1427
    - 1436
    - 1425
    - 1427
    - attribution
  - !!python/tuple
    - 1425
    - 1436
    - 1436
    - 1583
    - elaboration
  - !!python/tuple
    - 1440
    - 1463
    - 1436
    - 1440
    - attribution
  - !!python/tuple
    - 1436
    - 1463
    - 1463
    - 1583
    - elaboration
  - !!python/tuple
    - 1463
    - 1485
    - 1485
    - 1583
    - list
  - !!python/tuple
    - 1463
    - 1478
    - 1478
    - 1485
    - elaboration
  - !!python/tuple
    - 1485
    - 1583
    - 1463
    - 1485
    - list
  - !!python/tuple
    - 1485
    - 1487
    - 1487
    - 1583
    - list
  - !!python/tuple
    - 1487
    - 1583
    - 1485
    - 1487
    - list
  - !!python/tuple
    - 1507
    - 1583
    - 1487
    - 1507
    - attribution
  - !!python/tuple
    - 1507
    - 1520
    - 1520
    - 1583
    - question
  - !!python/tuple
    - 1513
    - 1520
    - 1507
    - 1513
    - attribution
  - !!python/tuple
    - 1520
    - 1583
    - 1507
    - 1520
    - question
  - !!python/tuple
    - 1521
    - 1558
    - 1520
    - 1521
    - attribution
  - !!python/tuple
    - 1527
    - 1558
    - 1521
    - 1527
    - attribution
  - !!python/tuple
    - 1527
    - 1542
    - 1542
    - 1558
    - elaboration
  - !!python/tuple
    - 1542
    - 1549
    - 1549
    - 1558
    - purpose
  - !!python/tuple
    - 1520
    - 1558
    - 1558
    - 1583
    - elaboration
  - !!python/tuple
    - 1558
    - 1566
    - 1566
    - 1583
    - elaboration
  - !!python/tuple
    - 1566
    - 1577
    - 1577
    - 1583
    - elaboration
  tokens:
  - This
  - works
  - propose
  - a
  - new
  - approach
  - to
  - learn
  - to
  - sample
  - -LRB-
  - or
  - generate
  - -RRB-
  - the
  - parameters
  - of
  - a
  - deep
  - neural
  - networks
  - to
  - solve
  - a
  - task
  - .
  - They
  - propose
  - a
  - new
  - architecture
  - inspired
  - by
  - hyper
  - networks
  - and
  - adversarial
  - auto-encoders
  - ','
  - where
  - the
  - parameters
  - of
  - the
  - networks
  - are
  - generated
  - from
  - a
  - low
  - dimensional
  - latent
  - space
  - .
  - By
  - using
  - an
  - ensemble
  - of
  - networks
  - sampled
  - with
  - their
  - approach
  - they
  - '''re'
  - able
  - to
  - get
  - state
  - of
  - the
  - art
  - results
  - 'on'
  - uncertainty
  - estimation
  - .
  - The
  - notations
  - are
  - confusing
  - and
  - the
  - paper
  - contains
  - several
  - mistakes
  - .
  - In
  - particular
  - ':'
  - '-'
  - P_z
  - is
  - used
  - to
  - represent
  - different
  - distributions
  - .
  - It
  - sometimes
  - refers
  - to
  - the
  - distribution
  - of
  - the
  - latent
  - variables
  - and
  - sometimes
  - to
  - the
  - prior
  - over
  - the
  - weight
  - embeddings
  - .
  - Different
  - notation
  - should
  - be
  - used
  - to
  - represent
  - different
  - quantity
  - .
  - '-'
  - D_z
  - sometimes
  - refers
  - to
  - the
  - regularization
  - term
  - or
  - to
  - the
  - discriminator
  - .
  - '-'
  - Eq
  - '2'
  - .
  - I
  - believe
  - there
  - is
  - a
  - bug
  - in
  - the
  - equation
  - ','
  - the
  - expectation
  - is
  - over
  - Q
  - -LRB-
  - z
  - -RRB-
  - but
  - it
  - should
  - be
  - P_z
  - -LRB-
  - distribution
  - of
  - the
  - latent
  - variable
  - z
  - -RRB-
  - ','
  - otherwise
  - it
  - does
  - n't
  - make
  - much
  - sense
  - .
  - '-'
  - The
  - equation
  - for
  - the
  - cross
  - entropy
  - is
  - wrong
  - .
  - If
  - y_i
  - are
  - the
  - 'true'
  - labels
  - and
  - F
  - -LRB-
  - x_i
  - ','
  - theta
  - -RRB-
  - is
  - the
  - prediction
  - then
  - it
  - should
  - be
  - y_i
  - '*'
  - log
  - -LRB-
  - F
  - -LRB-
  - x_i
  - ','
  - theta
  - -RRB-
  - -RRB-
  - .
  - '-'
  - It
  - '''s'
  - not
  - clear
  - if
  - the
  - loss
  - of
  - the
  - discriminator
  - should
  - be
  - maximized
  - for
  - the
  - parameters
  - of
  - the
  - discriminator
  - and
  - minimized
  - with
  - respect
  - to
  - the
  - parameters
  - of
  - the
  - encoder
  - .
  - Furthermore
  - it
  - would
  - be
  - interesting
  - to
  - study
  - what
  - is
  - the
  - impact
  - of
  - this
  - particular
  - choice
  - of
  - loss
  - for
  - the
  - discriminator
  - .
  - In
  - particular
  - I
  - invite
  - the
  - author
  - to
  - compare
  - the
  - loss
  - proposed
  - to
  - the
  - loss
  - in
  - -LSB-
  - '1'
  - -RSB-
  - .
  - Fixing
  - these
  - ','
  - would
  - make
  - the
  - paper
  - much
  - easier
  - to
  - understand
  - .
  - The
  - authors
  - motivates
  - their
  - approach
  - by
  - drawing
  - a
  - link
  - with
  - wasserstein
  - -LRB-
  - WAE
  - -RRB-
  - and
  - adversarial
  - auto-encoders
  - .
  - While
  - this
  - could
  - be
  - interesting
  - I
  - think
  - this
  - link
  - should
  - be
  - made
  - more
  - formal
  - .
  - Indeed
  - ','
  - the
  - WAE
  - is
  - derived
  - from
  - the
  - wasserstein
  - distance
  - between
  - the
  - 'true'
  - data
  - distribution
  - and
  - the
  - distribution
  - of
  - the
  - model
  - .
  - However
  - it
  - '''s'
  - not
  - clear
  - if
  - the
  - approach
  - proposed
  - can
  - still
  - be
  - derived
  - from
  - such
  - a
  - principle
  - .
  - I
  - would
  - invite
  - the
  - author
  - to
  - make
  - the
  - link
  - between
  - wasserstein
  - distance
  - minimization
  - and
  - their
  - approach
  - more
  - explicit
  - .
  - To
  - my
  - knowledge
  - the
  - method
  - proposed
  - is
  - novel
  - ','
  - however
  - using
  - implicit
  - posterior
  - to
  - learn
  - the
  - weights
  - is
  - not
  - novel
  - and
  - several
  - other
  - works
  - have
  - looked
  - at
  - it
  - .
  - In
  - particular
  - I
  - think
  - -LSB-
  - 1,2
  - -RSB-
  - should
  - be
  - discussed
  - in
  - the
  - related
  - work
  - .
  - The
  - difference
  - with
  - traditional
  - bayesian
  - approach
  - such
  - as
  - variational
  - inference
  - should
  - also
  - be
  - discussed
  - ','
  - since
  - the
  - approach
  - is
  - really
  - close
  - to
  - approximating
  - the
  - posterior
  - with
  - an
  - implicit
  - distribution
  - and
  - computing
  - the
  - KL
  - term
  - using
  - a
  - GAN
  - -LRB-
  - like
  - in
  - -LSB-
  - 3,4
  - -RSB-
  - -RRB-
  - .
  - I
  - think
  - one
  - interesting
  - novelty
  - that
  - needs
  - to
  - be
  - emphasized
  - is
  - that
  - the
  - model
  - has
  - both
  - ':'
  - parameters
  - that
  - are
  - point
  - estimates
  - -LRB-
  - the
  - parameters
  - of
  - the
  - generators
  - -RRB-
  - and
  - parameters
  - that
  - are
  - sampled
  - from
  - a
  - posterior
  - distribution
  - -LRB-
  - the
  - weight
  - embeddings
  - -RRB-
  - .
  - Pros
  - ':'
  - '-'
  - Good
  - and
  - promising
  - experimental
  - results
  - .
  - Cons
  - ':'
  - '-'
  - The
  - paper
  - combines
  - several
  - tricks
  - and
  - ideas
  - but
  - it
  - '''s'
  - not
  - really
  - clear
  - what
  - is
  - important
  - and
  - why
  - such
  - an
  - approach
  - works
  - .
  - For
  - example
  - how
  - important
  - is
  - the
  - latent
  - space
  - and
  - the
  - encoder
  - '?'
  - Could
  - we
  - just
  - sample
  - directly
  - the
  - weight
  - embeddings
  - from
  - a
  - gaussian
  - and
  - remove
  - the
  - regularization
  - '?'
  - '-'
  - The
  - other
  - points
  - mentioned
  - above
  - about
  - the
  - clarity
  - of
  - the
  - paper
  - .
  - Others
  - ':'
  - '-'
  - The
  - title
  - is
  - misleading
  - ','
  - the
  - manifold
  - is
  - not
  - really
  - explored
  - '...'
  - If
  - the
  - author
  - really
  - want
  - to
  - explore
  - the
  - manifold
  - some
  - interesting
  - questions
  - are
  - ':'
  - what
  - happens
  - if
  - we
  - try
  - to
  - interpolate
  - between
  - two
  - latent
  - variables
  - '?'
  - What
  - do
  - the
  - latent
  - variables
  - represent
  - '?'
  - what
  - '''s'
  - the
  - influence
  - of
  - the
  - dimension
  - of
  - the
  - latent
  - space
  - '?'
  - '-'
  - In
  - the
  - experiments
  - ':'
  - what
  - is
  - the
  - number
  - of
  - networks
  - used
  - for
  - the
  - other
  - methods
  - '?'
  - '-'
  - It
  - would
  - be
  - nice
  - to
  - have
  - a
  - plot
  - showing
  - the
  - accuracy
  - as
  - a
  - function
  - of
  - the
  - perturbation
  - in
  - section
  - '4.5'
  - .
  - Conclusion
  - ':'
  - The
  - experimental
  - results
  - seem
  - promising
  - however
  - the
  - motivation
  - for
  - the
  - approach
  - is
  - not
  - clear
  - .
  - I
  - think
  - fixing
  - some
  - of
  - the
  - points
  - mentioned
  - above
  - could
  - greatly
  - improve
  - the
  - clarity
  - of
  - the
  - paper
  - and
  - make
  - it
  - a
  - stronger
  - submission
  - .
  - In
  - the
  - current
  - state
  - I
  - do
  - n't
  - believe
  - the
  - paper
  - is
  - rigorous
  - enough
  - to
  - be
  - accepted
  - .
  - References
  - ':'
  - -LSB-
  - '1'
  - -RSB-
  - Pawlowski
  - ','
  - N.
  - ','
  - Rajchl
  - ','
  - M.
  - ','
  - '&'
  - Glocker
  - ','
  - B.
  - -LRB-
  - '2017'
  - -RRB-
  - .
  - Implicit
  - weight
  - uncertainty
  - in
  - neural
  - networks
  - .
  - arXiv
  - ':'
  - '1711.01297'
  - .
  - -LSB-
  - '2'
  - -RSB-
  - Wang
  - ','
  - K.
  - C.
  - ','
  - Vicol
  - ','
  - P.
  - ','
  - Lucas
  - ','
  - J.
  - ','
  - Gu
  - ','
  - L.
  - ','
  - Grosse
  - ','
  - R.
  - ','
  - '&'
  - Zemel
  - ','
  - R.
  - -LRB-
  - '2018'
  - ','
  - July
  - -RRB-
  - .
  - Adversarial
  - Distillation
  - of
  - Bayesian
  - Neural
  - Network
  - Posteriors
  - .
  - ICML
  - -LSB-
  - '3'
  - -RSB-
  - Mescheder
  - ','
  - L.
  - ','
  - Nowozin
  - ','
  - S.
  - ','
  - '&'
  - Geiger
  - ','
  - A.
  - -LRB-
  - '2017'
  - ','
  - July
  - -RRB-
  - .
  - Adversarial
  - Variational
  - Bayes
  - ':'
  - Unifying
  - Variational
  - Autoencoders
  - and
  - Generative
  - Adversarial
  - Networks
  - .
  - ICML
  - -LSB-
  - '4'
  - -RSB-
  - Huszr
  - ','
  - F.
  - -LRB-
  - '2017'
  - -RRB-
  - .
  - Variational
  - inference
  - using
  - implicit
  - distributions
  - .
  - arXiv
  - ':'
  - '1702.08235'
  - .
  - We
  - have
  - revised
  - the
  - manuscript
  - and
  - addressed
  - the
  - notation
  - and
  - terminology
  - concerns
  - as
  - well
  - as
  - the
  - comments
  - made
  - by
  - other
  - reviewers
  - .
  - We
  - have
  - improved
  - these
  - to
  - better
  - describe
  - our
  - method
  - .
  - We
  - would
  - really
  - appreciate
  - if
  - you
  - can
  - read
  - the
  - new
  - Section
  - '3'
  - and
  - give
  - us
  - a
  - new
  - evaluation
  - and
  - further
  - feedback
  - .
  - With
  - the
  - revision
  - ','
  - we
  - still
  - want
  - to
  - answer
  - the
  - questions
  - laid
  - out
  - here
  - .
  - Question
  - ':'
  - The
  - difference
  - between
  - a
  - traditional
  - Bayesian
  - approach
  - such
  - as
  - variational
  - inference
  - should
  - also
  - be
  - discussed
  - .
  - It
  - would
  - be
  - interesting
  - to
  - study
  - what
  - is
  - the
  - impact
  - of
  - this
  - particular
  - choice
  - of
  - loss
  - for
  - the
  - discriminator
  - .
  - In
  - particular
  - ','
  - I
  - invite
  - the
  - author
  - to
  - compare
  - the
  - loss
  - proposed
  - to
  - the
  - loss
  - in
  - -LSB-
  - '1'
  - -RSB-
  - .
  - Answer
  - ':'
  - From
  - the
  - new
  - description
  - one
  - can
  - see
  - that
  - there
  - is
  - a
  - significant
  - difference
  - between
  - our
  - approach
  - and
  - variational
  - Bayesian
  - approach
  - in
  - that
  - we
  - never
  - explicitly
  - model
  - the
  - KL-divergence
  - term
  - that
  - comes
  - from
  - p
  - -LRB-
  - z
  - '|'
  - \
  - theta
  - -RRB-
  - ','
  - because
  - we
  - do
  - not
  - have
  - explicit
  - theta
  - samples
  - nor
  - had
  - an
  - encoder
  - .
  - In
  - Bayes
  - by
  - Hypernetwork
  - -LRB-
  - BbH
  - -RRB-
  - -LSB-
  - '1'
  - -RSB-
  - ','
  - we
  - note
  - two
  - differences
  - .
  - First
  - ','
  - the
  - prior
  - matching
  - step
  - treats
  - each
  - weight
  - independently
  - .
  - This
  - is
  - different
  - from
  - HyperGAN
  - where
  - we
  - perform
  - the
  - prior
  - matching
  - step
  - between
  - the
  - prior
  - and
  - the
  - continuous
  - mixture
  - Q
  - -LRB-
  - s
  - -RRB-
  - using
  - the
  - adversarial
  - loss
  - .
  - Second
  - ','
  - BbH
  - uses
  - independent
  - noise
  - samples
  - as
  - input
  - to
  - the
  - generators
  - .
  - We
  - found
  - that
  - this
  - configuration
  - hurt
  - the
  - diversity
  - of
  - our
  - generated
  - networks
  - ','
  - which
  - is
  - why
  - we
  - use
  - the
  - mixer
  - Q
  - to
  - introduce
  - correlations
  - to
  - our
  - single
  - noise
  - sample
  - .
  - In
  - Table
  - '3'
  - and
  - '4'
  - we
  - compare
  - against
  - using
  - independent
  - generators
  - and
  - find
  - that
  - we
  - lose
  - significant
  - diversity
  - in
  - our
  - generated
  - ensembles
  - .
  - Question
  - ':'
  - What
  - is
  - the
  - number
  - of
  - networks
  - used
  - for
  - other
  - methods
  - '?'
  - Answer
  - ':'
  - Each
  - other
  - -LRB-
  - non-HyperGAN
  - -RRB-
  - method
  - in
  - Table
  - '2'
  - uses
  - the
  - mean
  - of
  - '100'
  - samples
  - ','
  - which
  - corresponds
  - to
  - our
  - strongest
  - considered
  - ensemble
  - of
  - '100'
  - networks
  - .
  - We
  - only
  - used
  - '10'
  - networks
  - for
  - the
  - L2
  - -LRB-
  - standard
  - -RRB-
  - ensembles
  - in
  - the
  - adversarial
  - detection
  - experiments
  - in
  - Sec.
  - '4.5'
  - ','
  - because
  - of
  - the
  - prohibitive
  - cost
  - of
  - training
  - '100'
  - neural
  - networks
  - 'on'
  - each
  - task
  - .
  - Question
  - ':'
  - It
  - would
  - be
  - nice
  - to
  - have
  - a
  - plot
  - showing
  - the
  - accuracy
  - as
  - a
  - function
  - of
  - the
  - perturbation
  - in
  - section
  - '4.5'
  - .
  - Answer
  - ':'
  - We
  - tested
  - only
  - 'on'
  - adversarial
  - examples
  - which
  - succeeded
  - in
  - fooling
  - our
  - ensemble
  - .
  - This
  - means
  - the
  - accuracy
  - of
  - the
  - ensemble
  - predictions
  - under
  - all
  - perturbation
  - levels
  - is
  - '0'
  - ','
  - so
  - we
  - chose
  - not
  - to
  - plot
  - it
  - .
  - Question
  - ':'
  - Title
  - is
  - inaccurate
  - We
  - have
  - also
  - edited
  - the
  - title
  - to
  - reflect
  - that
  - we
  - are
  - generating
  - diverse
  - neural
  - networks
  - ','
  - instead
  - of
  - a
  - whole
  - manifold
  - .
  - -LSB-
  - '1'
  - -RSB-
  - Pawlowski
  - ','
  - Nick
  - ','
  - et
  - al.
  - '``'
  - Implicit
  - weight
  - uncertainty
  - in
  - neural
  - networks
  - .
  - ''''''
  - arXiv
  - preprint
  - arXiv
  - ':'
  - '1711.01297'
  - -LRB-
  - '2017'
  - -RRB-
  - .
  - Thanks
  - for
  - the
  - clarifications
  - ','
  - the
  - section
  - '3'
  - has
  - been
  - greatly
  - improved
  - -LRB-
  - thus
  - I
  - slightly
  - increased
  - my
  - score
  - -RRB-
  - but
  - there
  - is
  - still
  - room
  - for
  - improvement
  - .
  - '1'
  - .
  - I
  - think
  - the
  - section
  - '3.1'
  - is
  - unclear
  - and
  - potentially
  - unnecessary
  - .
  - It
  - is
  - well
  - known
  - that
  - minimizing
  - the
  - KL
  - between
  - the
  - 'true'
  - data
  - distribution
  - and
  - the
  - model
  - is
  - equivalent
  - to
  - maximizing
  - the
  - log
  - likelihood
  - of
  - the
  - model
  - .
  - Plus
  - there
  - is
  - a
  - mistake
  - in
  - equation
  - '6'
  - the
  - expectation
  - should
  - be
  - taken
  - over
  - p
  - -LRB-
  - x
  - '|'
  - \
  - theta
  - -RRB-
  - .
  - '2'
  - .
  - I
  - '''m'
  - not
  - sure
  - to
  - understand
  - table
  - '3'
  - '&'
  - '4'
  - ','
  - do
  - you
  - sample
  - the
  - q_n
  - directly
  - from
  - P
  - '?'
  - If
  - so
  - I
  - do
  - n't
  - understand
  - why
  - this
  - would
  - '``'
  - collapse
  - ''''''
  - '?'
  - since
  - you
  - actually
  - argue
  - in
  - section
  - '3'
  - that
  - '``'
  - This
  - constraint
  - makes
  - it
  - closer
  - to
  - the
  - generated
  - parameters
  - and
  - ensures
  - that
  - Q
  - -LRB-
  - s
  - -RRB-
  - itself
  - does
  - not
  - collapse
  - to
  - always
  - outputting
  - the
  - same
  - latent
  - code
  - ''''''
  - .
  - As
  - a
  - note
  - I
  - recommend
  - for
  - next
  - time
  - that
  - you
  - do
  - n't
  - share
  - your
  - code
  - through
  - a
  - GitHub
  - link
  - as
  - this
  - can
  - compromise
  - anonymity
  - .
- comment_id: B1eFz3plRX
  rels:
  - !!python/tuple
    - 0
    - 12
    - 12
    - 199
    - elaboration
  - !!python/tuple
    - 12
    - 20
    - 20
    - 44
    - list
  - !!python/tuple
    - 20
    - 44
    - 12
    - 20
    - list
  - !!python/tuple
    - 20
    - 29
    - 29
    - 44
    - elaboration
  - !!python/tuple
    - 12
    - 44
    - 44
    - 199
    - elaboration
  - !!python/tuple
    - 44
    - 60
    - 60
    - 199
    - elaboration
  - !!python/tuple
    - 60
    - 70
    - 70
    - 199
    - list
  - !!python/tuple
    - 70
    - 199
    - 60
    - 70
    - list
  - !!python/tuple
    - 70
    - 78
    - 78
    - 199
    - elaboration
  - !!python/tuple
    - 78
    - 87
    - 87
    - 100
    - purpose
  - !!python/tuple
    - 87
    - 93
    - 93
    - 100
    - elaboration
  - !!python/tuple
    - 78
    - 100
    - 100
    - 199
    - elaboration
  - !!python/tuple
    - 100
    - 109
    - 109
    - 130
    - elaboration
  - !!python/tuple
    - 109
    - 113
    - 113
    - 130
    - means
  - !!python/tuple
    - 113
    - 119
    - 119
    - 130
    - elaboration
  - !!python/tuple
    - 119
    - 120
    - 120
    - 130
    - elaboration
  - !!python/tuple
    - 100
    - 130
    - 130
    - 199
    - elaboration
  - !!python/tuple
    - 135
    - 165
    - 130
    - 135
    - attribution
  - !!python/tuple
    - 135
    - 140
    - 140
    - 165
    - list
  - !!python/tuple
    - 140
    - 165
    - 135
    - 140
    - list
  - !!python/tuple
    - 155
    - 165
    - 140
    - 155
    - attribution
  - !!python/tuple
    - 140
    - 145
    - 145
    - 155
    - elaboration
  - !!python/tuple
    - 145
    - 151
    - 151
    - 155
    - elaboration
  - !!python/tuple
    - 155
    - 156
    - 156
    - 165
    - elaboration
  - !!python/tuple
    - 130
    - 165
    - 165
    - 199
    - elaboration
  - !!python/tuple
    - 165
    - 176
    - 176
    - 184
    - means
  - !!python/tuple
    - 165
    - 184
    - 184
    - 199
    - elaboration
  - !!python/tuple
    - 184
    - 193
    - 193
    - 199
    - purpose
  tokens:
  - This
  - paper
  - proposes
  - an
  - new
  - 8-bit
  - quantization
  - strategy
  - for
  - rapid
  - deployment
  - .
  - 8-bit
  - quantization
  - has
  - attracted
  - many
  - attentions
  - recently
  - .
  - And
  - it
  - is
  - already
  - well
  - used
  - in
  - GPU
  - servers
  - -LRB-
  - cudnn
  - -RRB-
  - ','
  - phones
  - ','
  - ARM
  - chips
  - and
  - various
  - ASIC
  - neural
  - network
  - chips
  - .
  - In
  - these
  - situations
  - ','
  - almost
  - 'no'
  - performance
  - drop
  - is
  - observed
  - for
  - classification
  - and
  - detection
  - tasks
  - .
  - So
  - ','
  - the
  - novelty
  - of
  - this
  - paper
  - is
  - limited
  - .
  - Thank
  - you
  - very
  - much
  - for
  - your
  - comments
  - .
  - Competing
  - methods
  - in
  - other
  - papers
  - require
  - retraining
  - or
  - needs
  - to
  - cope
  - with
  - high
  - accuracy
  - loss
  - when
  - quantized
  - in
  - a
  - layer-wise
  - fashion
  - .
  - The
  - proposed
  - method
  - is
  - the
  - first
  - of
  - its
  - kind
  - to
  - resolve
  - these
  - issues
  - by
  - incorporating
  - channel-wise
  - quantization
  - and
  - moment-analysis
  - method
  - which
  - DOES
  - NOT
  - require
  - retraining
  - or
  - the
  - training
  - dataset
  - .
  - Nave
  - channel-wise
  - quantization
  - requires
  - adding
  - huge
  - number
  - of
  - HW
  - shifters
  - and
  - providing
  - values
  - for
  - them
  - which
  - make
  - it
  - unrealistic
  - for
  - implementation
  - -LRB-
  - please
  - see
  - Figure
  - '1'
  - -LRB-
  - b
  - -RRB-
  - in
  - the
  - revised
  - manuscript
  - -RRB-
  - .
  - The
  - biggest
  - contribution
  - of
  - our
  - paper
  - is
  - the
  - HW-friendly
  - channel-wise
  - quantization
  - by
  - manipulating
  - the
  - kernels
  - prior
  - to
  - inference
  - .
  - For
  - your
  - reference
  - ','
  - Figure
  - '1'
  - has
  - been
  - modified
  - to
  - make
  - the
  - distinction
  - clearer
  - .
- comment_id: B1e2x7XI6m
  rels:
  - !!python/tuple
    - 0
    - 310
    - 310
    - 585
    - topic
  - !!python/tuple
    - 0
    - 15
    - 15
    - 310
    - elaboration
  - !!python/tuple
    - 15
    - 44
    - 44
    - 310
    - elaboration
  - !!python/tuple
    - 44
    - 56
    - 56
    - 310
    - elaboration
  - !!python/tuple
    - 56
    - 72
    - 72
    - 83
    - elaboration
  - !!python/tuple
    - 72
    - 73
    - 73
    - 83
    - elaboration
  - !!python/tuple
    - 73
    - 78
    - 78
    - 83
    - elaboration
  - !!python/tuple
    - 56
    - 83
    - 83
    - 310
    - elaboration
  - !!python/tuple
    - 83
    - 97
    - 97
    - 102
    - means
  - !!python/tuple
    - 83
    - 102
    - 102
    - 310
    - elaboration
  - !!python/tuple
    - 102
    - 125
    - 125
    - 310
    - list
  - !!python/tuple
    - 102
    - 110
    - 110
    - 125
    - elaboration
  - !!python/tuple
    - 110
    - 115
    - 115
    - 125
    - attribution
  - !!python/tuple
    - 125
    - 310
    - 102
    - 125
    - list
  - !!python/tuple
    - 125
    - 135
    - 135
    - 310
    - elaboration
  - !!python/tuple
    - 135
    - 143
    - 143
    - 310
    - condition
  - !!python/tuple
    - 143
    - 148
    - 148
    - 175
    - elaboration
  - !!python/tuple
    - 148
    - 158
    - 158
    - 175
    - elaboration
  - !!python/tuple
    - 143
    - 175
    - 175
    - 310
    - elaboration
  - !!python/tuple
    - 175
    - 186
    - 186
    - 310
    - elaboration
  - !!python/tuple
    - 186
    - 197
    - 197
    - 310
    - list
  - !!python/tuple
    - 197
    - 310
    - 186
    - 197
    - list
  - !!python/tuple
    - 197
    - 205
    - 205
    - 310
    - elaboration
  - !!python/tuple
    - 205
    - 207
    - 207
    - 310
    - elaboration
  - !!python/tuple
    - 207
    - 221
    - 221
    - 308
    - list
  - !!python/tuple
    - 207
    - 211
    - 211
    - 221
    - elaboration
  - !!python/tuple
    - 211
    - 218
    - 218
    - 221
    - manner
  - !!python/tuple
    - 221
    - 308
    - 207
    - 221
    - list
  - !!python/tuple
    - 221
    - 231
    - 231
    - 308
    - elaboration
  - !!python/tuple
    - 277
    - 308
    - 231
    - 277
    - antithesis
  - !!python/tuple
    - 231
    - 253
    - 253
    - 277
    - elaboration
  - !!python/tuple
    - 277
    - 290
    - 290
    - 308
    - elaboration
  - !!python/tuple
    - 207
    - 308
    - 308
    - 310
    - elaboration
  - !!python/tuple
    - 310
    - 585
    - 0
    - 310
    - topic
  - !!python/tuple
    - 310
    - 317
    - 317
    - 426
    - condition
  - !!python/tuple
    - 317
    - 322
    - 322
    - 351
    - elaboration
  - !!python/tuple
    - 322
    - 332
    - 332
    - 351
    - elaboration
  - !!python/tuple
    - 317
    - 351
    - 351
    - 426
    - elaboration
  - !!python/tuple
    - 351
    - 362
    - 362
    - 426
    - elaboration
  - !!python/tuple
    - 362
    - 373
    - 373
    - 387
    - elaboration
  - !!python/tuple
    - 373
    - 378
    - 378
    - 387
    - purpose
  - !!python/tuple
    - 378
    - 382
    - 382
    - 387
    - elaboration
  - !!python/tuple
    - 362
    - 387
    - 387
    - 426
    - elaboration
  - !!python/tuple
    - 387
    - 392
    - 392
    - 403
    - condition
  - !!python/tuple
    - 387
    - 403
    - 403
    - 426
    - elaboration
  - !!python/tuple
    - 403
    - 409
    - 409
    - 426
    - elaboration
  - !!python/tuple
    - 409
    - 411
    - 411
    - 426
    - manner
  - !!python/tuple
    - 411
    - 413
    - 413
    - 426
    - list
  - !!python/tuple
    - 413
    - 426
    - 411
    - 413
    - list
  - !!python/tuple
    - 415
    - 426
    - 413
    - 415
    - attribution
  - !!python/tuple
    - 310
    - 426
    - 426
    - 585
    - elaboration
  - !!python/tuple
    - 426
    - 428
    - 428
    - 585
    - list
  - !!python/tuple
    - 428
    - 585
    - 426
    - 428
    - list
  - !!python/tuple
    - 428
    - 440
    - 440
    - 585
    - elaboration
  - !!python/tuple
    - 440
    - 467
    - 467
    - 585
    - list
  - !!python/tuple
    - 440
    - 456
    - 456
    - 467
    - elaboration
  - !!python/tuple
    - 456
    - 457
    - 457
    - 467
    - elaboration
  - !!python/tuple
    - 457
    - 462
    - 462
    - 467
    - elaboration
  - !!python/tuple
    - 467
    - 585
    - 440
    - 467
    - list
  - !!python/tuple
    - 467
    - 481
    - 481
    - 486
    - means
  - !!python/tuple
    - 467
    - 486
    - 486
    - 585
    - elaboration
  - !!python/tuple
    - 486
    - 487
    - 487
    - 492
    - elaboration
  - !!python/tuple
    - 486
    - 492
    - 492
    - 517
    - elaboration
  - !!python/tuple
    - 486
    - 517
    - 517
    - 585
    - elaboration
  - !!python/tuple
    - 519
    - 531
    - 517
    - 519
    - attribution
  - !!python/tuple
    - 517
    - 531
    - 531
    - 585
    - elaboration
  - !!python/tuple
    - 531
    - 547
    - 547
    - 585
    - elaboration
  - !!python/tuple
    - 547
    - 556
    - 556
    - 585
    - list
  - !!python/tuple
    - 550
    - 556
    - 547
    - 550
    - attribution
  - !!python/tuple
    - 556
    - 585
    - 547
    - 556
    - list
  - !!python/tuple
    - 558
    - 585
    - 556
    - 558
    - attribution
  - !!python/tuple
    - 558
    - 564
    - 564
    - 585
    - circumstance
  tokens:
  - This
  - paper
  - studies
  - the
  - effect
  - of
  - batch
  - normalization
  - via
  - a
  - physics
  - style
  - mean-field
  - theory
  - .
  - The
  - theory
  - yields
  - a
  - prediction
  - of
  - maximal
  - learning
  - rate
  - for
  - fully-connected
  - and
  - convolutional
  - networks
  - ','
  - and
  - experimentally
  - the
  - max
  - learning
  - rate
  - agrees
  - very
  - well
  - with
  - the
  - theoretical
  - prediction
  - .
  - This
  - is
  - a
  - well-written
  - paper
  - with
  - a
  - clean
  - ','
  - novel
  - result
  - ':'
  - when
  - we
  - fix
  - the
  - BatchNorm
  - parameter
  - \
  - gamma
  - ','
  - a
  - smaller
  - \
  - gamma
  - stabilizes
  - the
  - training
  - better
  - -LRB-
  - allowing
  - a
  - greater
  - range
  - of
  - learning
  - rates
  - -RRB-
  - .
  - Though
  - in
  - practice
  - the
  - BatchNorm
  - parameters
  - are
  - also
  - trained
  - ','
  - this
  - result
  - may
  - suggest
  - using
  - a
  - smaller
  - initialization
  - .
  - A
  - couple
  - of
  - things
  - I
  - was
  - wondering
  - ':'
  - --
  - As
  - a
  - baseline
  - ','
  - how
  - would
  - the
  - max
  - learning
  - rate
  - behave
  - without
  - BatchNorm
  - '?'
  - Would
  - the
  - theories
  - again
  - match
  - the
  - experimental
  - result
  - there
  - '?'
  - --
  - Is
  - the
  - presence
  - of
  - momentum
  - important
  - '?'
  - If
  - I
  - set
  - the
  - momentum
  - to
  - be
  - zero
  - ','
  - it
  - does
  - not
  - change
  - the
  - theory
  - about
  - the
  - Fisher
  - information
  - and
  - only
  - affects
  - the
  - dependence
  - of
  - \
  - eta
  - 'on'
  - the
  - Fisher
  - information
  - .
  - In
  - this
  - case
  - would
  - the
  - theory
  - still
  - match
  - the
  - experiments
  - '?'
  - Thank
  - you
  - very
  - much
  - for
  - your
  - review
  - and
  - valuable
  - comments
  - .
  - We
  - address
  - your
  - questions
  - and
  - comments
  - below
  - ':'
  - '1'
  - .
  - As
  - a
  - baseline
  - ','
  - how
  - would
  - the
  - max
  - learning
  - rate
  - behave
  - without
  - BatchNorm
  - '?'
  - Would
  - the
  - theories
  - again
  - match
  - the
  - experimental
  - result
  - there
  - '?'
  - We
  - also
  - wondered
  - how
  - the
  - max
  - learning
  - rate
  - would
  - behave
  - without
  - BatchNorm
  - ','
  - and
  - thus
  - we
  - did
  - an
  - experiment
  - for
  - a
  - network
  - without
  - BatchNorm
  - where
  - we
  - varied
  - \
  - sigma_w
  - ','
  - the
  - weight
  - initialization
  - variance
  - ','
  - and
  - found
  - that
  - the
  - theory
  - again
  - matches
  - the
  - experimental
  - result
  - .
  - However
  - ','
  - we
  - did
  - n't
  - include
  - this
  - result
  - in
  - the
  - previous
  - draft
  - .
  - We
  - have
  - now
  - added
  - this
  - result
  - to
  - the
  - SM
  - in
  - the
  - new
  - revised
  - version
  - as
  - a
  - baseline
  - .
  - '2'
  - .
  - Is
  - the
  - presence
  - of
  - momentum
  - important
  - '?'
  - If
  - I
  - set
  - the
  - momentum
  - to
  - be
  - zero
  - ','
  - it
  - does
  - not
  - change
  - the
  - theory
  - about
  - the
  - Fisher
  - information
  - and
  - only
  - affects
  - the
  - dependence
  - of
  - $
  - \
  - eta
  - $
  - 'on'
  - the
  - Fisher
  - information
  - .
  - In
  - this
  - case
  - would
  - the
  - theory
  - still
  - match
  - the
  - experiments
  - '?'
  - The
  - presence
  - of
  - momentum
  - does
  - n't
  - change
  - the
  - picture
  - dramatically
  - .
  - We
  - set
  - momentum
  - to
  - '0.9'
  - to
  - match
  - the
  - value
  - frequently
  - used
  - in
  - practice
  - .
  - Indeed
  - ','
  - changing
  - the
  - momentum
  - only
  - affects
  - the
  - dependency
  - of
  - \
  - eta
  - 'on'
  - the
  - FIM
  - .
  - We
  - have
  - performed
  - an
  - additional
  - experiment
  - 'on'
  - training
  - without
  - momentum
  - and
  - find
  - that
  - in
  - this
  - case
  - the
  - theory
  - still
  - matches
  - the
  - experiment
  - .
  - '3'
  - .
  - This
  - is
  - a
  - well-written
  - paper
  - with
  - a
  - clean
  - ','
  - novel
  - result
  - ':'
  - when
  - we
  - fix
  - the
  - BatchNorm
  - parameter
  - \
  - gamma
  - ','
  - a
  - smaller
  - \
  - gamma
  - stabilizes
  - the
  - training
  - better
  - -LRB-
  - allowing
  - a
  - greater
  - range
  - of
  - learning
  - rates
  - -RRB-
  - .
  - Though
  - in
  - practice
  - the
  - BatchNorm
  - parameters
  - are
  - also
  - trained
  - ','
  - this
  - result
  - may
  - suggest
  - using
  - a
  - smaller
  - initialization
  - .
  - Thanks
  - for
  - the
  - positive
  - feedback
  - '!'
  - We
  - performed
  - additional
  - experiments
  - in
  - the
  - updated
  - version
  - of
  - our
  - paper
  - with
  - VGG11
  - and
  - Preact-Resnet18
  - ','
  - with
  - various
  - \
  - gamma-initializations
  - ','
  - trained
  - 'on'
  - CIFAR-10
  - .
  - We
  - find
  - that
  - the
  - smaller
  - \
  - gamma-initialization
  - indeed
  - increase
  - the
  - speed
  - of
  - convergence
  - .
  - This
  - result
  - can
  - be
  - found
  - in
  - the
  - SM
  - of
  - the
  - latest
  - version
  - of
  - our
  - paper
  - .
  - Thank
  - you
  - again
  - for
  - your
  - review
  - and
  - comments
  - .
  - We
  - believe
  - that
  - the
  - inclusion
  - of
  - a
  - baseline
  - without
  - BatchNorm
  - as
  - well
  - as
  - clarification
  - 'on'
  - the
  - role
  - of
  - momentum
  - has
  - improved
  - the
  - results
  - and
  - clarity
  - of
  - the
  - paper
  - .
- comment_id: B1eoKJXq0m
  rels:
  - !!python/tuple
    - 0
    - 19
    - 19
    - 52
    - elaboration
  - !!python/tuple
    - 19
    - 23
    - 23
    - 52
    - elaboration
  - !!python/tuple
    - 23
    - 38
    - 38
    - 52
    - elaboration
  - !!python/tuple
    - 38
    - 44
    - 44
    - 52
    - list
  - !!python/tuple
    - 44
    - 52
    - 38
    - 44
    - list
  - !!python/tuple
    - 44
    - 47
    - 47
    - 52
    - means
  - !!python/tuple
    - 0
    - 52
    - 52
    - 1483
    - elaboration
  - !!python/tuple
    - 52
    - 70
    - 70
    - 76
    - list
  - !!python/tuple
    - 70
    - 76
    - 52
    - 70
    - list
  - !!python/tuple
    - 52
    - 76
    - 76
    - 1483
    - elaboration
  - !!python/tuple
    - 76
    - 86
    - 86
    - 1483
    - elaboration
  - !!python/tuple
    - 86
    - 91
    - 91
    - 1483
    - elaboration
  - !!python/tuple
    - 91
    - 96
    - 96
    - 108
    - elaboration
  - !!python/tuple
    - 96
    - 101
    - 101
    - 108
    - list
  - !!python/tuple
    - 101
    - 108
    - 96
    - 101
    - list
  - !!python/tuple
    - 91
    - 108
    - 108
    - 133
    - elaboration
  - !!python/tuple
    - 108
    - 120
    - 120
    - 133
    - means
  - !!python/tuple
    - 91
    - 133
    - 133
    - 1483
    - elaboration
  - !!python/tuple
    - 133
    - 141
    - 141
    - 1483
    - list
  - !!python/tuple
    - 141
    - 1483
    - 133
    - 141
    - list
  - !!python/tuple
    - 141
    - 143
    - 143
    - 1483
    - list
  - !!python/tuple
    - 143
    - 1483
    - 141
    - 143
    - list
  - !!python/tuple
    - 143
    - 154
    - 154
    - 162
    - elaboration
  - !!python/tuple
    - 143
    - 162
    - 162
    - 242
    - elaboration
  - !!python/tuple
    - 162
    - 170
    - 170
    - 242
    - elaboration
  - !!python/tuple
    - 170
    - 175
    - 175
    - 242
    - elaboration
  - !!python/tuple
    - 175
    - 180
    - 180
    - 199
    - elaboration
  - !!python/tuple
    - 180
    - 183
    - 183
    - 199
    - elaboration
  - !!python/tuple
    - 175
    - 199
    - 199
    - 242
    - elaboration
  - !!python/tuple
    - 205
    - 221
    - 199
    - 205
    - attribution
  - !!python/tuple
    - 199
    - 221
    - 221
    - 242
    - elaboration
  - !!python/tuple
    - 224
    - 242
    - 221
    - 224
    - attribution
  - !!python/tuple
    - 224
    - 229
    - 229
    - 242
    - purpose
  - !!python/tuple
    - 229
    - 232
    - 232
    - 242
    - elaboration
  - !!python/tuple
    - 143
    - 242
    - 242
    - 1483
    - elaboration
  - !!python/tuple
    - 242
    - 244
    - 244
    - 1483
    - list
  - !!python/tuple
    - 244
    - 1483
    - 242
    - 244
    - list
  - !!python/tuple
    - 244
    - 255
    - 255
    - 267
    - elaboration
  - !!python/tuple
    - 244
    - 267
    - 267
    - 1483
    - elaboration
  - !!python/tuple
    - 267
    - 285
    - 285
    - 1483
    - elaboration
  - !!python/tuple
    - 285
    - 327
    - 327
    - 1483
    - list
  - !!python/tuple
    - 285
    - 302
    - 302
    - 327
    - elaboration
  - !!python/tuple
    - 305
    - 327
    - 302
    - 305
    - attribution
  - !!python/tuple
    - 305
    - 312
    - 312
    - 327
    - list
  - !!python/tuple
    - 312
    - 327
    - 305
    - 312
    - list
  - !!python/tuple
    - 327
    - 1483
    - 285
    - 327
    - list
  - !!python/tuple
    - 327
    - 329
    - 329
    - 1483
    - elaboration
  - !!python/tuple
    - 329
    - 337
    - 337
    - 1483
    - elaboration
  - !!python/tuple
    - 337
    - 346
    - 346
    - 1483
    - list
  - !!python/tuple
    - 346
    - 1483
    - 337
    - 346
    - list
  - !!python/tuple
    - 346
    - 359
    - 359
    - 371
    - list
  - !!python/tuple
    - 359
    - 371
    - 346
    - 359
    - list
  - !!python/tuple
    - 346
    - 371
    - 371
    - 1483
    - elaboration
  - !!python/tuple
    - 371
    - 434
    - 434
    - 1483
    - list
  - !!python/tuple
    - 371
    - 381
    - 381
    - 403
    - elaboration
  - !!python/tuple
    - 381
    - 389
    - 389
    - 403
    - elaboration
  - !!python/tuple
    - 371
    - 403
    - 403
    - 434
    - elaboration
  - !!python/tuple
    - 403
    - 411
    - 411
    - 413
    - elaboration
  - !!python/tuple
    - 403
    - 413
    - 413
    - 434
    - elaboration
  - !!python/tuple
    - 434
    - 1483
    - 371
    - 434
    - list
  - !!python/tuple
    - 434
    - 436
    - 436
    - 1483
    - list
  - !!python/tuple
    - 436
    - 1483
    - 434
    - 436
    - list
  - !!python/tuple
    - 436
    - 444
    - 444
    - 1483
    - elaboration
  - !!python/tuple
    - 444
    - 455
    - 455
    - 1483
    - list
  - !!python/tuple
    - 444
    - 446
    - 446
    - 455
    - elaboration
  - !!python/tuple
    - 455
    - 1483
    - 444
    - 455
    - list
  - !!python/tuple
    - 455
    - 459
    - 459
    - 1483
    - elaboration
  - !!python/tuple
    - 459
    - 479
    - 479
    - 490
    - elaboration
  - !!python/tuple
    - 459
    - 490
    - 490
    - 1483
    - elaboration
  - !!python/tuple
    - 490
    - 507
    - 507
    - 526
    - elaboration
  - !!python/tuple
    - 507
    - 518
    - 518
    - 526
    - elaboration
  - !!python/tuple
    - 490
    - 526
    - 526
    - 1483
    - elaboration
  - !!python/tuple
    - 534
    - 556
    - 526
    - 534
    - attribution
  - !!python/tuple
    - 534
    - 544
    - 544
    - 556
    - same_unit
  - !!python/tuple
    - 534
    - 535
    - 535
    - 544
    - elaboration
  - !!python/tuple
    - 544
    - 556
    - 534
    - 544
    - same_unit
  - !!python/tuple
    - 544
    - 548
    - 548
    - 556
    - elaboration
  - !!python/tuple
    - 526
    - 556
    - 556
    - 1483
    - elaboration
  - !!python/tuple
    - 556
    - 565
    - 565
    - 576
    - elaboration
  - !!python/tuple
    - 556
    - 576
    - 576
    - 1483
    - elaboration
  - !!python/tuple
    - 576
    - 602
    - 602
    - 1483
    - elaboration
  - !!python/tuple
    - 610
    - 619
    - 602
    - 610
    - attribution
  - !!python/tuple
    - 602
    - 619
    - 619
    - 749
    - elaboration
  - !!python/tuple
    - 619
    - 626
    - 626
    - 749
    - list
  - !!python/tuple
    - 619
    - 623
    - 623
    - 626
    - purpose
  - !!python/tuple
    - 626
    - 749
    - 619
    - 626
    - list
  - !!python/tuple
    - 626
    - 637
    - 637
    - 648
    - elaboration
  - !!python/tuple
    - 626
    - 648
    - 648
    - 749
    - elaboration
  - !!python/tuple
    - 680
    - 749
    - 648
    - 680
    - attribution
  - !!python/tuple
    - 648
    - 656
    - 656
    - 680
    - elaboration
  - !!python/tuple
    - 656
    - 660
    - 660
    - 680
    - elaboration
  - !!python/tuple
    - 660
    - 671
    - 671
    - 680
    - elaboration
  - !!python/tuple
    - 680
    - 708
    - 708
    - 749
    - elaboration
  - !!python/tuple
    - 708
    - 733
    - 733
    - 749
    - elaboration
  - !!python/tuple
    - 602
    - 749
    - 749
    - 1483
    - elaboration
  - !!python/tuple
    - 749
    - 758
    - 758
    - 776
    - elaboration
  - !!python/tuple
    - 758
    - 764
    - 764
    - 776
    - elaboration
  - !!python/tuple
    - 764
    - 768
    - 768
    - 776
    - elaboration
  - !!python/tuple
    - 749
    - 776
    - 776
    - 1483
    - elaboration
  - !!python/tuple
    - 776
    - 783
    - 783
    - 795
    - elaboration
  - !!python/tuple
    - 776
    - 795
    - 795
    - 1483
    - elaboration
  - !!python/tuple
    - 795
    - 834
    - 834
    - 849
    - elaboration
  - !!python/tuple
    - 834
    - 835
    - 835
    - 849
    - elaboration
  - !!python/tuple
    - 795
    - 849
    - 849
    - 1483
    - elaboration
  - !!python/tuple
    - 851
    - 890
    - 849
    - 851
    - attribution
  - !!python/tuple
    - 849
    - 890
    - 890
    - 1483
    - elaboration
  - !!python/tuple
    - 890
    - 906
    - 906
    - 1483
    - elaboration
  - !!python/tuple
    - 906
    - 925
    - 925
    - 1483
    - list
  - !!python/tuple
    - 906
    - 910
    - 910
    - 925
    - elaboration
  - !!python/tuple
    - 925
    - 1483
    - 906
    - 925
    - list
  - !!python/tuple
    - 925
    - 938
    - 938
    - 950
    - list
  - !!python/tuple
    - 938
    - 950
    - 925
    - 938
    - list
  - !!python/tuple
    - 925
    - 950
    - 950
    - 1002
    - elaboration
  - !!python/tuple
    - 950
    - 957
    - 957
    - 967
    - elaboration
  - !!python/tuple
    - 950
    - 967
    - 967
    - 1002
    - elaboration
  - !!python/tuple
    - 967
    - 972
    - 972
    - 1002
    - elaboration
  - !!python/tuple
    - 972
    - 976
    - 976
    - 1002
    - elaboration
  - !!python/tuple
    - 976
    - 981
    - 981
    - 1002
    - elaboration
  - !!python/tuple
    - 983
    - 1002
    - 981
    - 983
    - attribution
  - !!python/tuple
    - 983
    - 984
    - 984
    - 1002
    - same_unit
  - !!python/tuple
    - 984
    - 1002
    - 983
    - 984
    - same_unit
  - !!python/tuple
    - 984
    - 990
    - 990
    - 1002
    - elaboration
  - !!python/tuple
    - 925
    - 1002
    - 1002
    - 1483
    - elaboration
  - !!python/tuple
    - 1002
    - 1006
    - 1006
    - 1483
    - elaboration
  - !!python/tuple
    - 1008
    - 1022
    - 1006
    - 1008
    - attribution
  - !!python/tuple
    - 1006
    - 1022
    - 1022
    - 1483
    - elaboration
  - !!python/tuple
    - 1022
    - 1041
    - 1041
    - 1483
    - topic
  - !!python/tuple
    - 1022
    - 1029
    - 1029
    - 1041
    - elaboration
  - !!python/tuple
    - 1041
    - 1483
    - 1022
    - 1041
    - topic
  - !!python/tuple
    - 1045
    - 1073
    - 1041
    - 1045
    - attribution
  - !!python/tuple
    - 1057
    - 1073
    - 1045
    - 1057
    - attribution
  - !!python/tuple
    - 1057
    - 1065
    - 1065
    - 1073
    - elaboration
  - !!python/tuple
    - 1041
    - 1073
    - 1073
    - 1483
    - elaboration
  - !!python/tuple
    - 1073
    - 1097
    - 1097
    - 1483
    - topic
  - !!python/tuple
    - 1073
    - 1077
    - 1077
    - 1097
    - elaboration
  - !!python/tuple
    - 1083
    - 1097
    - 1077
    - 1083
    - attribution
  - !!python/tuple
    - 1083
    - 1090
    - 1090
    - 1097
    - purpose
  - !!python/tuple
    - 1092
    - 1097
    - 1090
    - 1092
    - attribution
  - !!python/tuple
    - 1097
    - 1483
    - 1073
    - 1097
    - topic
  - !!python/tuple
    - 1097
    - 1114
    - 1114
    - 1134
    - elaboration
  - !!python/tuple
    - 1097
    - 1134
    - 1134
    - 1483
    - elaboration
  - !!python/tuple
    - 1138
    - 1150
    - 1134
    - 1138
    - attribution
  - !!python/tuple
    - 1134
    - 1150
    - 1150
    - 1201
    - elaboration
  - !!python/tuple
    - 1150
    - 1158
    - 1158
    - 1169
    - elaboration
  - !!python/tuple
    - 1150
    - 1169
    - 1169
    - 1201
    - elaboration
  - !!python/tuple
    - 1171
    - 1201
    - 1169
    - 1171
    - attribution
  - !!python/tuple
    - 1134
    - 1201
    - 1201
    - 1483
    - elaboration
  - !!python/tuple
    - 1201
    - 1225
    - 1225
    - 1483
    - list
  - !!python/tuple
    - 1201
    - 1205
    - 1205
    - 1225
    - elaboration
  - !!python/tuple
    - 1225
    - 1483
    - 1201
    - 1225
    - list
  - !!python/tuple
    - 1225
    - 1241
    - 1241
    - 1483
    - elaboration
  - !!python/tuple
    - 1241
    - 1268
    - 1268
    - 1305
    - same_unit
  - !!python/tuple
    - 1241
    - 1252
    - 1252
    - 1268
    - elaboration
  - !!python/tuple
    - 1252
    - 1259
    - 1259
    - 1268
    - elaboration
  - !!python/tuple
    - 1268
    - 1305
    - 1241
    - 1268
    - same_unit
  - !!python/tuple
    - 1268
    - 1287
    - 1287
    - 1305
    - elaboration
  - !!python/tuple
    - 1241
    - 1305
    - 1305
    - 1483
    - elaboration
  - !!python/tuple
    - 1305
    - 1324
    - 1324
    - 1483
    - elaboration
  - !!python/tuple
    - 1328
    - 1354
    - 1324
    - 1328
    - attribution
  - !!python/tuple
    - 1328
    - 1335
    - 1335
    - 1354
    - list
  - !!python/tuple
    - 1335
    - 1354
    - 1328
    - 1335
    - list
  - !!python/tuple
    - 1335
    - 1338
    - 1338
    - 1354
    - purpose
  - !!python/tuple
    - 1338
    - 1341
    - 1341
    - 1354
    - same_unit
  - !!python/tuple
    - 1341
    - 1354
    - 1338
    - 1341
    - same_unit
  - !!python/tuple
    - 1324
    - 1354
    - 1354
    - 1483
    - elaboration
  - !!python/tuple
    - 1354
    - 1371
    - 1371
    - 1413
    - circumstance
  - !!python/tuple
    - 1354
    - 1413
    - 1413
    - 1483
    - elaboration
  - !!python/tuple
    - 1413
    - 1427
    - 1427
    - 1438
    - list
  - !!python/tuple
    - 1427
    - 1438
    - 1413
    - 1427
    - list
  - !!python/tuple
    - 1413
    - 1438
    - 1438
    - 1483
    - elaboration
  - !!python/tuple
    - 1438
    - 1446
    - 1446
    - 1450
    - elaboration
  - !!python/tuple
    - 1438
    - 1450
    - 1450
    - 1483
    - elaboration
  - !!python/tuple
    - 1450
    - 1469
    - 1469
    - 1483
    - elaboration
  - !!python/tuple
    - 1471
    - 1483
    - 1469
    - 1471
    - attribution
  - !!python/tuple
    - 1471
    - 1476
    - 1476
    - 1483
    - purpose
  tokens:
  - -LSB-
  - Overview
  - -RSB-
  - In
  - this
  - paper
  - ','
  - the
  - authors
  - studied
  - the
  - problem
  - of
  - composition
  - and
  - decomposition
  - of
  - GANs
  - .
  - Motivated
  - by
  - the
  - observations
  - that
  - images
  - are
  - naturally
  - composed
  - of
  - multiple
  - layouts
  - ','
  - the
  - authors
  - proposed
  - a
  - new
  - framework
  - to
  - study
  - the
  - compositional
  - image
  - generation
  - and
  - its
  - decomposition
  - by
  - defining
  - several
  - tasks
  - .
  - 'On'
  - those
  - various
  - tasks
  - ','
  - the
  - authors
  - demonstrate
  - the
  - possibility
  - of
  - the
  - proposed
  - model
  - to
  - composing
  - image
  - components
  - and
  - decompose
  - the
  - images
  - afterwards
  - .
  - These
  - results
  - are
  - interesting
  - and
  - insightful
  - to
  - some
  - extent
  - .
  - -LSB-
  - Strengthes
  - -RSB-
  - '1'
  - .
  - The
  - authors
  - proposed
  - a
  - framework
  - for
  - compose
  - images
  - from
  - components
  - and
  - decompose
  - the
  - images
  - into
  - components
  - .
  - Based
  - 'on'
  - this
  - new
  - framework
  - ','
  - the
  - authors
  - tried
  - different
  - settings
  - ','
  - by
  - fixing
  - the
  - learning
  - of
  - one
  - or
  - more
  - modules
  - in
  - the
  - model
  - .
  - The
  - experiments
  - 'on'
  - various
  - tasks
  - are
  - appreciated
  - .
  - '2'
  - .
  - In
  - the
  - experiments
  - ','
  - the
  - authors
  - tried
  - both
  - image
  - and
  - text
  - to
  - demonstrate
  - the
  - concepts
  - in
  - this
  - paper
  - .
  - Moreover
  - ','
  - some
  - qualitative
  - results
  - are
  - presented
  - .
  - -LSB-
  - Weaknesses
  - -RSB-
  - '1'
  - .
  - The
  - authors
  - performed
  - multiple
  - experiments
  - regarding
  - various
  - tasks
  - defined
  - in
  - this
  - paper.However
  - ','
  - I
  - can
  - hardly
  - find
  - any
  - quantitative
  - evaluation
  - for
  - the
  - results
  - .
  - It
  - is
  - not
  - clear
  - to
  - me
  - that
  - how
  - the
  - quality
  - of
  - the
  - composed
  - images
  - and
  - the
  - decomposed
  - components
  - from
  - images
  - are
  - .
  - I
  - would
  - suggest
  - the
  - authors
  - derive
  - some
  - metric
  - to
  - measure
  - quality
  - quantitatively
  - ','
  - provide
  - some
  - statistics
  - 'on'
  - the
  - whole
  - datasets
  - .
  - '2'
  - .
  - In
  - this
  - paper
  - ','
  - the
  - authors
  - proposed
  - multiple
  - tasks
  - in
  - terms
  - of
  - which
  - parts
  - are
  - fixed
  - and
  - known
  - in
  - the
  - training
  - process
  - .
  - However
  - ','
  - dominated
  - by
  - so
  - many
  - different
  - tasks
  - ','
  - the
  - core
  - idea
  - is
  - losses
  - in
  - the
  - paper
  - .
  - From
  - the
  - paper
  - ','
  - I
  - can
  - not
  - get
  - the
  - core
  - idea
  - the
  - authors
  - want
  - to
  - deliver
  - .
  - I
  - would
  - suggest
  - the
  - authors
  - focus
  - 'on'
  - one
  - certain
  - task
  - and
  - perform
  - more
  - qualitative
  - and
  - quantitative
  - analysis
  - and
  - comparisons
  - ','
  - as
  - also
  - mentioned
  - above
  - .
  - '3'
  - .
  - The
  - proposed
  - model
  - has
  - several
  - tricky
  - parts
  - .
  - First
  - ','
  - the
  - number
  - of
  - components
  - are
  - pre-determined
  - .
  - However
  - ','
  - in
  - realistic
  - cases
  - ','
  - the
  - number
  - of
  - components
  - are
  - unknown
  - ','
  - and
  - thus
  - how
  - many
  - component
  - generators
  - should
  - be
  - used
  - is
  - ill-posed
  - .
  - Second
  - ','
  - the
  - composing
  - operation
  - is
  - simple
  - and
  - tricky
  - .
  - Such
  - a
  - simple
  - composing
  - operation
  - make
  - it
  - hard
  - to
  - adapt
  - to
  - some
  - more
  - complicated
  - data
  - ','
  - such
  - as
  - cifar10
  - or
  - so
  - .
  - Thirdly
  - ','
  - almost
  - all
  - tasks
  - need
  - some
  - components
  - known
  - .
  - Even
  - for
  - the
  - Task
  - '4'
  - ','
  - c
  - is
  - known
  - ','
  - and
  - the
  - model
  - performs
  - poorly
  - for
  - generating
  - the
  - disentangled
  - components
  - .
  - '4'
  - .
  - The
  - authors
  - missed
  - one
  - very
  - relevant
  - paper
  - ':'
  - LR-GAN
  - ':'
  - Layered
  - Recursive
  - Generative
  - Adversarial
  - Networks
  - for
  - Image
  - Generation
  - .
  - Yang
  - et
  - al.
  - .
  - In
  - the
  - above
  - paper
  - ','
  - the
  - authors
  - proposed
  - an
  - end-to-end
  - model
  - for
  - generating
  - images
  - with
  - background
  - and
  - foreground
  - compositionally
  - .
  - It
  - can
  - be
  - applied
  - to
  - a
  - number
  - of
  - realistic
  - datasets
  - .
  - Regardless
  - of
  - the
  - decomposition
  - part
  - in
  - this
  - paper
  - ','
  - the
  - proposed
  - method
  - in
  - the
  - above
  - paper
  - seems
  - to
  - be
  - clearly
  - superior
  - to
  - the
  - composition
  - part
  - in
  - this
  - paper
  - considering
  - this
  - paper
  - fails
  - 'on'
  - Task
  - '4'
  - .
  - The
  - authors
  - should
  - give
  - credit
  - to
  - the
  - above
  - paper
  - -LRB-
  - even
  - the
  - synthesized
  - MNIST
  - dataset
  - looks
  - similar
  - -RRB-
  - and
  - pay
  - some
  - efforts
  - to
  - explain
  - the
  - advantages
  - in
  - comparison
  - it
  - .
  - -LSB-
  - Summary
  - -RSB-
  - This
  - paper
  - proposed
  - a
  - new
  - framework
  - to
  - study
  - the
  - compositionally
  - of
  - images
  - during
  - generation
  - and
  - decomposition
  - .
  - Through
  - several
  - experiments
  - 'on'
  - various
  - tasks
  - ','
  - the
  - authors
  - presented
  - some
  - interesting
  - results
  - and
  - provided
  - some
  - insights
  - 'on'
  - the
  - potentials
  - and
  - difficulties
  - in
  - this
  - direction
  - .
  - However
  - ','
  - as
  - pointed
  - above
  - ','
  - I
  - think
  - this
  - paper
  - lacks
  - enough
  - experimental
  - analysis
  - and
  - comparison
  - .
  - Its
  - core
  - idea
  - hard
  - to
  - capture
  - .
  - Also
  - ','
  - it
  - missed
  - a
  - comparison
  - to
  - some
  - related
  - work
  - .
  - We
  - thank
  - the
  - reviewer
  - for
  - their
  - detailed
  - and
  - thoughtful
  - review
  - .
  - We
  - have
  - made
  - some
  - improvements
  - to
  - our
  - paper
  - based
  - 'on'
  - these
  - suggestions
  - '-'
  - adding
  - quantitative
  - evaluations
  - and
  - expanding
  - our
  - comparison
  - to
  - related
  - work
  - '-'
  - please
  - see
  - inline
  - for
  - our
  - detailed
  - responses
  - ':'
  - Reply
  - to
  - '1'
  - ':'
  - First
  - ','
  - we
  - '''d'
  - like
  - to
  - clarify
  - that
  - the
  - primary
  - intent
  - of
  - our
  - work
  - was
  - to
  - suggest
  - a
  - set
  - of
  - composition
  - /
  - decomposition
  - subtasks
  - -LRB-
  - c.f.
  - tasks
  - '1'
  - through
  - '4'
  - in
  - our
  - submission
  - -RRB-
  - ','
  - as
  - well
  - as
  - deriving
  - some
  - basic
  - theoretical
  - results
  - about
  - the
  - identifiability
  - of
  - these
  - tasks
  - -LRB-
  - e.g.
  - ','
  - conditions
  - where
  - one
  - can
  - learn
  - component
  - models
  - from
  - composed
  - data
  - etc
  - -RRB-
  - .
  - The
  - experimental
  - results
  - were
  - intended
  - more
  - as
  - illustrative
  - examples
  - of
  - when
  - such
  - models
  - were
  - learnable
  - -LRB-
  - or
  - not
  - -RRB-
  - which
  - explained
  - our
  - lack
  - of
  - quantitative
  - evaluations
  - .
  - However
  - ','
  - we
  - agree
  - with
  - the
  - reviewer
  - that
  - providing
  - a
  - qualitative
  - evaluation
  - across
  - the
  - entire
  - dataset
  - is
  - useful
  - .
  - We
  - supplemented
  - our
  - original
  - qualitative
  - results
  - with
  - quantitative
  - metrics
  - '-'
  - specifically
  - ','
  - we
  - evaluated
  - the
  - foreground
  - generator
  - learned
  - from
  - composed
  - examples
  - using
  - a
  - standard
  - FID
  - score
  - and
  - compared
  - this
  - to
  - our
  - base
  - GAN
  - model
  - trained
  - 'on'
  - the
  - actual
  - foreground
  - dataset
  - -LRB-
  - as
  - a
  - theoretical
  - upper
  - bound
  - 'on'
  - performance
  - for
  - the
  - compositional
  - model
  - -RRB-
  - .
  - We
  - show
  - that
  - ','
  - as
  - expected
  - ','
  - we
  - do
  - not
  - do
  - quite
  - as
  - well
  - when
  - we
  - have
  - to
  - learn
  - to
  - decompose
  - and
  - model
  - the
  - foreground
  - simultaneously
  - ','
  - but
  - are
  - within
  - range
  - of
  - the
  - FID
  - scores
  - reported
  - in
  - literature
  - 'on'
  - MNIST
  - .
  - We
  - further
  - evaluated
  - FID
  - scores
  - 'on'
  - Fashion-MNIST
  - in
  - the
  - same
  - manner
  - as
  - an
  - additional
  - validation
  - .
  - Reply
  - to
  - '2'
  - ':'
  - We
  - apologise
  - for
  - the
  - lack
  - of
  - clarity
  - in
  - our
  - presentation
  - of
  - the
  - various
  - sub-tasks
  - .
  - Part
  - of
  - the
  - contribution
  - of
  - our
  - work
  - is
  - to
  - enumerate
  - various
  - composition/decomposition
  - tasks
  - and
  - to
  - demonstrate
  - the
  - feasibility
  - of
  - a
  - subset
  - of
  - these
  - tasks
  - .
  - However
  - ','
  - we
  - agree
  - with
  - the
  - reviewer
  - that
  - this
  - may
  - result
  - in
  - confusion
  - for
  - the
  - reader
  - .
  - We
  - '''ve'
  - edited
  - the
  - introduction
  - to
  - make
  - it
  - clearer
  - that
  - our
  - main
  - focus
  - is
  - to
  - demonstrate
  - that
  - '``'
  - chain
  - learning
  - ''''''
  - is
  - possible
  - since
  - it
  - provides
  - a
  - simple
  - proof-of-concept
  - for
  - modular
  - extensions
  - of
  - GANs
  - .
  - Reply
  - to
  - '3'
  - ':'
  - We
  - agree
  - that
  - having
  - a
  - pre-specified
  - number
  - of
  - components
  - is
  - a
  - limitation
  - of
  - this
  - framework
  - .
  - We
  - are
  - definitely
  - interested
  - in
  - exploring
  - extensions
  - of
  - such
  - models
  - beyond
  - a
  - fixed
  - ','
  - pre-specified
  - number
  - of
  - components
  - .
  - However
  - ','
  - we
  - believe
  - that
  - even
  - this
  - constrained
  - version
  - of
  - compositionality
  - has
  - not
  - been
  - extensively
  - explored
  - '-'
  - especially
  - in
  - terms
  - of
  - our
  - theoretical
  - understanding
  - of
  - when
  - such
  - compositional
  - training
  - is
  - possible
  - .
  - Regarding
  - our
  - compositional
  - operation
  - being
  - too
  - simple
  - ','
  - we
  - agree
  - that
  - our
  - composition
  - transformations
  - are
  - not
  - sufficient
  - to
  - capture
  - '``'
  - real-world
  - ''''''
  - composition
  - .
  - Our
  - goal
  - was
  - to
  - show
  - a
  - proof-of-concept
  - 'on'
  - a
  - challenging
  - but
  - still
  - feasible
  - set
  - of
  - composition
  - operations
  - -LRB-
  - e.g.
  - ','
  - in
  - our
  - chain
  - learning
  - example
  - ','
  - the
  - composition
  - consists
  - of
  - scaling
  - ','
  - rotation
  - and
  - masking
  - -RRB-
  - .
  - Lastly
  - ','
  - we
  - agree
  - that
  - most
  - of
  - the
  - tasks
  - assume
  - knowledge
  - of
  - a
  - component
  - generator
  - .
  - This
  - was
  - the
  - main
  - motivation
  - behind
  - our
  - work
  - -LRB-
  - how
  - to
  - re-use
  - GANs
  - in
  - a
  - modular
  - fashion
  - -RRB-
  - ','
  - we
  - believe
  - that
  - the
  - chain
  - learning
  - example
  - shows
  - a
  - possible
  - approach
  - for
  - how
  - one
  - can
  - iteratively
  - build
  - up
  - a
  - collection
  - of
  - component
  - generators
  - and
  - hence
  - handle
  - compositional
  - data
  - of
  - increasing
  - complexity
  - .
  - Reply
  - to
  - '4'
  - ':'
  - We
  - thank
  - the
  - reviewer
  - for
  - the
  - pointer
  - to
  - LR-GANs
  - ','
  - that
  - is
  - certainly
  - very
  - interesting
  - and
  - relevant
  - related
  - work
  - .
  - However
  - ','
  - there
  - are
  - some
  - key
  - differences
  - between
  - our
  - work
  - and
  - the
  - work
  - 'on'
  - LR-GANs
  - .
  - Firstly
  - ','
  - we
  - learn
  - a
  - marginal
  - component
  - model
  - for
  - the
  - foreground
  - that
  - is
  - able
  - to
  - generate
  - foreground
  - samples
  - -LRB-
  - instead
  - of
  - generating
  - foreground
  - conditioned
  - 'on'
  - background
  - -RRB-
  - this
  - is
  - important
  - for
  - us
  - to
  - be
  - able
  - to
  - reuse
  - component
  - generators
  - as
  - demonstrated
  - in
  - our
  - chain
  - learning
  - examples
  - -LRB-
  - we
  - have
  - included
  - a
  - cross-domain
  - chain
  - learning
  - example
  - in
  - the
  - appendix
  - to
  - further
  - illustrate
  - this
  - -RRB-
  - .
  - Secondly
  - ','
  - the
  - LR-GAN
  - is
  - restricted
  - to
  - modelling
  - affine
  - compositions
  - and
  - do
  - not
  - learn
  - a
  - corresponding
  - decomposition
  - operation
  - .
  - The
  - authors
  - also
  - demonstrate
  - that
  - both
  - having
  - a
  - good
  - foreground
  - mask
  - and
  - restricting
  - composition
  - to
  - affine
  - transformations
  - is
  - required
  - for
  - good
  - performance
  - of
  - their
  - model
  - in
  - their
  - ablative
  - analysis
  - .
  - We
  - appreciate
  - the
  - insights
  - provided
  - by
  - the
  - authors
  - of
  - LR-GAN
  - ','
  - and
  - while
  - these
  - priors
  - are
  - useful
  - when
  - modeling
  - images
  - specifically
  - and
  - may
  - be
  - useful
  - in
  - our
  - contexts
  - as
  - well
  - ','
  - we
  - are
  - more
  - focused
  - 'on'
  - identifying
  - where
  - compositional
  - learning
  - is
  - identifiable
  - more
  - generally
  - without
  - In
  - summary
  - ','
  - there
  - are
  - two
  - main
  - differences
  - in
  - the
  - model
  - formulation
  - directly
  - .
  - First
  - ','
  - in
  - our
  - framework
  - the
  - foreground
  - generator
  - and
  - background
  - generator
  - are
  - independent
  - ','
  - while
  - LR-GAN
  - '''s'
  - foreground
  - generator
  - is
  - dependent
  - 'on'
  - background
  - generator
  - .
  - This
  - independence
  - is
  - required
  - for
  - part
  - generators
  - learnt
  - to
  - be
  - reusable
  - .
  - Second
  - ','
  - in
  - our
  - framework
  - there
  - is
  - a
  - decomposition
  - operation
  - and
  - a
  - cycle
  - consistency
  - regularization
  - in
  - the
  - model
  - .
  - We
  - showed
  - that
  - this
  - regularization
  - is
  - beneficial
  - to
  - learning
  - a
  - good
  - part
  - generators
  - .
- comment_id: B1eJDnWmy4
  rels:
  - !!python/tuple
    - 0
    - 21
    - 21
    - 23
    - elaboration
  - !!python/tuple
    - 0
    - 23
    - 23
    - 417
    - means
  - !!python/tuple
    - 23
    - 41
    - 41
    - 86
    - elaboration
  - !!python/tuple
    - 41
    - 61
    - 61
    - 71
    - same_unit
  - !!python/tuple
    - 41
    - 54
    - 54
    - 61
    - elaboration
  - !!python/tuple
    - 61
    - 71
    - 41
    - 61
    - same_unit
  - !!python/tuple
    - 61
    - 64
    - 64
    - 71
    - elaboration
  - !!python/tuple
    - 64
    - 65
    - 65
    - 71
    - elaboration
  - !!python/tuple
    - 41
    - 71
    - 71
    - 86
    - elaboration
  - !!python/tuple
    - 71
    - 81
    - 81
    - 86
    - list
  - !!python/tuple
    - 81
    - 86
    - 71
    - 81
    - list
  - !!python/tuple
    - 23
    - 86
    - 86
    - 417
    - elaboration
  - !!python/tuple
    - 86
    - 95
    - 95
    - 118
    - elaboration
  - !!python/tuple
    - 95
    - 108
    - 108
    - 118
    - same_unit
  - !!python/tuple
    - 95
    - 99
    - 99
    - 108
    - elaboration
  - !!python/tuple
    - 108
    - 118
    - 95
    - 108
    - same_unit
  - !!python/tuple
    - 108
    - 109
    - 109
    - 118
    - elaboration
  - !!python/tuple
    - 86
    - 118
    - 118
    - 417
    - elaboration
  - !!python/tuple
    - 118
    - 125
    - 125
    - 417
    - list
  - !!python/tuple
    - 125
    - 417
    - 118
    - 125
    - list
  - !!python/tuple
    - 125
    - 137
    - 137
    - 163
    - elaboration
  - !!python/tuple
    - 137
    - 155
    - 155
    - 162
    - elaboration
  - !!python/tuple
    - 137
    - 162
    - 162
    - 163
    - elaboration
  - !!python/tuple
    - 125
    - 163
    - 163
    - 170
    - elaboration
  - !!python/tuple
    - 125
    - 170
    - 170
    - 417
    - elaboration
  - !!python/tuple
    - 170
    - 187
    - 187
    - 417
    - list
  - !!python/tuple
    - 187
    - 417
    - 170
    - 187
    - list
  - !!python/tuple
    - 187
    - 203
    - 203
    - 417
    - list
  - !!python/tuple
    - 203
    - 417
    - 187
    - 203
    - list
  - !!python/tuple
    - 203
    - 204
    - 204
    - 228
    - elaboration
  - !!python/tuple
    - 204
    - 207
    - 207
    - 228
    - elaboration
  - !!python/tuple
    - 203
    - 228
    - 228
    - 417
    - elaboration
  - !!python/tuple
    - 228
    - 245
    - 245
    - 417
    - elaboration
  - !!python/tuple
    - 245
    - 252
    - 252
    - 417
    - list
  - !!python/tuple
    - 252
    - 417
    - 245
    - 252
    - list
  - !!python/tuple
    - 276
    - 286
    - 252
    - 276
    - attribution
  - !!python/tuple
    - 276
    - 281
    - 281
    - 286
    - elaboration
  - !!python/tuple
    - 252
    - 286
    - 286
    - 336
    - elaboration
  - !!python/tuple
    - 286
    - 290
    - 290
    - 323
    - elaboration
  - !!python/tuple
    - 290
    - 308
    - 308
    - 323
    - elaboration
  - !!python/tuple
    - 308
    - 315
    - 315
    - 323
    - elaboration
  - !!python/tuple
    - 286
    - 323
    - 323
    - 336
    - elaboration
  - !!python/tuple
    - 325
    - 336
    - 323
    - 325
    - attribution
  - !!python/tuple
    - 332
    - 336
    - 325
    - 332
    - attribution
  - !!python/tuple
    - 252
    - 336
    - 336
    - 417
    - means
  - !!python/tuple
    - 336
    - 351
    - 351
    - 360
    - same_unit
  - !!python/tuple
    - 336
    - 343
    - 343
    - 351
    - elaboration
  - !!python/tuple
    - 351
    - 360
    - 336
    - 351
    - same_unit
  - !!python/tuple
    - 351
    - 356
    - 356
    - 360
    - elaboration
  - !!python/tuple
    - 336
    - 360
    - 360
    - 417
    - elaboration
  - !!python/tuple
    - 360
    - 370
    - 370
    - 375
    - elaboration
  - !!python/tuple
    - 360
    - 375
    - 375
    - 417
    - elaboration
  - !!python/tuple
    - 375
    - 385
    - 385
    - 395
    - list
  - !!python/tuple
    - 385
    - 395
    - 375
    - 385
    - list
  - !!python/tuple
    - 385
    - 392
    - 392
    - 395
    - purpose
  - !!python/tuple
    - 375
    - 395
    - 395
    - 417
    - elaboration
  - !!python/tuple
    - 405
    - 417
    - 395
    - 405
    - attribution
  - !!python/tuple
    - 405
    - 409
    - 409
    - 417
    - purpose
  - !!python/tuple
    - 0
    - 417
    - 417
    - 2482
    - elaboration
  - !!python/tuple
    - 424
    - 455
    - 417
    - 424
    - attribution
  - !!python/tuple
    - 424
    - 432
    - 432
    - 455
    - temporal
  - !!python/tuple
    - 432
    - 438
    - 438
    - 455
    - elaboration
  - !!python/tuple
    - 417
    - 455
    - 455
    - 2482
    - elaboration
  - !!python/tuple
    - 455
    - 462
    - 462
    - 473
    - elaboration
  - !!python/tuple
    - 462
    - 465
    - 465
    - 473
    - elaboration
  - !!python/tuple
    - 455
    - 473
    - 473
    - 2482
    - means
  - !!python/tuple
    - 473
    - 498
    - 498
    - 502
    - elaboration
  - !!python/tuple
    - 473
    - 502
    - 502
    - 2482
    - elaboration
  - !!python/tuple
    - 502
    - 510
    - 510
    - 2482
    - textualorganization
  - !!python/tuple
    - 510
    - 2482
    - 502
    - 510
    - textualorganization
  - !!python/tuple
    - 510
    - 528
    - 528
    - 2482
    - elaboration
  - !!python/tuple
    - 549
    - 604
    - 528
    - 549
    - attribution
  - !!python/tuple
    - 549
    - 568
    - 568
    - 604
    - elaboration
  - !!python/tuple
    - 568
    - 573
    - 573
    - 604
    - circumstance
  - !!python/tuple
    - 573
    - 587
    - 587
    - 604
    - elaboration
  - !!python/tuple
    - 528
    - 604
    - 604
    - 2482
    - elaboration
  - !!python/tuple
    - 604
    - 647
    - 647
    - 2482
    - topic
  - !!python/tuple
    - 604
    - 620
    - 620
    - 647
    - elaboration
  - !!python/tuple
    - 620
    - 632
    - 632
    - 647
    - elaboration
  - !!python/tuple
    - 632
    - 638
    - 638
    - 647
    - elaboration
  - !!python/tuple
    - 647
    - 2482
    - 604
    - 647
    - topic
  - !!python/tuple
    - 647
    - 690
    - 690
    - 2482
    - elaboration
  - !!python/tuple
    - 690
    - 704
    - 704
    - 705
    - same_unit
  - !!python/tuple
    - 690
    - 700
    - 700
    - 704
    - elaboration
  - !!python/tuple
    - 704
    - 705
    - 690
    - 704
    - same_unit
  - !!python/tuple
    - 690
    - 705
    - 705
    - 740
    - elaboration
  - !!python/tuple
    - 705
    - 721
    - 721
    - 740
    - same_unit
  - !!python/tuple
    - 705
    - 706
    - 706
    - 721
    - elaboration
  - !!python/tuple
    - 721
    - 740
    - 705
    - 721
    - same_unit
  - !!python/tuple
    - 721
    - 730
    - 730
    - 740
    - elaboration
  - !!python/tuple
    - 690
    - 740
    - 740
    - 2482
    - elaboration
  - !!python/tuple
    - 740
    - 748
    - 748
    - 758
    - elaboration
  - !!python/tuple
    - 740
    - 758
    - 758
    - 2482
    - elaboration
  - !!python/tuple
    - 758
    - 763
    - 763
    - 776
    - elaboration
  - !!python/tuple
    - 758
    - 776
    - 776
    - 796
    - elaboration
  - !!python/tuple
    - 776
    - 785
    - 785
    - 796
    - elaboration
  - !!python/tuple
    - 758
    - 796
    - 796
    - 2482
    - elaboration
  - !!python/tuple
    - 796
    - 803
    - 803
    - 2482
    - elaboration
  - !!python/tuple
    - 803
    - 820
    - 820
    - 832
    - same_unit
  - !!python/tuple
    - 803
    - 813
    - 813
    - 820
    - elaboration
  - !!python/tuple
    - 820
    - 832
    - 803
    - 820
    - same_unit
  - !!python/tuple
    - 821
    - 832
    - 820
    - 821
    - attribution
  - !!python/tuple
    - 803
    - 832
    - 832
    - 2482
    - elaboration
  - !!python/tuple
    - 832
    - 854
    - 854
    - 863
    - elaboration
  - !!python/tuple
    - 832
    - 863
    - 863
    - 2482
    - elaboration
  - !!python/tuple
    - 863
    - 892
    - 892
    - 893
    - elaboration
  - !!python/tuple
    - 863
    - 893
    - 893
    - 956
    - elaboration
  - !!python/tuple
    - 863
    - 956
    - 956
    - 2482
    - elaboration
  - !!python/tuple
    - 956
    - 962
    - 962
    - 990
    - elaboration
  - !!python/tuple
    - 962
    - 968
    - 968
    - 990
    - circumstance
  - !!python/tuple
    - 956
    - 990
    - 990
    - 2482
    - elaboration
  - !!python/tuple
    - 990
    - 1005
    - 1005
    - 2482
    - elaboration
  - !!python/tuple
    - 1005
    - 1024
    - 1024
    - 1039
    - elaboration
  - !!python/tuple
    - 1024
    - 1027
    - 1027
    - 1039
    - same_unit
  - !!python/tuple
    - 1027
    - 1039
    - 1024
    - 1027
    - same_unit
  - !!python/tuple
    - 1027
    - 1038
    - 1038
    - 1039
    - same_unit
  - !!python/tuple
    - 1027
    - 1031
    - 1031
    - 1038
    - elaboration
  - !!python/tuple
    - 1038
    - 1039
    - 1027
    - 1038
    - same_unit
  - !!python/tuple
    - 1005
    - 1039
    - 1039
    - 2482
    - circumstance
  - !!python/tuple
    - 1039
    - 1047
    - 1047
    - 1052
    - elaboration
  - !!python/tuple
    - 1039
    - 1052
    - 1052
    - 2482
    - elaboration
  - !!python/tuple
    - 1052
    - 1074
    - 1074
    - 1075
    - same_unit
  - !!python/tuple
    - 1052
    - 1063
    - 1063
    - 1074
    - elaboration
  - !!python/tuple
    - 1063
    - 1066
    - 1066
    - 1074
    - elaboration
  - !!python/tuple
    - 1074
    - 1075
    - 1052
    - 1074
    - same_unit
  - !!python/tuple
    - 1052
    - 1075
    - 1075
    - 1076
    - elaboration
  - !!python/tuple
    - 1052
    - 1076
    - 1076
    - 1097
    - attribution
  - !!python/tuple
    - 1076
    - 1084
    - 1084
    - 1097
    - elaboration
  - !!python/tuple
    - 1052
    - 1097
    - 1097
    - 2482
    - elaboration
  - !!python/tuple
    - 1097
    - 1107
    - 1107
    - 1114
    - attribution
  - !!python/tuple
    - 1097
    - 1114
    - 1114
    - 1161
    - elaboration
  - !!python/tuple
    - 1114
    - 1117
    - 1117
    - 1161
    - elaboration
  - !!python/tuple
    - 1117
    - 1143
    - 1143
    - 1161
    - elaboration
  - !!python/tuple
    - 1097
    - 1161
    - 1161
    - 2482
    - elaboration
  - !!python/tuple
    - 1161
    - 1174
    - 1174
    - 1207
    - elaboration
  - !!python/tuple
    - 1174
    - 1190
    - 1190
    - 1207
    - elaboration
  - !!python/tuple
    - 1161
    - 1207
    - 1207
    - 2482
    - elaboration
  - !!python/tuple
    - 1207
    - 1230
    - 1230
    - 1258
    - list
  - !!python/tuple
    - 1207
    - 1223
    - 1223
    - 1230
    - elaboration
  - !!python/tuple
    - 1230
    - 1258
    - 1207
    - 1230
    - list
  - !!python/tuple
    - 1230
    - 1238
    - 1238
    - 1258
    - elaboration
  - !!python/tuple
    - 1238
    - 1246
    - 1246
    - 1258
    - elaboration
  - !!python/tuple
    - 1207
    - 1258
    - 1258
    - 2482
    - elaboration
  - !!python/tuple
    - 1258
    - 1267
    - 1267
    - 1290
    - elaboration
  - !!python/tuple
    - 1258
    - 1290
    - 1290
    - 1319
    - elaboration
  - !!python/tuple
    - 1293
    - 1319
    - 1290
    - 1293
    - attribution
  - !!python/tuple
    - 1293
    - 1297
    - 1297
    - 1319
    - same_unit
  - !!python/tuple
    - 1293
    - 1296
    - 1296
    - 1297
    - elaboration
  - !!python/tuple
    - 1297
    - 1319
    - 1293
    - 1297
    - same_unit
  - !!python/tuple
    - 1297
    - 1301
    - 1301
    - 1319
    - elaboration
  - !!python/tuple
    - 1258
    - 1319
    - 1319
    - 2482
    - elaboration
  - !!python/tuple
    - 1319
    - 1335
    - 1335
    - 2482
    - list
  - !!python/tuple
    - 1319
    - 1329
    - 1329
    - 1335
    - means
  - !!python/tuple
    - 1335
    - 2482
    - 1319
    - 1335
    - list
  - !!python/tuple
    - 1335
    - 1352
    - 1352
    - 1383
    - elaboration
  - !!python/tuple
    - 1362
    - 1383
    - 1352
    - 1362
    - antithesis
  - !!python/tuple
    - 1362
    - 1365
    - 1365
    - 1383
    - circumstance
  - !!python/tuple
    - 1372
    - 1383
    - 1365
    - 1372
    - attribution
  - !!python/tuple
    - 1335
    - 1383
    - 1383
    - 2482
    - elaboration
  - !!python/tuple
    - 1383
    - 1391
    - 1391
    - 1418
    - elaboration
  - !!python/tuple
    - 1391
    - 1413
    - 1413
    - 1418
    - elaboration
  - !!python/tuple
    - 1383
    - 1418
    - 1418
    - 1474
    - elaboration
  - !!python/tuple
    - 1418
    - 1431
    - 1431
    - 1474
    - elaboration
  - !!python/tuple
    - 1444
    - 1451
    - 1431
    - 1444
    - attribution
  - !!python/tuple
    - 1444
    - 1447
    - 1447
    - 1451
    - elaboration
  - !!python/tuple
    - 1431
    - 1451
    - 1451
    - 1474
    - elaboration
  - !!python/tuple
    - 1451
    - 1453
    - 1453
    - 1474
    - purpose
  - !!python/tuple
    - 1453
    - 1461
    - 1461
    - 1474
    - elaboration
  - !!python/tuple
    - 1461
    - 1465
    - 1465
    - 1474
    - purpose
  - !!python/tuple
    - 1465
    - 1469
    - 1469
    - 1474
    - elaboration
  - !!python/tuple
    - 1383
    - 1474
    - 1474
    - 2482
    - elaboration
  - !!python/tuple
    - 1481
    - 1510
    - 1474
    - 1481
    - attribution
  - !!python/tuple
    - 1481
    - 1504
    - 1504
    - 1510
    - list
  - !!python/tuple
    - 1481
    - 1494
    - 1494
    - 1504
    - elaboration
  - !!python/tuple
    - 1494
    - 1503
    - 1503
    - 1504
    - same_unit
  - !!python/tuple
    - 1494
    - 1499
    - 1499
    - 1503
    - elaboration
  - !!python/tuple
    - 1503
    - 1504
    - 1494
    - 1503
    - same_unit
  - !!python/tuple
    - 1504
    - 1510
    - 1481
    - 1504
    - list
  - !!python/tuple
    - 1474
    - 1510
    - 1510
    - 2482
    - elaboration
  - !!python/tuple
    - 1510
    - 1511
    - 1511
    - 1524
    - elaboration
  - !!python/tuple
    - 1510
    - 1524
    - 1524
    - 1545
    - means
  - !!python/tuple
    - 1524
    - 1534
    - 1534
    - 1545
    - elaboration
  - !!python/tuple
    - 1510
    - 1545
    - 1545
    - 2482
    - circumstance
  - !!python/tuple
    - 1545
    - 1562
    - 1562
    - 2482
    - elaboration
  - !!python/tuple
    - 1562
    - 1606
    - 1606
    - 1666
    - elaboration
  - !!python/tuple
    - 1606
    - 1617
    - 1617
    - 1625
    - elaboration
  - !!python/tuple
    - 1606
    - 1625
    - 1625
    - 1646
    - elaboration
  - !!python/tuple
    - 1625
    - 1626
    - 1626
    - 1646
    - elaboration
  - !!python/tuple
    - 1606
    - 1646
    - 1646
    - 1666
    - elaboration
  - !!python/tuple
    - 1646
    - 1653
    - 1653
    - 1666
    - explanation
  - !!python/tuple
    - 1653
    - 1655
    - 1655
    - 1666
    - elaboration
  - !!python/tuple
    - 1562
    - 1666
    - 1666
    - 2482
    - elaboration
  - !!python/tuple
    - 1666
    - 1672
    - 1672
    - 1676
    - elaboration
  - !!python/tuple
    - 1666
    - 1676
    - 1676
    - 1678
    - elaboration
  - !!python/tuple
    - 1666
    - 1678
    - 1678
    - 2482
    - elaboration
  - !!python/tuple
    - 1678
    - 1687
    - 1687
    - 2482
    - elaboration
  - !!python/tuple
    - 1687
    - 1697
    - 1697
    - 1701
    - same_unit
  - !!python/tuple
    - 1687
    - 1691
    - 1691
    - 1697
    - elaboration
  - !!python/tuple
    - 1691
    - 1692
    - 1692
    - 1697
    - elaboration
  - !!python/tuple
    - 1697
    - 1701
    - 1687
    - 1697
    - same_unit
  - !!python/tuple
    - 1687
    - 1701
    - 1701
    - 2482
    - elaboration
  - !!python/tuple
    - 1701
    - 1710
    - 1710
    - 2482
    - list
  - !!python/tuple
    - 1710
    - 2482
    - 1701
    - 1710
    - list
  - !!python/tuple
    - 1711
    - 1722
    - 1710
    - 1711
    - attribution
  - !!python/tuple
    - 1712
    - 1722
    - 1711
    - 1712
    - attribution
  - !!python/tuple
    - 1710
    - 1722
    - 1722
    - 2482
    - temporal
  - !!python/tuple
    - 1727
    - 1751
    - 1722
    - 1727
    - condition
  - !!python/tuple
    - 1727
    - 1732
    - 1732
    - 1751
    - reason
  - !!python/tuple
    - 1722
    - 1751
    - 1751
    - 2482
    - elaboration
  - !!python/tuple
    - 1753
    - 1763
    - 1751
    - 1753
    - attribution
  - !!python/tuple
    - 1753
    - 1757
    - 1757
    - 1763
    - elaboration
  - !!python/tuple
    - 1751
    - 1763
    - 1763
    - 2482
    - temporal
  - !!python/tuple
    - 1763
    - 1784
    - 1784
    - 2482
    - list
  - !!python/tuple
    - 1763
    - 1775
    - 1775
    - 1784
    - elaboration
  - !!python/tuple
    - 1784
    - 2482
    - 1763
    - 1784
    - list
  - !!python/tuple
    - 1784
    - 2231
    - 2231
    - 2482
    - textualorganization
  - !!python/tuple
    - 1784
    - 1790
    - 1790
    - 2231
    - elaboration
  - !!python/tuple
    - 1790
    - 1812
    - 1812
    - 2231
    - elaboration
  - !!python/tuple
    - 1812
    - 1835
    - 1835
    - 1839
    - elaboration
  - !!python/tuple
    - 1812
    - 1839
    - 1839
    - 2231
    - elaboration
  - !!python/tuple
    - 1839
    - 1846
    - 1846
    - 1849
    - manner
  - !!python/tuple
    - 1839
    - 1849
    - 1849
    - 2231
    - elaboration
  - !!python/tuple
    - 1853
    - 1875
    - 1849
    - 1853
    - attribution
  - !!python/tuple
    - 1853
    - 1864
    - 1864
    - 1875
    - same_unit
  - !!python/tuple
    - 1853
    - 1863
    - 1863
    - 1864
    - same_unit
  - !!python/tuple
    - 1853
    - 1859
    - 1859
    - 1863
    - elaboration
  - !!python/tuple
    - 1863
    - 1864
    - 1853
    - 1863
    - same_unit
  - !!python/tuple
    - 1864
    - 1875
    - 1853
    - 1864
    - same_unit
  - !!python/tuple
    - 1849
    - 1875
    - 1875
    - 2231
    - elaboration
  - !!python/tuple
    - 1875
    - 1887
    - 1887
    - 1902
    - elaboration
  - !!python/tuple
    - 1887
    - 1897
    - 1897
    - 1902
    - elaboration
  - !!python/tuple
    - 1875
    - 1902
    - 1902
    - 2231
    - elaboration
  - !!python/tuple
    - 1903
    - 1933
    - 1902
    - 1903
    - attribution
  - !!python/tuple
    - 1903
    - 1924
    - 1924
    - 1933
    - elaboration
  - !!python/tuple
    - 1924
    - 1928
    - 1928
    - 1933
    - circumstance
  - !!python/tuple
    - 1902
    - 1933
    - 1933
    - 2231
    - elaboration
  - !!python/tuple
    - 1933
    - 1943
    - 1943
    - 1953
    - elaboration
  - !!python/tuple
    - 1933
    - 1953
    - 1953
    - 2231
    - elaboration
  - !!python/tuple
    - 1953
    - 1970
    - 1970
    - 1976
    - same_unit
  - !!python/tuple
    - 1953
    - 1958
    - 1958
    - 1970
    - elaboration
  - !!python/tuple
    - 1970
    - 1976
    - 1953
    - 1970
    - same_unit
  - !!python/tuple
    - 1953
    - 1976
    - 1976
    - 2231
    - elaboration
  - !!python/tuple
    - 1976
    - 1980
    - 1980
    - 1992
    - same_unit
  - !!python/tuple
    - 1980
    - 1992
    - 1976
    - 1980
    - same_unit
  - !!python/tuple
    - 1980
    - 1984
    - 1984
    - 1992
    - elaboration
  - !!python/tuple
    - 1976
    - 1992
    - 1992
    - 2231
    - elaboration
  - !!python/tuple
    - 1992
    - 2020
    - 2020
    - 2231
    - elaboration
  - !!python/tuple
    - 2020
    - 2030
    - 2030
    - 2038
    - elaboration
  - !!python/tuple
    - 2030
    - 2034
    - 2034
    - 2035
    - means
  - !!python/tuple
    - 2030
    - 2035
    - 2035
    - 2038
    - elaboration
  - !!python/tuple
    - 2020
    - 2038
    - 2038
    - 2071
    - elaboration
  - !!python/tuple
    - 2038
    - 2058
    - 2058
    - 2071
    - same_unit
  - !!python/tuple
    - 2038
    - 2039
    - 2039
    - 2058
    - elaboration
  - !!python/tuple
    - 2058
    - 2071
    - 2038
    - 2058
    - same_unit
  - !!python/tuple
    - 2058
    - 2062
    - 2062
    - 2071
    - elaboration
  - !!python/tuple
    - 2062
    - 2070
    - 2070
    - 2071
    - same_unit
  - !!python/tuple
    - 2062
    - 2063
    - 2063
    - 2070
    - elaboration
  - !!python/tuple
    - 2070
    - 2071
    - 2062
    - 2070
    - same_unit
  - !!python/tuple
    - 2020
    - 2071
    - 2071
    - 2231
    - elaboration
  - !!python/tuple
    - 2075
    - 2082
    - 2071
    - 2075
    - attribution
  - !!python/tuple
    - 2071
    - 2082
    - 2082
    - 2231
    - condition
  - !!python/tuple
    - 2082
    - 2098
    - 2098
    - 2231
    - elaboration
  - !!python/tuple
    - 2098
    - 2106
    - 2106
    - 2112
    - elaboration
  - !!python/tuple
    - 2098
    - 2112
    - 2112
    - 2126
    - elaboration
  - !!python/tuple
    - 2098
    - 2126
    - 2126
    - 2231
    - elaboration
  - !!python/tuple
    - 2126
    - 2131
    - 2131
    - 2231
    - elaboration
  - !!python/tuple
    - 2131
    - 2136
    - 2136
    - 2155
    - same_unit
  - !!python/tuple
    - 2131
    - 2135
    - 2135
    - 2136
    - elaboration
  - !!python/tuple
    - 2136
    - 2155
    - 2131
    - 2136
    - same_unit
  - !!python/tuple
    - 2136
    - 2151
    - 2151
    - 2155
    - elaboration
  - !!python/tuple
    - 2131
    - 2155
    - 2155
    - 2231
    - elaboration
  - !!python/tuple
    - 2155
    - 2160
    - 2160
    - 2172
    - purpose
  - !!python/tuple
    - 2155
    - 2172
    - 2172
    - 2231
    - elaboration
  - !!python/tuple
    - 2172
    - 2188
    - 2188
    - 2189
    - same_unit
  - !!python/tuple
    - 2172
    - 2185
    - 2185
    - 2188
    - elaboration
  - !!python/tuple
    - 2188
    - 2189
    - 2172
    - 2188
    - same_unit
  - !!python/tuple
    - 2172
    - 2189
    - 2189
    - 2231
    - elaboration
  - !!python/tuple
    - 2189
    - 2201
    - 2201
    - 2209
    - elaboration
  - !!python/tuple
    - 2189
    - 2209
    - 2209
    - 2230
    - purpose
  - !!python/tuple
    - 2209
    - 2226
    - 2226
    - 2230
    - elaboration
  - !!python/tuple
    - 2189
    - 2230
    - 2230
    - 2231
    - elaboration
  - !!python/tuple
    - 2231
    - 2482
    - 1784
    - 2231
    - textualorganization
  - !!python/tuple
    - 2231
    - 2243
    - 2243
    - 2482
    - list
  - !!python/tuple
    - 2243
    - 2482
    - 2231
    - 2243
    - list
  - !!python/tuple
    - 2243
    - 2273
    - 2273
    - 2482
    - list
  - !!python/tuple
    - 2243
    - 2246
    - 2246
    - 2273
    - elaboration
  - !!python/tuple
    - 2246
    - 2262
    - 2262
    - 2273
    - elaboration
  - !!python/tuple
    - 2262
    - 2267
    - 2267
    - 2273
    - elaboration
  - !!python/tuple
    - 2273
    - 2482
    - 2243
    - 2273
    - list
  - !!python/tuple
    - 2273
    - 2280
    - 2280
    - 2308
    - elaboration
  - !!python/tuple
    - 2280
    - 2290
    - 2290
    - 2308
    - elaboration
  - !!python/tuple
    - 2290
    - 2300
    - 2300
    - 2308
    - same_unit
  - !!python/tuple
    - 2290
    - 2293
    - 2293
    - 2300
    - same_unit
  - !!python/tuple
    - 2290
    - 2292
    - 2292
    - 2293
    - purpose
  - !!python/tuple
    - 2293
    - 2300
    - 2290
    - 2293
    - same_unit
  - !!python/tuple
    - 2300
    - 2308
    - 2290
    - 2300
    - same_unit
  - !!python/tuple
    - 2273
    - 2308
    - 2308
    - 2482
    - elaboration
  - !!python/tuple
    - 2308
    - 2332
    - 2332
    - 2482
    - list
  - !!python/tuple
    - 2308
    - 2316
    - 2316
    - 2332
    - condition
  - !!python/tuple
    - 2321
    - 2332
    - 2316
    - 2321
    - attribution
  - !!python/tuple
    - 2332
    - 2482
    - 2308
    - 2332
    - list
  - !!python/tuple
    - 2333
    - 2364
    - 2332
    - 2333
    - attribution
  - !!python/tuple
    - 2333
    - 2340
    - 2340
    - 2364
    - elaboration
  - !!python/tuple
    - 2340
    - 2342
    - 2342
    - 2364
    - purpose
  - !!python/tuple
    - 2345
    - 2364
    - 2342
    - 2345
    - attribution
  - !!python/tuple
    - 2332
    - 2364
    - 2364
    - 2482
    - elaboration
  - !!python/tuple
    - 2374
    - 2394
    - 2364
    - 2374
    - attribution
  - !!python/tuple
    - 2374
    - 2384
    - 2384
    - 2394
    - elaboration
  - !!python/tuple
    - 2364
    - 2394
    - 2394
    - 2482
    - elaboration
  - !!python/tuple
    - 2394
    - 2397
    - 2397
    - 2415
    - purpose
  - !!python/tuple
    - 2397
    - 2410
    - 2410
    - 2415
    - purpose
  - !!python/tuple
    - 2394
    - 2415
    - 2415
    - 2482
    - elaboration
  - !!python/tuple
    - 2415
    - 2429
    - 2429
    - 2482
    - elaboration
  - !!python/tuple
    - 2429
    - 2437
    - 2437
    - 2482
    - list
  - !!python/tuple
    - 2437
    - 2482
    - 2429
    - 2437
    - list
  - !!python/tuple
    - 2437
    - 2447
    - 2447
    - 2464
    - elaboration
  - !!python/tuple
    - 2447
    - 2451
    - 2451
    - 2464
    - elaboration
  - !!python/tuple
    - 2437
    - 2464
    - 2464
    - 2482
    - elaboration
  - !!python/tuple
    - 2464
    - 2473
    - 2473
    - 2482
    - list
  - !!python/tuple
    - 2464
    - 2467
    - 2467
    - 2473
    - elaboration
  - !!python/tuple
    - 2473
    - 2482
    - 2464
    - 2473
    - list
  tokens:
  - '##'
  - Summary
  - This
  - work
  - presents
  - a
  - probabilistic
  - training
  - method
  - for
  - binary
  - Neural
  - Network
  - with
  - stochastic
  - versions
  - of
  - Batch
  - Normalization
  - and
  - max
  - pooling
  - .
  - By
  - sampling
  - from
  - the
  - weight
  - distribution
  - an
  - ensemble
  - of
  - Binary
  - Neural
  - Networks
  - could
  - further
  - improve
  - the
  - performance
  - .
  - In
  - the
  - experimental
  - section
  - ','
  - the
  - authors
  - compare
  - proposed
  - PBNet
  - with
  - Binarized
  - NN
  - -LRB-
  - Hubara
  - et
  - al.
  - ','
  - '2016'
  - -RRB-
  - in
  - two
  - image
  - datasets
  - -LRB-
  - MNIST
  - and
  - CIFAR10
  - -RRB-
  - .
  - In
  - general
  - ','
  - the
  - paper
  - was
  - written
  - in
  - poor
  - quality
  - and
  - without
  - enough
  - details
  - .
  - The
  - idea
  - behind
  - the
  - paper
  - is
  - not
  - novel
  - .
  - Stochastic
  - binarization
  - and
  - the
  - -LRB-
  - local
  - -RRB-
  - reparametrization
  - trick
  - were
  - used
  - to
  - training
  - binary
  - -LRB-
  - quantized
  - -RRB-
  - neural
  - networks
  - in
  - previous
  - works
  - .
  - The
  - empirical
  - results
  - are
  - not
  - significant
  - .
  - '##'
  - Detail
  - comments
  - Issues
  - with
  - the
  - training
  - algorithm
  - of
  - stochastic
  - neural
  - network
  - The
  - authors
  - did
  - not
  - give
  - details
  - of
  - the
  - training
  - method
  - and
  - vaguely
  - mentioned
  - that
  - the
  - variational
  - optimization
  - framework
  - -LRB-
  - Staines
  - '&'
  - Barber
  - ','
  - '2012'
  - -RRB-
  - .
  - I
  - do
  - not
  - understand
  - equation
  - '1'
  - .
  - Since
  - B
  - is
  - binary
  - ','
  - the
  - left
  - part
  - of
  - equation
  - '2'
  - is
  - a
  - combination
  - optimization
  - problem
  - .
  - If
  - B
  - is
  - sampled
  - during
  - the
  - training
  - ','
  - the
  - gradient
  - would
  - suffer
  - from
  - high
  - variance
  - .
  - Issues
  - with
  - propagating
  - distributions
  - throughout
  - the
  - network
  - Equation
  - '3'
  - is
  - based
  - 'on'
  - the
  - assumption
  - of
  - that
  - the
  - activations
  - are
  - random
  - variables
  - from
  - Bernoulli
  - distribution
  - .
  - In
  - equation
  - '4'
  - ','
  - the
  - activations
  - of
  - the
  - current
  - layer
  - become
  - random
  - variables
  - from
  - Gaussian
  - distribution
  - .
  - How
  - the
  - activations
  - to
  - further
  - propagate
  - '?'
  - Issues
  - with
  - ternary
  - Neural
  - Networks
  - in
  - section
  - '2.4'
  - For
  - a
  - ternary
  - NN
  - ','
  - the
  - weight
  - will
  - be
  - from
  - a
  - multinomial
  - distribution
  - ','
  - I
  - think
  - it
  - will
  - break
  - the
  - assumption
  - used
  - by
  - equation
  - '3'
  - .
  - Issues
  - with
  - empirical
  - evidences
  - Since
  - the
  - activations
  - are
  - sampled
  - in
  - PBNET-S
  - ','
  - a
  - more
  - appropriate
  - baseline
  - should
  - be
  - BNN
  - with
  - stochastic
  - binarization
  - -LRB-
  - Hubara
  - et
  - al.
  - ','
  - '2016'
  - -RRB-
  - which
  - achieved
  - '89.85'
  - '%'
  - accuracy
  - 'on'
  - CIFAR-10
  - .
  - It
  - means
  - that
  - the
  - proposed
  - methods
  - did
  - not
  - show
  - any
  - significant
  - improvements
  - .
  - By
  - the
  - way
  - BNN
  - with
  - stochastic
  - binarization
  - -LRB-
  - Hubara
  - et
  - al.
  - ','
  - '2016'
  - -RRB-
  - can
  - also
  - allow
  - for
  - ensemble
  - predictions
  - to
  - improve
  - performance
  - .
  - Dear
  - reviewer
  - ','
  - We
  - have
  - tried
  - to
  - address
  - the
  - questions/remarks
  - raised
  - in
  - the
  - review
  - .
  - Moreover
  - ','
  - we
  - have
  - updated
  - the
  - writing
  - in
  - the
  - paper
  - and
  - hope
  - the
  - presentation
  - is
  - now
  - easier
  - to
  - follow
  - .
  - Our
  - response
  - follows
  - the
  - structure
  - of
  - the
  - original
  - review
  - such
  - that
  - it
  - is
  - easy
  - to
  - refer
  - back
  - to
  - the
  - original
  - remarks
  - .
  - '#'
  - 'On'
  - the
  - general
  - remarks
  - We
  - agree
  - that
  - the
  - local
  - reparametrization
  - trick
  - has
  - been
  - used
  - before
  - in
  - order
  - to
  - train
  - binary
  - -LRB-
  - or
  - quantized
  - -RRB-
  - neural
  - networks
  - ','
  - however
  - ','
  - we
  - binarize
  - both
  - the
  - weights
  - and
  - activations
  - .
  - Moreover
  - ','
  - we
  - propagate
  - the
  - activation
  - distribution
  - throughout
  - the
  - network/layer
  - in
  - order
  - to
  - backpropagate
  - through
  - binarization
  - functions
  - .
  - By
  - doing
  - so
  - ','
  - the
  - gradient
  - of
  - the
  - binarization
  - function
  - with
  - respect
  - to
  - the
  - parameters
  - of
  - the
  - pre-activation
  - distribution
  - exists
  - and
  - can
  - easily
  - be
  - computed
  - using
  - standard
  - tools
  - .
  - Although
  - our
  - method
  - does
  - n't
  - achieve
  - better
  - performance
  - when
  - compared
  - to
  - the
  - Binarized
  - Neural
  - Networks
  - by
  - Hubara
  - et
  - al.
  - ','
  - the
  - performance
  - is
  - 'on'
  - par
  - .
  - For
  - this
  - reason
  - ','
  - we
  - also
  - do
  - not
  - make
  - claims
  - about
  - outperforming
  - existing
  - methods
  - ','
  - however
  - ','
  - we
  - do
  - argue
  - that
  - our
  - stochastic
  - training
  - method
  - has
  - various
  - favorable
  - properties
  - ','
  - i.e.
  - ','
  - we
  - obtain
  - a
  - distribution
  - over
  - binary
  - network
  - parameters
  - that
  - allow
  - for
  - any-time
  - ensembles
  - without
  - retraining
  - anything
  - ','
  - it
  - allows
  - for
  - more
  - complex
  - network
  - architectures
  - than
  - earlier
  - work
  - -LRB-
  - 'on'
  - probabilistic
  - networks
  - -RRB-
  - ','
  - and
  - it
  - is
  - easily
  - implemented
  - in
  - existing
  - deep
  - learning
  - frameworks
  - .
  - Moreover
  - ','
  - the
  - probabilistic
  - approach
  - allows
  - for
  - straightforward
  - inclusion
  - of
  - priors
  - 'on'
  - the
  - weights
  - and/or
  - activations
  - which
  - can
  - help
  - to
  - impose
  - more
  - structure
  - 'on'
  - the
  - Binary
  - Neural
  - Network
  - -LRB-
  - e.g.
  - ','
  - sparsity
  - priors
  - -RRB-
  - that
  - can
  - lead
  - to
  - even
  - more
  - efficient
  - networks
  - .
  - '##'
  - Response
  - to
  - '``'
  - Issues
  - with
  - the
  - training
  - algorithm
  - of
  - stochastic
  - neural
  - network
  - ''''''
  - ':'
  - We
  - indeed
  - did
  - n't
  - elaborate
  - in
  - much
  - detail
  - 'on'
  - the
  - training
  - method
  - because
  - in
  - many
  - aspects
  - our
  - training
  - method
  - follows
  - the
  - standard
  - approach
  - in
  - the
  - current
  - literature
  - .
  - Since
  - we
  - ensured
  - that
  - all
  - the
  - operations
  - in
  - the
  - PBNet
  - -LRB-
  - '-'
  - S
  - -RRB-
  - are
  - differentiable
  - -LRB-
  - w.r.t.
  - the
  - parameters
  - of
  - the
  - input
  - distributions
  - for
  - most
  - of
  - the
  - operations
  - -RRB-
  - ','
  - we
  - can
  - train
  - the
  - PBNet
  - as
  - any
  - other
  - network
  - -LRB-
  - and
  - thus
  - leverage
  - existing
  - Deep
  - Learning
  - frameworks
  - -RRB-
  - .
  - For
  - more
  - specifics
  - ','
  - see
  - algorithm
  - '1'
  - ','
  - which
  - outlines
  - the
  - forward
  - pass
  - for
  - a
  - single
  - layer
  - .
  - '##'
  - Response
  - to
  - equation
  - '1'
  - being
  - unclear
  - Equation
  - '1'
  - states
  - the
  - upper
  - bound
  - 'on'
  - the
  - training
  - objective
  - .
  - Our
  - actual
  - objective
  - is
  - to
  - obtain
  - the
  - binary
  - weights
  - that
  - minimize
  - the
  - loss
  - as
  - states
  - 'on'
  - the
  - left-hand
  - side
  - .
  - This
  - is
  - indeed
  - a
  - combinatorial
  - problem
  - .
  - As
  - such
  - we
  - make
  - use
  - of
  - the
  - variational
  - optimization
  - framework
  - -LRB-
  - Staines
  - '&'
  - Barber
  - ','
  - '2012'
  - -RRB-
  - in
  - order
  - to
  - obtain
  - an
  - upper
  - bound
  - 'on'
  - the
  - training
  - objective
  - .
  - I.e.
  - ','
  - we
  - introduce
  - a
  - distribution
  - over
  - the
  - binary
  - parameters
  - of
  - the
  - network
  - and
  - instead
  - of
  - optimizing
  - the
  - binary
  - parameters
  - directly
  - ','
  - we
  - optimize
  - the
  - parameters
  - of
  - the
  - binary
  - distributions
  - .
  - As
  - pointed
  - out
  - by
  - the
  - reviewer
  - ','
  - optimizing
  - this
  - upper
  - bound
  - may
  - result
  - in
  - high
  - variance
  - 'on'
  - the
  - gradients
  - ','
  - but
  - we
  - deal
  - with
  - this
  - in
  - the
  - following
  - way
  - ':'
  - '-'
  - For
  - PBNet
  - ','
  - we
  - never
  - sample
  - weights
  - but
  - instead
  - propagate
  - the
  - variance
  - throughout
  - the
  - network
  - ','
  - i.e.
  - ','
  - the
  - forward
  - pass
  - is
  - deterministic
  - and
  - we
  - do
  - n't
  - suffer
  - from
  - high
  - variance
  - 'on'
  - the
  - gradients
  - '-'
  - For
  - PBNet-S
  - ','
  - we
  - leverage
  - the
  - local
  - reparametrization
  - trick
  - ','
  - which
  - is
  - known
  - to
  - have
  - lower
  - gradient
  - variance
  - compared
  - to
  - simply
  - sampling
  - the
  - weights
  - during
  - training
  - .
  - However
  - ','
  - instead
  - of
  - sampling
  - the
  - -LRB-
  - pre
  - '-'
  - -RRB-
  - activations
  - directly
  - after
  - computing
  - the
  - linear
  - operation
  - of
  - a
  - layer
  - ','
  - we
  - sample
  - the
  - activations
  - at
  - the
  - very
  - last
  - operation
  - of
  - the
  - layer
  - .
  - Moreover
  - ','
  - the
  - gradient
  - variance
  - is
  - related
  - to
  - the
  - variance
  - of
  - the
  - weight
  - distribution
  - .
  - For
  - this
  - reason
  - ','
  - we
  - initialize
  - the
  - parameters
  - from
  - a
  - pre-trained
  - network
  - and
  - specifically
  - initialize
  - the
  - weight
  - distribution
  - q
  - -LRB-
  - B
  - -RRB-
  - to
  - have
  - low
  - variance
  - -LRB-
  - compared
  - to
  - a
  - random
  - initialization
  - -RRB-
  - .
  - Following
  - this
  - ','
  - we
  - empirically
  - find
  - 'no'
  - issues
  - with
  - training
  - these
  - networks
  - .
  - '##'
  - Response
  - to
  - '``'
  - Issues
  - with
  - propagating
  - distributions
  - throughout
  - the
  - network
  - '``'
  - The
  - activations
  - -LRB-
  - and
  - the
  - weights
  - for
  - that
  - matter
  - -RRB-
  - are
  - distributed
  - according
  - to
  - a
  - scaled
  - and
  - translated
  - Bernoulli
  - distribution
  - --
  - which
  - we
  - have
  - called
  - the
  - Binary
  - distribution
  - of
  - ease
  - of
  - notation
  - .
  - Any
  - inner
  - product
  - between
  - two
  - Binary
  - distributed
  - vectors
  - is
  - distributed
  - according
  - to
  - a
  - Poisson
  - Binomial
  - distribution
  - .
  - Given
  - the
  - fact
  - that
  - computing
  - the
  - CDF
  - of
  - a
  - Poisson
  - binomial
  - is
  - not
  - straightforward
  - -LSB-
  - '1'
  - -RSB-
  - and
  - that
  - the
  - CDF
  - is
  - well-approximated
  - by
  - the
  - CDF
  - of
  - a
  - Gaussian
  - -LRB-
  - under
  - some
  - assumptions
  - -RRB-
  - ','
  - we
  - approximate
  - the
  - distribution
  - of
  - the
  - pre-activation
  - by
  - a
  - Gaussian
  - distribution
  - .
  - Since
  - the
  - actual
  - activations
  - of
  - a
  - layer
  - are
  - obtained
  - by
  - applying
  - a
  - binarization
  - -LRB-
  - activation
  - -RRB-
  - function
  - to
  - the
  - pre-activation
  - ','
  - we
  - again
  - obtain
  - a
  - binary
  - distribution
  - as
  - activations
  - -LRB-
  - for
  - which
  - we
  - obtain
  - the
  - parameters
  - by
  - evaluating
  - the
  - CDF
  - of
  - the
  - Gaussian
  - distribution
  - -RRB-
  - .
  - As
  - such
  - ','
  - for
  - the
  - PBNet
  - ','
  - the
  - input
  - to
  - a
  - layer
  - is
  - a
  - Binary
  - distribution
  - -LRB-
  - i.e.
  - ','
  - the
  - parameters
  - thereof
  - -RRB-
  - and
  - the
  - output
  - is
  - also
  - a
  - Binary
  - distribution
  - -LRB-
  - i.e.
  - ','
  - the
  - parameters
  - thereof
  - -RRB-
  - ','
  - which
  - allows
  - us
  - to
  - stack
  - multiple
  - layers
  - in
  - a
  - Neural
  - Network
  - .
  - '##'
  - Response
  - to
  - '``'
  - Issues
  - with
  - ternary
  - Neural
  - Networks
  - '``'
  - We
  - obtain
  - a
  - Ternary
  - Neural
  - Network
  - as
  - a
  - post-processing
  - step
  - ','
  - i.e.
  - ','
  - after
  - training
  - ','
  - we
  - obtain
  - a
  - binary
  - distribution
  - .
  - We
  - only
  - claim
  - that
  - we
  - can
  - --
  - as
  - a
  - post-processing
  - step
  - --
  - set
  - some
  - of
  - the
  - weights
  - in
  - a
  - BNN
  - to
  - zero
  - to
  - obtain
  - a
  - Ternary
  - Neural
  - Network
  - .
  - I.e.
  - ','
  - we
  - remove
  - the
  - noisy
  - weights
  - from
  - the
  - network
  - by
  - setting
  - them
  - to
  - zero
  - .
  - This
  - can
  - either
  - be
  - interpreted
  - as
  - a
  - Ternary
  - Neural
  - Network
  - or
  - a
  - Sparse
  - Binary
  - Neural
  - Network
  - .
  - In
  - short
  - ','
  - we
  - never
  - train
  - a
  - Ternary
  - Network
  - ','
  - but
  - obtain
  - one
  - after
  - training
  - a
  - BNN
  - ','
  - and
  - observe
  - that
  - the
  - results
  - improve
  - slightly
  - by
  - using
  - this
  - post-processing
  - step
  - .
  - '##'
  - Response
  - 'on'
  - '``'
  - Issues
  - with
  - empirical
  - evidences
  - '``'
  - We
  - indeed
  - do
  - not
  - improve
  - over
  - earlier
  - work
  - ','
  - however
  - ','
  - our
  - results
  - are
  - competitive
  - and
  - our
  - method
  - has
  - favorable
  - properties
  - compared
  - to
  - earlier
  - work
  - .
  - For
  - this
  - we
  - refer
  - back
  - to
  - the
  - first
  - paragraph
  - of
  - this
  - response
  - .
  - Moreover
  - ','
  - we
  - are
  - aware
  - of
  - the
  - results
  - reported
  - by
  - Hubara
  - et
  - al.
  - -LRB-
  - '2016'
  - -RRB-
  - using
  - stochastic
  - activations
  - .
  - We
  - chose
  - to
  - compare
  - to
  - the
  - BNN
  - using
  - deterministic
  - activiations
  - since
  - we
  - were
  - unable
  - to
  - reproduce
  - the
  - results
  - presented
  - 'on'
  - stochastic
  - activations
  - .
  - Furthermore
  - ','
  - we
  - like
  - to
  - point
  - out
  - that
  - the
  - architecture
  - used
  - in
  - our
  - paper
  - uses
  - one
  - fully
  - connected
  - layer
  - less
  - -LRB-
  - following
  - Shayer
  - et
  - al.
  - -LRB-
  - '2018'
  - -RRB-
  - -RRB-
  - ','
  - and
  - thus
  - has
  - less
  - parameters
  - .
  - In
  - -LRB-
  - Hubara
  - et
  - al.
  - ','
  - '2016'
  - -RRB-
  - ','
  - the
  - binary
  - weights
  - are
  - trained
  - using
  - full
  - precision
  - shadow
  - weights
  - and
  - a
  - deterministic
  - binarization
  - function
  - -LRB-
  - only
  - stochastic
  - binarization
  - of
  - the
  - activations
  - is
  - considered
  - -RRB-
  - .
  - As
  - such
  - ','
  - after
  - training
  - ','
  - there
  - is
  - only
  - a
  - point
  - estimate
  - of
  - the
  - binary
  - weights
  - .
  - Although
  - it
  - is
  - ','
  - of
  - course
  - ','
  - possible
  - to
  - train
  - multiple
  - networks
  - to
  - obtain
  - an
  - ensemble
  - ','
  - in
  - our
  - work
  - ','
  - we
  - train
  - a
  - single
  - PBNet-S
  - once
  - and
  - obtain
  - multiple
  - network
  - instantiations
  - from
  - the
  - binary
  - weight
  - distribution
  - in
  - order
  - to
  - create
  - an
  - ensemble
  - .
  - As
  - such
  - ','
  - it
  - is
  - possible
  - to
  - perform
  - any-time
  - ensemble
  - predictions
  - -LRB-
  - without
  - any
  - extra
  - cost
  - during
  - training
  - -RRB-
  - ','
  - which
  - improve
  - both
  - accuracy
  - and
  - uncertainty
  - estimates
  - as
  - is
  - shown
  - in
  - Table
  - '1'
  - and
  - Figure
  - 3a
  - in
  - our
  - paper
  - .
  - -LSB-
  - '1'
  - -RSB-
  - Hong
  - ','
  - Yili
  - .
  - '``'
  - 'On'
  - computing
  - the
  - distribution
  - function
  - for
  - the
  - Poisson
  - binomial
  - distribution
  - .
  - ''''''
  - Computational
  - Statistics
  - '&'
  - Data
  - Analysis
  - '59'
  - -LRB-
  - '2013'
  - -RRB-
  - ':'
  - 41-51
  - .
  - I
  - still
  - do
  - not
  - understand
  - the
  - training
  - method
  - .
  - Equation
  - '1'
  - and
  - Algorithm
  - '1'
  - -LRB-
  - the
  - forward
  - pass
  - -RRB-
  - are
  - not
  - enough
  - .
  - Please
  - describe
  - also
  - the
  - backward
  - and
  - the
  - update
  - .
  - Please
  - explain
  - why
  - this
  - upper
  - bound
  - and
  - how
  - about
  - the
  - tightness
  - .
  - After
  - reading
  - the
  - response
  - ','
  - I
  - will
  - keep
  - my
  - rating
  - because
  - '``'
  - the
  - idea
  - behind
  - the
  - paper
  - is
  - not
  - novel
  - and
  - the
  - empirical
  - results
  - are
  - not
  - significant
  - .
  - ''''''
  - Thank
  - you
  - for
  - taking
  - the
  - time
  - to
  - respond
  - to
  - our
  - rebuttal
  - .
  - Since
  - not
  - everything
  - is
  - clear
  - yet
  - ','
  - we
  - will
  - take
  - the
  - time
  - to
  - give
  - some
  - clarifications
  - 'on'
  - the
  - points
  - raised
  - .
  - '#'
  - 'On'
  - the
  - training
  - algorithm
  - ':'
  - We
  - noticed
  - that
  - one
  - small
  - detail
  - is
  - missing
  - from
  - our
  - paper
  - ','
  - i.e.
  - ','
  - the
  - parameters
  - NN
  - are
  - constrained
  - to
  - lie
  - in
  - -LSB-
  - '-1'
  - ','
  - '1'
  - -RSB-
  - ','
  - for
  - this
  - reason
  - ','
  - we
  - update
  - a
  - set
  - of
  - unconstrained
  - parameters
  - NN
  - and
  - obtain
  - NN
  - '='
  - tanh
  - -LRB-
  - NN
  - -RRB-
  - .
  - As
  - such
  - ','
  - NN
  - can
  - be
  - updated
  - without
  - constraints
  - .
  - we
  - explicitly
  - made
  - sure
  - that
  - all
  - operations
  - in
  - the
  - PBNet
  - -LRB-
  - '-'
  - S
  - -RRB-
  - are
  - '`'
  - auto-diffable
  - ''''
  - both
  - with
  - respect
  - to
  - weight
  - and
  - input
  - .
  - For
  - all
  - layers
  - ','
  - the
  - inputs
  - to
  - the
  - layers
  - are
  - the
  - parameters
  - -LRB-
  - i.e.
  - ','
  - sufficient
  - statistics
  - -RRB-
  - of
  - the
  - binary
  - distributions
  - that
  - describe
  - the
  - activations
  - .
  - Note
  - that
  - the
  - binary
  - distribution
  - includes
  - deterministic
  - activations
  - of
  - either
  - '-1'
  - or
  - '+1'
  - as
  - a
  - corner
  - case
  - ','
  - which
  - also
  - allows
  - modeling
  - observed
  - ','
  - sampled
  - activations
  - as
  - encountered
  - in
  - PBNet-S
  - .
  - All
  - operations
  - we
  - apply
  - 'on'
  - these
  - sufficient
  - statistics
  - are
  - differentiable
  - -LRB-
  - and
  - can
  - be
  - computed
  - using
  - auto-diff
  - implementations
  - -RRB-
  - .
  - Hence
  - ','
  - the
  - forward
  - pass
  - -LRB-
  - which
  - is
  - specified
  - for
  - a
  - single
  - layer
  - in
  - algorithm
  - '1'
  - -RRB-
  - completely
  - defines
  - the
  - backward
  - pass
  - .
  - The
  - update
  - of
  - NN
  - is
  - performed
  - using
  - Adam
  - -LRB-
  - as
  - stated
  - in
  - our
  - paper
  - -RRB-
  - .
  - Since
  - we
  - can
  - backpropagate
  - throughout
  - the
  - whole
  - model
  - ','
  - the
  - gradient
  - for
  - NN
  - is
  - easily
  - obtained
  - using
  - any
  - standard
  - backpropagation/auto-diff
  - implementation
  - ','
  - such
  - as
  - available
  - in
  - PyTorch
  - .
  - This
  - training
  - method
  - is
  - very
  - similar
  - to
  - various
  - other
  - works
  - that
  - optimize
  - an
  - ELBO
  - using
  - -LRB-
  - local
  - -RRB-
  - reparametrization
  - -LRB-
  - such
  - as
  - -LSB-
  - '1'
  - -RSB-
  - -RRB-
  - ','
  - only
  - instead
  - of
  - optimizing
  - the
  - ELBO
  - ','
  - we
  - optimize
  - the
  - expected
  - -LRB-
  - binary
  - -RRB-
  - cross
  - entropy
  - -LRB-
  - similar
  - to
  - -LSB-
  - '2'
  - -RSB-
  - -RRB-
  - .
  - We
  - hope
  - this
  - clarifies
  - the
  - backward
  - pass
  - and
  - update
  - step
  - .
  - If
  - not
  - ','
  - maybe
  - the
  - reviewer
  - can
  - kindly
  - point
  - out
  - the
  - source
  - of
  - the
  - ambiguities
  - '?'
  - Of
  - course
  - ','
  - we
  - will
  - update
  - the
  - paper
  - to
  - include
  - the
  - discussion
  - above
  - .
  - Moreover
  - ','
  - we
  - are
  - also
  - planning
  - to
  - release
  - the
  - code
  - for
  - our
  - experiments
  - .
  - '#'
  - 'On'
  - the
  - bound
  - ':'
  - We
  - use
  - this
  - specific
  - bound
  - as
  - it
  - is
  - a
  - differentiable
  - bound
  - 'on'
  - the
  - minimum
  - of
  - the
  - non-differentiable
  - objective
  - function
  - L
  - -LRB-
  - B
  - -RRB-
  - .
  - Thus
  - ','
  - it
  - allows
  - us
  - to
  - train
  - a
  - Neural
  - Network
  - with
  - discrete
  - weights
  - using
  - gradient-based
  - methods
  - .
  - 'On'
  - the
  - tightness
  - of
  - the
  - bound
  - ','
  - we
  - quote
  - from
  - Staines
  - '&'
  - Barber
  - -LRB-
  - '2012'
  - -RRB-
  - ':'
  - '``'
  - This
  - bound
  - can
  - be
  - trivially
  - made
  - tight
  - provided
  - the
  - distribution
  - q
  - -LRB-
  - B
  - '|'
  - NN
  - -RRB-
  - is
  - flexible
  - enough
  - to
  - allow
  - all
  - its
  - mass
  - to
  - be
  - placed
  - in
  - the
  - optimal
  - state
  - B
  - ''''
  - '='
  - argmax_B
  - L
  - -LRB-
  - B
  - -RRB-
  - .
  - ''''''
  - were
  - we
  - have
  - changed
  - the
  - variable
  - names
  - to
  - match
  - our
  - case
  - .
  - It
  - is
  - easy
  - to
  - see
  - that
  - in
  - our
  - case
  - all
  - the
  - mass
  - can
  - be
  - placed
  - 'on'
  - the
  - optimal
  - state
  - as
  - this
  - is
  - the
  - case
  - where
  - NN
  - '='
  - B
  - ''''
  - .
  - '#'
  - 'On'
  - the
  - lack
  - of
  - novelty
  - ':'
  - To
  - our
  - knowledge
  - ','
  - our
  - paper
  - introduces
  - various
  - novel
  - aspects
  - with
  - respect
  - to
  - -LRB-
  - probabilistic
  - -RRB-
  - Binary
  - Neural
  - Networks
  - ','
  - as
  - pointed
  - out
  - in
  - our
  - earlier
  - responses
  - .
  - We
  - would
  - like
  - to
  - kindly
  - ask
  - the
  - reviewer
  - if
  - we
  - missed
  - any
  - references
  - that
  - we
  - should
  - compare
  - the
  - novelty
  - of
  - our
  - work
  - to
  - '?'
  - '#'
  - 'On'
  - the
  - significance
  - of
  - the
  - experiments
  - ':'
  - We
  - like
  - to
  - point
  - out
  - that
  - the
  - main
  - goal
  - of
  - our
  - paper
  - was
  - to
  - introduce
  - a
  - novel
  - method
  - for
  - training
  - Binary
  - Neural
  - Networks
  - .
  - In
  - the
  - experiments
  - presented
  - in
  - the
  - paper
  - ','
  - we
  - investigate
  - if
  - our
  - method
  - is
  - 'on'
  - par
  - with
  - an
  - existing
  - method
  - in
  - which
  - an
  - equivalent
  - Binary
  - Neural
  - Network
  - is
  - obtained
  - .
  - We
  - would
  - like
  - to
  - ask
  - how
  - we
  - should
  - extend
  - our
  - experiments
  - in
  - order
  - for
  - the
  - reviewer
  - to
  - consider
  - them
  - significant
  - '?'
  - -LSB-
  - '1'
  - -RSB-
  - Diederik
  - P
  - Kingma
  - ','
  - Tim
  - Salimans
  - ','
  - and
  - Max
  - Welling
  - .
  - Variational
  - dropout
  - and
  - the
  - local
  - reparameteri-zation
  - trick
  - .
  - In
  - Advances
  - in
  - Neural
  - Information
  - Processing
  - Systems
  - ','
  - pp.
  - '2575'
  - --
  - '2583'
  - ','
  - '2015'
  - -LSB-
  - '2'
  - -RSB-
  - Oran
  - Shayer
  - ','
  - Dan
  - Levi
  - ','
  - and
  - Ethan
  - Fetaya
  - .
  - Learning
  - discrete
  - weights
  - using
  - the
  - local
  - repa-rameterization
  - trick
  - .
  - In
  - International
  - Conference
  - 'on'
  - Learning
  - Representations
  - ','
  - '2018'
  - .
- comment_id: B1eFQNnYCQ
  rels:
  - !!python/tuple
    - 0
    - 45
    - 45
    - 1743
    - elaboration
  - !!python/tuple
    - 45
    - 71
    - 71
    - 108
    - list
  - !!python/tuple
    - 45
    - 56
    - 56
    - 71
    - elaboration
  - !!python/tuple
    - 56
    - 65
    - 65
    - 71
    - elaboration
  - !!python/tuple
    - 71
    - 108
    - 45
    - 71
    - list
  - !!python/tuple
    - 45
    - 108
    - 108
    - 1743
    - elaboration
  - !!python/tuple
    - 114
    - 145
    - 108
    - 114
    - attribution
  - !!python/tuple
    - 108
    - 145
    - 145
    - 1743
    - elaboration
  - !!python/tuple
    - 145
    - 158
    - 158
    - 164
    - elaboration
  - !!python/tuple
    - 145
    - 164
    - 164
    - 1743
    - elaboration
  - !!python/tuple
    - 164
    - 170
    - 170
    - 190
    - explanation
  - !!python/tuple
    - 170
    - 181
    - 181
    - 190
    - elaboration
  - !!python/tuple
    - 181
    - 186
    - 186
    - 190
    - elaboration
  - !!python/tuple
    - 164
    - 190
    - 190
    - 1743
    - elaboration
  - !!python/tuple
    - 190
    - 196
    - 196
    - 209
    - purpose
  - !!python/tuple
    - 198
    - 209
    - 196
    - 198
    - attribution
  - !!python/tuple
    - 190
    - 209
    - 209
    - 1743
    - elaboration
  - !!python/tuple
    - 209
    - 216
    - 216
    - 221
    - elaboration
  - !!python/tuple
    - 209
    - 221
    - 221
    - 1743
    - elaboration
  - !!python/tuple
    - 221
    - 234
    - 234
    - 1743
    - elaboration
  - !!python/tuple
    - 234
    - 254
    - 254
    - 262
    - same_unit
  - !!python/tuple
    - 234
    - 246
    - 246
    - 254
    - elaboration
  - !!python/tuple
    - 254
    - 262
    - 234
    - 254
    - same_unit
  - !!python/tuple
    - 234
    - 262
    - 262
    - 1743
    - example
  - !!python/tuple
    - 262
    - 287
    - 287
    - 1743
    - elaboration
  - !!python/tuple
    - 287
    - 302
    - 302
    - 1743
    - list
  - !!python/tuple
    - 287
    - 293
    - 293
    - 302
    - elaboration
  - !!python/tuple
    - 302
    - 1743
    - 287
    - 302
    - list
  - !!python/tuple
    - 302
    - 310
    - 310
    - 1743
    - list
  - !!python/tuple
    - 302
    - 306
    - 306
    - 310
    - elaboration
  - !!python/tuple
    - 310
    - 1743
    - 302
    - 310
    - list
  - !!python/tuple
    - 310
    - 313
    - 313
    - 1743
    - elaboration
  - !!python/tuple
    - 313
    - 324
    - 324
    - 1743
    - list
  - !!python/tuple
    - 313
    - 315
    - 315
    - 324
    - elaboration
  - !!python/tuple
    - 324
    - 1743
    - 313
    - 324
    - list
  - !!python/tuple
    - 324
    - 330
    - 330
    - 366
    - elaboration
  - !!python/tuple
    - 333
    - 366
    - 330
    - 333
    - attribution
  - !!python/tuple
    - 334
    - 366
    - 333
    - 334
    - attribution
  - !!python/tuple
    - 334
    - 354
    - 354
    - 366
    - elaboration
  - !!python/tuple
    - 324
    - 366
    - 366
    - 1743
    - elaboration
  - !!python/tuple
    - 366
    - 402
    - 402
    - 1743
    - list
  - !!python/tuple
    - 366
    - 395
    - 395
    - 402
    - elaboration
  - !!python/tuple
    - 402
    - 1743
    - 366
    - 402
    - list
  - !!python/tuple
    - 402
    - 436
    - 436
    - 1743
    - list
  - !!python/tuple
    - 402
    - 410
    - 410
    - 436
    - elaboration
  - !!python/tuple
    - 410
    - 414
    - 414
    - 436
    - list
  - !!python/tuple
    - 414
    - 436
    - 410
    - 414
    - list
  - !!python/tuple
    - 414
    - 424
    - 424
    - 436
    - elaboration
  - !!python/tuple
    - 436
    - 1743
    - 402
    - 436
    - list
  - !!python/tuple
    - 436
    - 483
    - 483
    - 1743
    - list
  - !!python/tuple
    - 436
    - 463
    - 463
    - 482
    - elaboration
  - !!python/tuple
    - 436
    - 482
    - 482
    - 483
    - elaboration
  - !!python/tuple
    - 483
    - 1743
    - 436
    - 483
    - list
  - !!python/tuple
    - 483
    - 509
    - 509
    - 1743
    - list
  - !!python/tuple
    - 483
    - 489
    - 489
    - 509
    - elaboration
  - !!python/tuple
    - 489
    - 496
    - 496
    - 509
    - elaboration
  - !!python/tuple
    - 509
    - 1743
    - 483
    - 509
    - list
  - !!python/tuple
    - 509
    - 529
    - 529
    - 1743
    - list
  - !!python/tuple
    - 529
    - 1743
    - 509
    - 529
    - list
  - !!python/tuple
    - 529
    - 559
    - 559
    - 1743
    - list
  - !!python/tuple
    - 529
    - 534
    - 534
    - 559
    - elaboration
  - !!python/tuple
    - 535
    - 559
    - 534
    - 535
    - attribution
  - !!python/tuple
    - 559
    - 1743
    - 529
    - 559
    - list
  - !!python/tuple
    - 559
    - 566
    - 566
    - 580
    - elaboration
  - !!python/tuple
    - 559
    - 580
    - 580
    - 1743
    - example
  - !!python/tuple
    - 580
    - 606
    - 606
    - 1743
    - list
  - !!python/tuple
    - 580
    - 593
    - 593
    - 606
    - elaboration
  - !!python/tuple
    - 593
    - 596
    - 596
    - 606
    - elaboration
  - !!python/tuple
    - 606
    - 1743
    - 580
    - 606
    - list
  - !!python/tuple
    - 606
    - 613
    - 613
    - 1743
    - list
  - !!python/tuple
    - 613
    - 1743
    - 606
    - 613
    - list
  - !!python/tuple
    - 613
    - 646
    - 646
    - 1743
    - question
  - !!python/tuple
    - 613
    - 635
    - 635
    - 636
    - elaboration
  - !!python/tuple
    - 613
    - 636
    - 636
    - 646
    - purpose
  - !!python/tuple
    - 646
    - 1743
    - 613
    - 646
    - question
  - !!python/tuple
    - 646
    - 665
    - 665
    - 1743
    - list
  - !!python/tuple
    - 665
    - 1743
    - 646
    - 665
    - list
  - !!python/tuple
    - 665
    - 671
    - 671
    - 683
    - purpose
  - !!python/tuple
    - 665
    - 683
    - 683
    - 1743
    - elaboration
  - !!python/tuple
    - 683
    - 686
    - 686
    - 1743
    - elaboration
  - !!python/tuple
    - 686
    - 729
    - 729
    - 1743
    - list
  - !!python/tuple
    - 691
    - 729
    - 686
    - 691
    - attribution
  - !!python/tuple
    - 691
    - 699
    - 699
    - 729
    - elaboration
  - !!python/tuple
    - 729
    - 1743
    - 686
    - 729
    - list
  - !!python/tuple
    - 729
    - 740
    - 740
    - 1743
    - list
  - !!python/tuple
    - 731
    - 740
    - 729
    - 731
    - attribution
  - !!python/tuple
    - 740
    - 1743
    - 729
    - 740
    - list
  - !!python/tuple
    - 742
    - 764
    - 740
    - 742
    - attribution
  - !!python/tuple
    - 742
    - 751
    - 751
    - 764
    - elaboration
  - !!python/tuple
    - 751
    - 757
    - 757
    - 764
    - elaboration
  - !!python/tuple
    - 740
    - 764
    - 764
    - 1743
    - elaboration
  - !!python/tuple
    - 764
    - 767
    - 767
    - 1743
    - elaboration
  - !!python/tuple
    - 772
    - 802
    - 767
    - 772
    - attribution
  - !!python/tuple
    - 775
    - 802
    - 772
    - 775
    - attribution
  - !!python/tuple
    - 775
    - 779
    - 779
    - 802
    - purpose
  - !!python/tuple
    - 781
    - 802
    - 779
    - 781
    - attribution
  - !!python/tuple
    - 781
    - 785
    - 785
    - 802
    - elaboration
  - !!python/tuple
    - 767
    - 802
    - 802
    - 1743
    - elaboration
  - !!python/tuple
    - 804
    - 830
    - 802
    - 804
    - attribution
  - !!python/tuple
    - 804
    - 807
    - 807
    - 830
    - purpose
  - !!python/tuple
    - 809
    - 830
    - 807
    - 809
    - attribution
  - !!python/tuple
    - 816
    - 830
    - 809
    - 816
    - attribution
  - !!python/tuple
    - 816
    - 825
    - 825
    - 830
    - purpose
  - !!python/tuple
    - 802
    - 830
    - 830
    - 1743
    - elaboration
  - !!python/tuple
    - 830
    - 870
    - 870
    - 1743
    - list
  - !!python/tuple
    - 830
    - 852
    - 852
    - 870
    - list
  - !!python/tuple
    - 830
    - 844
    - 844
    - 852
    - elaboration
  - !!python/tuple
    - 852
    - 870
    - 830
    - 852
    - list
  - !!python/tuple
    - 852
    - 857
    - 857
    - 870
    - elaboration
  - !!python/tuple
    - 870
    - 1743
    - 830
    - 870
    - list
  - !!python/tuple
    - 870
    - 874
    - 874
    - 888
    - elaboration
  - !!python/tuple
    - 870
    - 888
    - 888
    - 1743
    - elaboration
  - !!python/tuple
    - 888
    - 908
    - 908
    - 1743
    - list
  - !!python/tuple
    - 890
    - 908
    - 888
    - 890
    - attribution
  - !!python/tuple
    - 893
    - 908
    - 890
    - 893
    - attribution
  - !!python/tuple
    - 893
    - 901
    - 901
    - 908
    - same_unit
  - !!python/tuple
    - 893
    - 894
    - 894
    - 901
    - elaboration
  - !!python/tuple
    - 901
    - 908
    - 893
    - 901
    - same_unit
  - !!python/tuple
    - 901
    - 902
    - 902
    - 908
    - same_unit
  - !!python/tuple
    - 902
    - 908
    - 901
    - 902
    - same_unit
  - !!python/tuple
    - 908
    - 1743
    - 888
    - 908
    - list
  - !!python/tuple
    - 908
    - 916
    - 916
    - 940
    - elaboration
  - !!python/tuple
    - 916
    - 924
    - 924
    - 940
    - purpose
  - !!python/tuple
    - 924
    - 933
    - 933
    - 940
    - elaboration
  - !!python/tuple
    - 908
    - 940
    - 940
    - 1743
    - elaboration
  - !!python/tuple
    - 940
    - 964
    - 964
    - 969
    - purpose
  - !!python/tuple
    - 940
    - 969
    - 969
    - 1743
    - elaboration
  - !!python/tuple
    - 969
    - 979
    - 979
    - 1005
    - list
  - !!python/tuple
    - 979
    - 1005
    - 969
    - 979
    - list
  - !!python/tuple
    - 979
    - 982
    - 982
    - 1005
    - elaboration
  - !!python/tuple
    - 982
    - 989
    - 989
    - 1005
    - contrast
  - !!python/tuple
    - 982
    - 983
    - 983
    - 989
    - elaboration
  - !!python/tuple
    - 989
    - 1005
    - 982
    - 989
    - contrast
  - !!python/tuple
    - 969
    - 1005
    - 1005
    - 1743
    - elaboration
  - !!python/tuple
    - 1005
    - 1026
    - 1026
    - 1743
    - elaboration
  - !!python/tuple
    - 1026
    - 1048
    - 1048
    - 1116
    - same_unit
  - !!python/tuple
    - 1026
    - 1043
    - 1043
    - 1048
    - elaboration
  - !!python/tuple
    - 1048
    - 1116
    - 1026
    - 1048
    - same_unit
  - !!python/tuple
    - 1059
    - 1116
    - 1048
    - 1059
    - attribution
  - !!python/tuple
    - 1048
    - 1058
    - 1058
    - 1059
    - same_unit
  - !!python/tuple
    - 1048
    - 1051
    - 1051
    - 1058
    - elaboration
  - !!python/tuple
    - 1058
    - 1059
    - 1048
    - 1058
    - same_unit
  - !!python/tuple
    - 1072
    - 1116
    - 1059
    - 1072
    - attribution
  - !!python/tuple
    - 1072
    - 1085
    - 1085
    - 1089
    - elaboration
  - !!python/tuple
    - 1072
    - 1089
    - 1089
    - 1116
    - elaboration
  - !!python/tuple
    - 1094
    - 1116
    - 1089
    - 1094
    - attribution
  - !!python/tuple
    - 1094
    - 1099
    - 1099
    - 1116
    - same_unit
  - !!python/tuple
    - 1094
    - 1095
    - 1095
    - 1099
    - elaboration
  - !!python/tuple
    - 1099
    - 1116
    - 1094
    - 1099
    - same_unit
  - !!python/tuple
    - 1099
    - 1108
    - 1108
    - 1116
    - elaboration
  - !!python/tuple
    - 1026
    - 1116
    - 1116
    - 1743
    - elaboration
  - !!python/tuple
    - 1116
    - 1124
    - 1124
    - 1165
    - same_unit
  - !!python/tuple
    - 1116
    - 1119
    - 1119
    - 1124
    - elaboration
  - !!python/tuple
    - 1124
    - 1165
    - 1116
    - 1124
    - same_unit
  - !!python/tuple
    - 1124
    - 1154
    - 1154
    - 1165
    - elaboration
  - !!python/tuple
    - 1116
    - 1165
    - 1165
    - 1743
    - elaboration
  - !!python/tuple
    - 1165
    - 1172
    - 1172
    - 1743
    - elaboration
  - !!python/tuple
    - 1172
    - 1206
    - 1206
    - 1743
    - topic
  - !!python/tuple
    - 1172
    - 1185
    - 1185
    - 1206
    - same_unit
  - !!python/tuple
    - 1172
    - 1180
    - 1180
    - 1185
    - elaboration
  - !!python/tuple
    - 1185
    - 1206
    - 1172
    - 1185
    - same_unit
  - !!python/tuple
    - 1185
    - 1197
    - 1197
    - 1206
    - same_unit
  - !!python/tuple
    - 1185
    - 1193
    - 1193
    - 1197
    - elaboration
  - !!python/tuple
    - 1197
    - 1206
    - 1185
    - 1197
    - same_unit
  - !!python/tuple
    - 1197
    - 1201
    - 1201
    - 1206
    - elaboration
  - !!python/tuple
    - 1206
    - 1743
    - 1172
    - 1206
    - topic
  - !!python/tuple
    - 1210
    - 1243
    - 1206
    - 1210
    - attribution
  - !!python/tuple
    - 1210
    - 1223
    - 1223
    - 1243
    - elaboration
  - !!python/tuple
    - 1223
    - 1232
    - 1232
    - 1233
    - elaboration
  - !!python/tuple
    - 1223
    - 1233
    - 1233
    - 1243
    - elaboration
  - !!python/tuple
    - 1234
    - 1243
    - 1233
    - 1234
    - attribution
  - !!python/tuple
    - 1206
    - 1243
    - 1243
    - 1743
    - elaboration
  - !!python/tuple
    - 1243
    - 1283
    - 1283
    - 1743
    - list
  - !!python/tuple
    - 1243
    - 1245
    - 1245
    - 1283
    - list
  - !!python/tuple
    - 1245
    - 1283
    - 1243
    - 1245
    - list
  - !!python/tuple
    - 1245
    - 1262
    - 1262
    - 1283
    - elaboration
  - !!python/tuple
    - 1262
    - 1269
    - 1269
    - 1283
    - same_unit
  - !!python/tuple
    - 1262
    - 1268
    - 1268
    - 1269
    - same_unit
  - !!python/tuple
    - 1262
    - 1263
    - 1263
    - 1268
    - elaboration
  - !!python/tuple
    - 1268
    - 1269
    - 1262
    - 1268
    - same_unit
  - !!python/tuple
    - 1269
    - 1283
    - 1262
    - 1269
    - same_unit
  - !!python/tuple
    - 1269
    - 1276
    - 1276
    - 1283
    - list
  - !!python/tuple
    - 1276
    - 1283
    - 1269
    - 1276
    - list
  - !!python/tuple
    - 1276
    - 1277
    - 1277
    - 1283
    - elaboration
  - !!python/tuple
    - 1283
    - 1743
    - 1243
    - 1283
    - list
  - !!python/tuple
    - 1283
    - 1327
    - 1327
    - 1437
    - list
  - !!python/tuple
    - 1283
    - 1285
    - 1285
    - 1327
    - elaboration
  - !!python/tuple
    - 1285
    - 1315
    - 1315
    - 1327
    - elaboration
  - !!python/tuple
    - 1327
    - 1437
    - 1283
    - 1327
    - list
  - !!python/tuple
    - 1327
    - 1341
    - 1341
    - 1356
    - same_unit
  - !!python/tuple
    - 1327
    - 1336
    - 1336
    - 1341
    - elaboration
  - !!python/tuple
    - 1341
    - 1356
    - 1327
    - 1341
    - same_unit
  - !!python/tuple
    - 1341
    - 1342
    - 1342
    - 1356
    - attribution
  - !!python/tuple
    - 1342
    - 1349
    - 1349
    - 1356
    - list
  - !!python/tuple
    - 1349
    - 1356
    - 1342
    - 1349
    - list
  - !!python/tuple
    - 1349
    - 1350
    - 1350
    - 1356
    - elaboration
  - !!python/tuple
    - 1327
    - 1356
    - 1356
    - 1437
    - elaboration
  - !!python/tuple
    - 1356
    - 1362
    - 1362
    - 1386
    - purpose
  - !!python/tuple
    - 1365
    - 1386
    - 1362
    - 1365
    - attribution
  - !!python/tuple
    - 1365
    - 1378
    - 1378
    - 1386
    - elaboration
  - !!python/tuple
    - 1356
    - 1386
    - 1386
    - 1437
    - elaboration
  - !!python/tuple
    - 1386
    - 1389
    - 1389
    - 1437
    - elaboration
  - !!python/tuple
    - 1389
    - 1414
    - 1414
    - 1437
    - elaboration
  - !!python/tuple
    - 1414
    - 1425
    - 1425
    - 1437
    - elaboration
  - !!python/tuple
    - 1283
    - 1437
    - 1437
    - 1743
    - elaboration
  - !!python/tuple
    - 1437
    - 1443
    - 1443
    - 1454
    - elaboration
  - !!python/tuple
    - 1437
    - 1454
    - 1454
    - 1743
    - elaboration
  - !!python/tuple
    - 1454
    - 1480
    - 1480
    - 1487
    - elaboration
  - !!python/tuple
    - 1454
    - 1487
    - 1487
    - 1743
    - elaboration
  - !!python/tuple
    - 1487
    - 1501
    - 1501
    - 1743
    - elaboration
  - !!python/tuple
    - 1507
    - 1545
    - 1501
    - 1507
    - attribution
  - !!python/tuple
    - 1507
    - 1514
    - 1514
    - 1545
    - elaboration
  - !!python/tuple
    - 1501
    - 1545
    - 1545
    - 1743
    - elaboration
  - !!python/tuple
    - 1545
    - 1575
    - 1575
    - 1743
    - elaboration
  - !!python/tuple
    - 1575
    - 1586
    - 1586
    - 1743
    - elaboration
  - !!python/tuple
    - 1586
    - 1597
    - 1597
    - 1607
    - elaboration
  - !!python/tuple
    - 1586
    - 1607
    - 1607
    - 1743
    - elaboration
  - !!python/tuple
    - 1617
    - 1633
    - 1607
    - 1617
    - attribution
  - !!python/tuple
    - 1607
    - 1633
    - 1633
    - 1743
    - elaboration
  - !!python/tuple
    - 1633
    - 1678
    - 1678
    - 1698
    - same_unit
  - !!python/tuple
    - 1633
    - 1671
    - 1671
    - 1678
    - elaboration
  - !!python/tuple
    - 1678
    - 1698
    - 1633
    - 1678
    - same_unit
  - !!python/tuple
    - 1678
    - 1679
    - 1679
    - 1698
    - elaboration
  - !!python/tuple
    - 1679
    - 1691
    - 1691
    - 1698
    - circumstance
  - !!python/tuple
    - 1633
    - 1698
    - 1698
    - 1743
    - elaboration
  - !!python/tuple
    - 1698
    - 1706
    - 1706
    - 1743
    - elaboration
  - !!python/tuple
    - 1706
    - 1713
    - 1713
    - 1743
    - elaboration
  - !!python/tuple
    - 1713
    - 1736
    - 1736
    - 1743
    - same_unit
  - !!python/tuple
    - 1713
    - 1729
    - 1729
    - 1736
    - elaboration
  - !!python/tuple
    - 1736
    - 1743
    - 1713
    - 1736
    - same_unit
  tokens:
  - The
  - focus
  - of
  - this
  - paper
  - is
  - to
  - show
  - that
  - finite-width
  - deep
  - neural
  - networks
  - with
  - fully
  - connected
  - layers
  - and
  - ReLU
  - activations
  - are
  - rate-distortion
  - optimal
  - approximators
  - of
  - certain
  - classes
  - of
  - functions
  - ','
  - meaning
  - the
  - approximation
  - error
  - decays
  - exponentially
  - in
  - the
  - number
  - of
  - neurons
  - in
  - the
  - network
  - .
  - The
  - function
  - classes
  - explored
  - in
  - this
  - paper
  - are
  - ':'
  - 1-d
  - polynomials
  - -LRB-
  - 'on'
  - bounded
  - intervals
  - -RRB-
  - ','
  - 1-d
  - sinusoidal
  - functions
  - -LRB-
  - 'on'
  - bounded
  - intervals
  - -RRB-
  - ','
  - and
  - other
  - 1-d
  - functions
  - built
  - from
  - compositions
  - or
  - linear
  - combinations
  - of
  - these
  - ','
  - such
  - as
  - the
  - so-called
  - class
  - of
  - '``'
  - oscillatory
  - textures
  - ''''''
  - and
  - a
  - class
  - of
  - continuous
  - but
  - nowhere
  - differentiable
  - functions
  - known
  - as
  - Weierstrass
  - functions
  - .
  - Finally
  - ','
  - the
  - paper
  - also
  - shows
  - that
  - as
  - the
  - desired
  - approximation
  - accuracy
  - goes
  - to
  - zero
  - finite-width
  - deep
  - ReLU
  - networks
  - require
  - asymptotically
  - fewer
  - neurons
  - than
  - finite-depth
  - wide
  - ReLU
  - networks
  - in
  - approximating
  - a
  - broad
  - class
  - of
  - smooth
  - functions
  - .
  - The
  - paper
  - is
  - well-written
  - and
  - the
  - technical
  - results
  - are
  - presented
  - in
  - a
  - way
  - that
  - is
  - easy
  - to
  - understand
  - .
  - The
  - results
  - are
  - somewhat
  - novel
  - ','
  - although
  - they
  - do
  - build
  - 'off'
  - other
  - recent
  - works
  - ','
  - namely
  - Yarotsky
  - -LRB-
  - '2016'
  - -RRB-
  - and
  - Telgarsky
  - -LRB-
  - '2015'
  - -RRB-
  - .
  - However
  - ','
  - the
  - authors
  - were
  - careful
  - to
  - cite
  - when
  - they
  - reuse
  - proof
  - techniques
  - from
  - these
  - and
  - other
  - works
  - .
  - The
  - results
  - in
  - the
  - main
  - text
  - appear
  - to
  - be
  - technically
  - sound
  - .
  - I
  - did
  - not
  - check
  - carefully
  - all
  - the
  - proofs
  - in
  - the
  - supplemental
  - materials
  - .
  - My
  - major
  - criticism
  - is
  - that
  - the
  - focus
  - 'on'
  - certain
  - specific
  - function
  - classes
  - -LRB-
  - oscillatory
  - textures
  - ','
  - Weierstrauss
  - functions
  - -RRB-
  - seems
  - arbitrary
  - ','
  - and
  - leaves
  - open
  - many
  - questions
  - .
  - For
  - example
  - ','
  - there
  - is
  - existing
  - work
  - 'on'
  - the
  - approximation
  - ability
  - of
  - deep
  - ReLU
  - networks
  - for
  - functions
  - in
  - more
  - general
  - Holder
  - and
  - Sobolev
  - spaces
  - ':'
  - Hadrien
  - Montanelli
  - and
  - Qiang
  - Du
  - .
  - Deep
  - ReLU
  - networks
  - lessen
  - the
  - curse
  - of
  - dimensionality
  - .
  - arXiv
  - preprint
  - arXiv
  - ':'
  - '1712.08688'
  - ','
  - '2017'
  - .
  - J.
  - Schmidt-Hieber
  - .
  - Nonparametric
  - regression
  - using
  - deep
  - neural
  - networks
  - with
  - ReLU
  - activation
  - function
  - .
  - ArXiv
  - e-prints
  - ','
  - August
  - '2017'
  - .
  - I
  - was
  - left
  - wondering
  - how
  - the
  - present
  - results
  - relate
  - to
  - these
  - works
  - ','
  - and
  - what
  - insight
  - we
  - get
  - from
  - understanding
  - these
  - particular
  - function
  - classes
  - that
  - we
  - do
  - n't
  - get
  - from
  - understanding
  - Holder
  - or
  - Sobolev
  - spaces
  - .
  - Major
  - comments
  - In
  - Section
  - '3'
  - ','
  - I
  - found
  - the
  - progression
  - of
  - the
  - results
  - from
  - approximation
  - of
  - x
  - ^
  - '2'
  - ','
  - to
  - multiplication
  - xy
  - ','
  - and
  - to
  - general
  - smooth
  - functions
  - to
  - be
  - very
  - natural
  - and
  - well-motivated
  - .
  - However
  - ','
  - sections
  - '4'
  - and
  - '5'
  - seem
  - lack
  - somewhat
  - in
  - motivation
  - ','
  - since
  - here
  - the
  - authors
  - focus
  - 'on'
  - very
  - specific
  - function
  - classes
  - -LRB-
  - sinusoidal
  - functions
  - ','
  - oscillatory
  - textures
  - ','
  - and
  - Weierstrass
  - functions
  - -RRB-
  - .
  - While
  - these
  - results
  - are
  - still
  - interesting
  - ','
  - focusing
  - 'on'
  - such
  - specific
  - functions
  - is
  - less
  - satisfactory
  - ','
  - since
  - it
  - raises
  - questions
  - about
  - the
  - 'true'
  - scope
  - of
  - the
  - results
  - -LRB-
  - e.g.
  - ','
  - will
  - similar
  - approximation
  - rates
  - extend
  - to
  - other
  - fractal
  - functions
  - ','
  - or
  - just
  - Weierstrauss
  - functions
  - '?'
  - -RRB-
  - .
  - Could
  - the
  - authors
  - give
  - further
  - justification
  - for
  - why
  - these
  - function
  - classes
  - are
  - interesting
  - to
  - focus
  - 'on'
  - ','
  - or
  - why
  - they
  - limit
  - themselves
  - in
  - this
  - way
  - '?'
  - Can
  - the
  - authors
  - also
  - put
  - these
  - results
  - more
  - into
  - context
  - with
  - existing
  - results
  - 'on'
  - the
  - approximation
  - with
  - ReLU
  - networks
  - '?'
  - The
  - authors
  - state
  - multiple
  - times
  - that
  - '``'
  - all
  - our
  - results
  - apply
  - to
  - the
  - multivariate
  - case
  - ''''''
  - but
  - that
  - they
  - restrict
  - themselves
  - to
  - the
  - univariate
  - case
  - for
  - simplicity
  - of
  - presentation
  - .
  - While
  - this
  - is
  - fine
  - ','
  - some
  - indication
  - of
  - how
  - the
  - results
  - are
  - altered
  - in
  - the
  - multivariate
  - case
  - would
  - be
  - useful
  - .
  - For
  - example
  - ','
  - does
  - the
  - fixed-width
  - M
  - in
  - multivariate
  - generalizations
  - of
  - Prop
  - '3.1'
  - --
  - '3.3'
  - need
  - to
  - be
  - bigger
  - ','
  - smaller
  - ','
  - or
  - the
  - same
  - '?'
  - What
  - other
  - constants
  - are
  - dimensionally
  - dependent
  - '?'
  - Do
  - the
  - multivariate
  - generalization
  - of
  - their
  - results
  - bear
  - the
  - '``'
  - curse
  - of
  - dimensionality
  - ''''''
  - ','
  - i.e.
  - ','
  - does
  - the
  - number
  - of
  - neurons
  - needed
  - to
  - reach
  - epsilon
  - accuracy
  - depend
  - geometrically
  - 'on'
  - the
  - dimension
  - '?'
  - Minor
  - comments
  - A
  - conclusion
  - or
  - discussion
  - section
  - summarizing
  - the
  - overall
  - technical
  - contribution
  - would
  - be
  - useful
  - for
  - the
  - reader
  - .
  - Also
  - ','
  - it
  - would
  - be
  - useful
  - to
  - include
  - some
  - discussion
  - 'on'
  - remaining
  - open
  - problems
  - or
  - future
  - work
  - .
  - 'On'
  - pg
  - .
  - '2'
  - ','
  - the
  - authors
  - state
  - '``'
  - the
  - approximation
  - results
  - throughout
  - the
  - paper
  - guarantee
  - that
  - the
  - magnitude
  - of
  - the
  - weights
  - in
  - the
  - network
  - does
  - not
  - grow
  - faster
  - than
  - polynomially
  - in
  - the
  - cardinality
  - of
  - of
  - the
  - domain
  - over
  - which
  - the
  - approximation
  - takes
  - place
  - ''''''
  - .
  - What
  - does
  - '``'
  - cardinality
  - of
  - the
  - domain
  - ''''''
  - here
  - mean
  - '?'
  - I
  - think
  - the
  - authors
  - mean
  - the
  - size
  - D
  - of
  - the
  - interval
  - -LSB-
  - '-'
  - D
  - ','
  - D
  - -RSB-
  - over
  - which
  - the
  - approximation
  - is
  - valid
  - .
  - 'On'
  - pg
  - .
  - '7'
  - ','
  - the
  - authors
  - say
  - '``'
  - We
  - note
  - that
  - this
  - result
  - allows
  - to
  - show
  - that
  - local
  - cosine
  - bases
  - -LRB-
  - cite
  - -RRB-
  - can
  - be
  - approximated
  - by
  - deep
  - ReLU
  - networks
  - with
  - exponential
  - error
  - decay
  - '...'
  - ''''''
  - .
  - I
  - think
  - the
  - authors
  - mean
  - to
  - say
  - '``'
  - '...'
  - this
  - result
  - allows
  - us
  - to
  - show
  - '...'
  - ''''''
  - or
  - '``'
  - this
  - result
  - allows
  - one
  - to
  - show
  - '...'
  - ''''''
  - .
  - Although
  - it
  - '''s'
  - not
  - clear
  - to
  - me
  - whether
  - this
  - means
  - it
  - has
  - been
  - shown
  - -LRB-
  - it
  - '''s'
  - a
  - direct
  - corollary
  - -RRB-
  - ','
  - or
  - could
  - possible
  - be
  - shown
  - -LRB-
  - it
  - '''s'
  - a
  - corollary
  - ','
  - but
  - needs
  - some
  - non-trivial
  - work
  - -RRB-
  - .
  - Also
  - ','
  - one
  - line
  - to
  - specify
  - what
  - a
  - '``'
  - local
  - cosine
  - basis
  - ''''''
  - is
  - would
  - be
  - helpful
  - .
  - Thank
  - you
  - for
  - pointing
  - out
  - references
  - -LRB-
  - Montanelli
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - and
  - -LRB-
  - Schmidt-Hieber
  - ','
  - '2017'
  - -RRB-
  - .
  - We
  - considered
  - the
  - Weierstrass
  - function
  - and
  - oscillatory
  - textures
  - as
  - these
  - functions
  - are
  - known
  - to
  - be
  - hard
  - to
  - approximate
  - and
  - there
  - are
  - 'no'
  - known
  - classical
  - methods
  - that
  - would
  - yield
  - exponential
  - approximation
  - accuracy
  - .
  - The
  - main
  - point
  - of
  - our
  - results
  - here
  - is
  - that
  - deep
  - ReLU
  - networks
  - do
  - provide
  - exponential
  - approximation
  - accuracy
  - and
  - are
  - hence
  - the
  - first
  - approximation
  - method
  - to
  - deliver
  - exponential
  - accuracy
  - .
  - Moreover
  - ','
  - the
  - Weierstrass
  - function
  - does
  - not
  - have
  - weak
  - derivatives
  - and
  - possesses
  - Hlder
  - smoothness
  - which
  - can
  - be
  - infinitely
  - small
  - ;
  - it
  - can
  - therefore
  - not
  - be
  - considered
  - an
  - element
  - of
  - a
  - particular
  - Hlder
  - or
  - Sobolev
  - space
  - .
  - Regarding
  - our
  - other
  - results
  - ','
  - we
  - achieve
  - faster
  - approximation
  - rates
  - than
  - the
  - ones
  - obtained
  - for
  - general
  - Sobolev
  - and
  - Hlder
  - spaces
  - .
  - The
  - ReLU
  - network
  - approximation
  - results
  - for
  - functions
  - in
  - the
  - unit
  - ball
  - in
  - Sobolev
  - spaces
  - first
  - obtained
  - in
  - -LRB-
  - Yarotsky
  - ','
  - '2017'
  - -RRB-
  - and
  - cited
  - in
  - -LRB-
  - Montanelli
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - show
  - that
  - the
  - number
  - of
  - nonzero
  - weights
  - needed
  - in
  - the
  - approximating
  - ReLU
  - network
  - grows
  - '``'
  - polynomially
  - ''''''
  - in
  - the
  - inverse
  - of
  - the
  - approximation
  - error
  - ','
  - e.g.
  - O
  - -LRB-
  - \
  - eps
  - ^
  - -LCB-
  - '-'
  - d/m
  - -RCB-
  - \
  - log
  - -LRB-
  - 1/eps
  - -RRB-
  - -RRB-
  - for
  - W
  - ^
  - -LCB-
  - m
  - ','
  - \
  - infty
  - -RCB-
  - -LRB-
  - -LSB-
  - 0,1
  - -RSB-
  - ^
  - d
  - -RRB-
  - .
  - Theorem
  - '5'
  - in
  - -LRB-
  - Schmidt-Hieber
  - ','
  - '2017'
  - -RRB-
  - also
  - states
  - that
  - in
  - the
  - approximation
  - of
  - functions
  - in
  - the
  - unit
  - ball
  - in
  - Hlder
  - spaces
  - ','
  - the
  - number
  - of
  - nonzero
  - weights
  - needed
  - in
  - the
  - approximating
  - ReLU
  - network
  - has
  - to
  - grow
  - '``'
  - polynomially
  - ''''''
  - in
  - the
  - inverse
  - of
  - the
  - approximation
  - error
  - .
  - In
  - contrast
  - ','
  - we
  - show
  - '1'
  - .
  - Exponential
  - approximation
  - accuracy
  - for
  - highly
  - oscillatory
  - sinusoidal
  - functions
  - -LRB-
  - Theorem
  - '4.1'
  - -RRB-
  - ','
  - leading
  - to
  - exponential
  - approximation
  - accuracy
  - for
  - oscillatory
  - textures
  - -LRB-
  - Proposition
  - '5.2'
  - -RRB-
  - and
  - the
  - Weierstrass
  - function
  - -LRB-
  - Proposition
  - '5.4'
  - -RRB-
  - .
  - Exponential
  - approximation
  - accuracy
  - means
  - that
  - the
  - approximation
  - error
  - decays
  - exponentially
  - in
  - the
  - number
  - of
  - nonzero
  - weights
  - ','
  - which
  - is
  - equivalent
  - to
  - the
  - number
  - of
  - nonzero
  - weights
  - required
  - growing
  - '``'
  - logarithmically
  - ''''''
  - in
  - the
  - inverse
  - approximation
  - error
  - .
  - '2'
  - .
  - Our
  - results
  - are
  - stated
  - for
  - arbitrary
  - domains
  - ','
  - in
  - contrast
  - to
  - a
  - restriction
  - to
  - the
  - unit
  - ball
  - in
  - -LRB-
  - Yarotsky
  - ','
  - '2017'
  - -RRB-
  - ','
  - -LRB-
  - Montanelli
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - and
  - -LRB-
  - Schmidt-Hieber
  - ','
  - '2017'
  - -RRB-
  - .
  - '3'
  - .
  - We
  - establish
  - that
  - the
  - weights
  - in
  - the
  - networks
  - we
  - construct
  - grow
  - 'no'
  - faster
  - than
  - polynomial
  - in
  - the
  - size
  - of
  - the
  - domain
  - over
  - which
  - approximation
  - is
  - carried
  - out
  - ','
  - a
  - result
  - that
  - is
  - crucial
  - to
  - be
  - able
  - to
  - conclude
  - exponential
  - approximation
  - accuracy
  - .
  - Such
  - a
  - result
  - is
  - not
  - available
  - in
  - existing
  - work
  - -LRB-
  - Yarotsky
  - ','
  - '2017'
  - -RRB-
  - ','
  - -LRB-
  - Montanelli
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - and
  - -LRB-
  - Schmidt-Hieber
  - ','
  - '2017'
  - -RRB-
  - .
  - These
  - three
  - novel
  - aspects
  - allow
  - us
  - to
  - then
  - conclude
  - that
  - the
  - approximation
  - results
  - we
  - found
  - are
  - best
  - possible
  - in
  - the
  - sense
  - of
  - -LRB-
  - Blcskei
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - .
  - --
  - Major
  - comments
  - --
  - 'On'
  - the
  - motivation
  - to
  - focus
  - 'on'
  - specific
  - function
  - classes
  - We
  - find
  - these
  - results
  - interesting
  - as
  - they
  - demonstrate
  - that
  - deep
  - ReLU
  - networks
  - can
  - deliver
  - something
  - that
  - to
  - date
  - has
  - not
  - been
  - accomplished
  - ','
  - namely
  - to
  - approximate
  - -LRB-
  - certain
  - -RRB-
  - fractal
  - functions
  - and
  - oscillatory
  - textures
  - with
  - exponential
  - accuracy
  - .
  - We
  - chose
  - these
  - two
  - function
  - classes
  - as
  - they
  - are
  - known
  - to
  - be
  - notoriously
  - hard
  - to
  - approximate
  - .
  - In
  - fact
  - ','
  - the
  - best
  - approximation
  - results
  - available
  - in
  - the
  - literature
  - for
  - these
  - function
  - classes
  - exhibit
  - low-order
  - polynomial
  - approximation
  - accuracy
  - ','
  - i.e.
  - ','
  - polynomial
  - error
  - decay
  - as
  - opposed
  - to
  - exponential
  - error
  - decay
  - .
  - In
  - practice
  - ','
  - the
  - functional
  - dependence
  - a
  - network
  - is
  - to
  - learn
  - is
  - unknown
  - .
  - In
  - this
  - sense
  - our
  - results
  - say
  - that
  - even
  - very
  - unusual
  - functional
  - dependencies
  - can
  - be
  - learned
  - ','
  - in
  - fact
  - ','
  - optimally
  - so
  - in
  - an
  - approximation-theoretic
  - sense
  - ','
  - by
  - a
  - deep
  - ReLU
  - network
  - ','
  - thus
  - speaking
  - to
  - the
  - universal
  - approximation
  - capability
  - of
  - deep
  - ReLU
  - networks
  - .
  - The
  - results
  - 'on'
  - the
  - approximation
  - of
  - the
  - Weierstrass
  - function
  - can
  - be
  - extended
  - in
  - a
  - straightforward
  - way
  - to
  - certain
  - fractal
  - functions
  - of
  - the
  - same
  - nature
  - ','
  - e.g.
  - the
  - Blancmange
  - curve
  - .
  - The
  - generalization
  - to
  - all
  - fractal
  - functions
  - remains
  - an
  - open
  - problem
  - .
  - 'On'
  - the
  - curse
  - of
  - dimensionality
  - The
  - multivariate
  - generalizations
  - of
  - Props
  - .
  - '3.1'
  - '&'
  - '3.2'
  - follow
  - straightforwardly
  - from
  - the
  - univariate
  - case
  - .
  - For
  - the
  - multivariate
  - generalization
  - of
  - Prop.
  - '3.3'
  - one
  - can
  - show
  - that
  - the
  - width
  - grows
  - linearly
  - ','
  - and
  - hence
  - optimally
  - ','
  - in
  - the
  - number
  - of
  - dimensions
  - .
  - The
  - multivariate
  - generalizations
  - of
  - our
  - results
  - do
  - not
  - suffer
  - from
  - a
  - '``'
  - curse
  - of
  - dimensionality
  - ''''''
  - ;
  - in
  - fact
  - ','
  - our
  - results
  - exhibit
  - the
  - same
  - qualitative
  - behavior
  - as
  - those
  - reported
  - previously
  - in
  - Prop.
  - '3.3'
  - and
  - Lemma
  - '3.8'
  - in
  - -LRB-
  - Schwab
  - '&'
  - Zech
  - ','
  - '2017'
  - -RRB-
  - ','
  - which
  - show
  - that
  - the
  - '``'
  - curse
  - of
  - dimensionality
  - ''''''
  - can
  - be
  - overcome
  - when
  - approximating
  - polynomials
  - by
  - ReLU
  - networks
  - .
  - In
  - addition
  - ','
  - it
  - was
  - shown
  - recently
  - in
  - -LRB-
  - Grohs
  - et
  - al.
  - ','
  - '2018'
  - -RRB-
  - that
  - deep
  - ReLU
  - networks
  - can
  - approximate
  - certain
  - solution
  - families
  - of
  - parametric
  - PDEs
  - depending
  - 'on'
  - a
  - large
  - -LRB-
  - possibly
  - infinite
  - -RRB-
  - number
  - of
  - parameters
  - while
  - overcoming
  - the
  - curse
  - of
  - dimensionality
  - .
- comment_id: B1esP4YtAm
  rels:
  - !!python/tuple
    - 0
    - 20
    - 20
    - 457
    - textualorganization
  - !!python/tuple
    - 0
    - 2
    - 2
    - 20
    - elaboration
  - !!python/tuple
    - 20
    - 457
    - 0
    - 20
    - textualorganization
  - !!python/tuple
    - 20
    - 36
    - 36
    - 457
    - elaboration
  - !!python/tuple
    - 36
    - 48
    - 48
    - 54
    - elaboration
  - !!python/tuple
    - 36
    - 54
    - 54
    - 58
    - manner
  - !!python/tuple
    - 36
    - 58
    - 58
    - 457
    - elaboration
  - !!python/tuple
    - 58
    - 66
    - 66
    - 77
    - elaboration
  - !!python/tuple
    - 66
    - 71
    - 71
    - 77
    - same_unit
  - !!python/tuple
    - 66
    - 70
    - 70
    - 71
    - elaboration
  - !!python/tuple
    - 71
    - 77
    - 66
    - 71
    - same_unit
  - !!python/tuple
    - 58
    - 77
    - 77
    - 457
    - elaboration
  - !!python/tuple
    - 77
    - 87
    - 87
    - 94
    - elaboration
  - !!python/tuple
    - 77
    - 94
    - 94
    - 457
    - elaboration
  - !!python/tuple
    - 94
    - 103
    - 103
    - 118
    - contrast
  - !!python/tuple
    - 103
    - 118
    - 94
    - 103
    - contrast
  - !!python/tuple
    - 106
    - 118
    - 103
    - 106
    - attribution
  - !!python/tuple
    - 94
    - 118
    - 118
    - 457
    - elaboration
  - !!python/tuple
    - 118
    - 119
    - 119
    - 120
    - elaboration
  - !!python/tuple
    - 118
    - 120
    - 120
    - 142
    - elaboration
  - !!python/tuple
    - 120
    - 128
    - 128
    - 142
    - elaboration
  - !!python/tuple
    - 118
    - 142
    - 142
    - 153
    - elaboration
  - !!python/tuple
    - 142
    - 150
    - 150
    - 153
    - elaboration
  - !!python/tuple
    - 118
    - 153
    - 153
    - 457
    - elaboration
  - !!python/tuple
    - 153
    - 174
    - 174
    - 181
    - elaboration
  - !!python/tuple
    - 153
    - 181
    - 181
    - 457
    - elaboration
  - !!python/tuple
    - 181
    - 331
    - 331
    - 457
    - list
  - !!python/tuple
    - 181
    - 182
    - 182
    - 183
    - elaboration
  - !!python/tuple
    - 181
    - 183
    - 183
    - 193
    - elaboration
  - !!python/tuple
    - 181
    - 193
    - 193
    - 224
    - elaboration
  - !!python/tuple
    - 193
    - 196
    - 196
    - 224
    - purpose
  - !!python/tuple
    - 196
    - 198
    - 198
    - 224
    - purpose
  - !!python/tuple
    - 198
    - 207
    - 207
    - 224
    - list
  - !!python/tuple
    - 207
    - 224
    - 198
    - 207
    - list
  - !!python/tuple
    - 181
    - 224
    - 224
    - 234
    - elaboration
  - !!python/tuple
    - 181
    - 234
    - 234
    - 331
    - elaboration
  - !!python/tuple
    - 247
    - 259
    - 234
    - 247
    - condition
  - !!python/tuple
    - 247
    - 253
    - 253
    - 259
    - elaboration
  - !!python/tuple
    - 234
    - 259
    - 259
    - 331
    - elaboration
  - !!python/tuple
    - 259
    - 279
    - 279
    - 331
    - elaboration
  - !!python/tuple
    - 279
    - 286
    - 286
    - 300
    - purpose
  - !!python/tuple
    - 286
    - 290
    - 290
    - 300
    - elaboration
  - !!python/tuple
    - 279
    - 300
    - 300
    - 331
    - elaboration
  - !!python/tuple
    - 300
    - 308
    - 308
    - 331
    - elaboration
  - !!python/tuple
    - 308
    - 312
    - 312
    - 331
    - list
  - !!python/tuple
    - 312
    - 331
    - 308
    - 312
    - list
  - !!python/tuple
    - 314
    - 331
    - 312
    - 314
    - attribution
  - !!python/tuple
    - 314
    - 315
    - 315
    - 331
    - circumstance
  - !!python/tuple
    - 331
    - 457
    - 181
    - 331
    - list
  - !!python/tuple
    - 331
    - 344
    - 344
    - 367
    - elaboration
  - !!python/tuple
    - 344
    - 359
    - 359
    - 367
    - elaboration
  - !!python/tuple
    - 331
    - 367
    - 367
    - 457
    - elaboration
  - !!python/tuple
    - 367
    - 375
    - 375
    - 407
    - purpose
  - !!python/tuple
    - 377
    - 407
    - 375
    - 377
    - attribution
  - !!python/tuple
    - 377
    - 401
    - 401
    - 407
    - elaboration
  - !!python/tuple
    - 367
    - 407
    - 407
    - 457
    - elaboration
  - !!python/tuple
    - 427
    - 457
    - 407
    - 427
    - attribution
  - !!python/tuple
    - 427
    - 440
    - 440
    - 457
    - elaboration
  tokens:
  - Summary
  - ':'
  - The
  - role
  - of
  - auxiliary
  - tasks
  - is
  - to
  - improve
  - the
  - generalization
  - performance
  - of
  - the
  - principal
  - task
  - of
  - interest
  - .
  - So
  - far
  - ','
  - hand-crafted
  - auxiliary
  - tasks
  - are
  - generated
  - ','
  - tailored
  - for
  - a
  - problem
  - of
  - interest
  - .
  - The
  - current
  - work
  - addresses
  - a
  - meta-learning
  - approach
  - to
  - automatically
  - generate
  - auxiliary
  - tasks
  - suited
  - to
  - the
  - principal
  - task
  - ','
  - without
  - human
  - knowledge
  - .
  - The
  - key
  - components
  - of
  - the
  - method
  - are
  - ':'
  - -LRB-
  - '1'
  - -RRB-
  - meta-generator
  - ;
  - -LRB-
  - '2'
  - -RRB-
  - multi-task
  - evaluator
  - .
  - These
  - two
  - models
  - are
  - trained
  - using
  - the
  - gradient-based
  - meta-learning
  - technique
  - -LRB-
  - for
  - instance
  - ','
  - MAML
  - -RRB-
  - .
  - The
  - problem
  - of
  - image
  - classification
  - is
  - considered
  - only
  - ','
  - while
  - authors
  - claimed
  - the
  - method
  - can
  - be
  - easily
  - applied
  - to
  - other
  - problems
  - as
  - well
  - .
  - Strengths
  - ':'
  - '-'
  - To
  - my
  - best
  - knowledge
  - ','
  - the
  - idea
  - of
  - applying
  - the
  - meta-learning
  - to
  - the
  - automatic
  - generation
  - of
  - auxiliary
  - tasks
  - is
  - novel
  - .
  - '-'
  - The
  - paper
  - is
  - well
  - written
  - and
  - easy
  - to
  - read
  - .
  - '-'
  - The
  - method
  - nicely
  - blends
  - a
  - few
  - components
  - such
  - as
  - self-supervised
  - learning
  - ','
  - meta-learning
  - ','
  - auxiliary
  - tasks
  - into
  - a
  - single
  - model
  - to
  - tackle
  - the
  - meta
  - auxiliary
  - learning
  - .
  - Weakness
  - ':'
  - '-'
  - The
  - performance
  - gain
  - is
  - not
  - substantial
  - in
  - experiments
  - .
  - I
  - would
  - like
  - to
  - suggest
  - to
  - use
  - the
  - state-of-the-arts
  - classifier
  - for
  - the
  - principal
  - task
  - and
  - to
  - evaluate
  - how
  - much
  - gain
  - your
  - method
  - can
  - get
  - with
  - the
  - help
  - of
  - auxiliary
  - tasks
  - .
  - You
  - can
  - refer
  - to
  - the
  - state-of-the-arts
  - performance
  - 'on'
  - CIFAR
  - .
  - '-'
  - If
  - the
  - information
  - 'on'
  - the
  - hierarchy
  - of
  - sub-categories
  - is
  - not
  - available
  - ','
  - it
  - will
  - be
  - an
  - annoying
  - hyperparameters
  - that
  - should
  - be
  - well
  - tuned
  - .
  - We
  - thank
  - for
  - the
  - reviewer
  - for
  - their
  - positive
  - comments
  - 'on'
  - our
  - work
  - ','
  - and
  - we
  - share
  - our
  - responses
  - below
  - .
  - The
  - purpose
  - of
  - our
  - work
  - is
  - not
  - to
  - achieve
  - state-of-the-art
  - performance
  - simply
  - by
  - incorporating
  - the
  - latest
  - network
  - architectures
  - and
  - optimisers
  - .
  - Instead
  - ','
  - we
  - provide
  - a
  - novel
  - general
  - framework
  - for
  - automating
  - generalisation
  - ','
  - and
  - show
  - that
  - when
  - used
  - with
  - standard
  - classification
  - networks
  - across
  - all
  - baselines
  - ','
  - our
  - method
  - performs
  - the
  - best
  - .
  - Furthermore
  - ','
  - as
  - we
  - also
  - explained
  - in
  - Reviewer
  - '#'
  - '3'
  - ','
  - the
  - hyper-parameters
  - for
  - defining
  - a
  - hierarchy
  - is
  - not
  - critical
  - ','
  - and
  - we
  - can
  - choose
  - an
  - arbitrary
  - hierarchy
  - whilst
  - still
  - achieving
  - better
  - performance
  - than
  - baselines
  - .
  - In
  - the
  - future
  - work
  - ','
  - we
  - would
  - like
  - to
  - explore
  - how
  - to
  - find
  - the
  - optimal
  - hierarchy
  - in
  - an
  - automatic
  - manner
  - ','
  - or
  - provide
  - an
  - alternative
  - solution
  - 'on'
  - building
  - a
  - general
  - type
  - of
  - auxiliary
  - tasks
  - -LRB-
  - such
  - as
  - regression
  - -RRB-
  - .
  - However
  - ','
  - this
  - is
  - the
  - first
  - work
  - to
  - present
  - a
  - double-gradient
  - method
  - for
  - auxiliary
  - task
  - generation
  - ','
  - and
  - we
  - believe
  - that
  - it
  - is
  - important
  - to
  - present
  - the
  - success
  - of
  - this
  - initial
  - method
  - now
  - given
  - how
  - simple
  - and
  - general
  - it
  - is
  - ','
  - and
  - then
  - fine-tune
  - other
  - aspects
  - in
  - future
  - work
  - .
- comment_id: B1efH9Ngg4
  rels:
  - !!python/tuple
    - 0
    - 8
    - 8
    - 17
    - list
  - !!python/tuple
    - 8
    - 17
    - 0
    - 8
    - list
  - !!python/tuple
    - 8
    - 15
    - 15
    - 17
    - elaboration
  - !!python/tuple
    - 0
    - 17
    - 17
    - 1161
    - elaboration
  - !!python/tuple
    - 17
    - 26
    - 26
    - 87
    - elaboration
  - !!python/tuple
    - 29
    - 41
    - 26
    - 29
    - attribution
  - !!python/tuple
    - 26
    - 41
    - 41
    - 87
    - elaboration
  - !!python/tuple
    - 17
    - 87
    - 87
    - 1161
    - elaboration
  - !!python/tuple
    - 87
    - 112
    - 112
    - 147
    - elaboration
  - !!python/tuple
    - 112
    - 137
    - 137
    - 147
    - means
  - !!python/tuple
    - 87
    - 147
    - 147
    - 1161
    - elaboration
  - !!python/tuple
    - 147
    - 164
    - 164
    - 1161
    - list
  - !!python/tuple
    - 147
    - 152
    - 152
    - 164
    - elaboration
  - !!python/tuple
    - 155
    - 164
    - 152
    - 155
    - attribution
  - !!python/tuple
    - 155
    - 161
    - 161
    - 164
    - purpose
  - !!python/tuple
    - 164
    - 1161
    - 147
    - 164
    - list
  - !!python/tuple
    - 164
    - 177
    - 177
    - 1161
    - topic
  - !!python/tuple
    - 164
    - 174
    - 174
    - 177
    - same_unit
  - !!python/tuple
    - 174
    - 177
    - 164
    - 174
    - same_unit
  - !!python/tuple
    - 177
    - 1161
    - 164
    - 177
    - topic
  - !!python/tuple
    - 177
    - 221
    - 221
    - 1161
    - elaboration
  - !!python/tuple
    - 221
    - 231
    - 231
    - 241
    - elaboration
  - !!python/tuple
    - 221
    - 241
    - 241
    - 1161
    - elaboration
  - !!python/tuple
    - 241
    - 256
    - 256
    - 1161
    - elaboration
  - !!python/tuple
    - 256
    - 267
    - 267
    - 1161
    - elaboration
  - !!python/tuple
    - 270
    - 298
    - 267
    - 270
    - attribution
  - !!python/tuple
    - 273
    - 298
    - 270
    - 273
    - attribution
  - !!python/tuple
    - 273
    - 281
    - 281
    - 298
    - manner
  - !!python/tuple
    - 281
    - 284
    - 284
    - 298
    - elaboration
  - !!python/tuple
    - 267
    - 298
    - 298
    - 1161
    - elaboration
  - !!python/tuple
    - 298
    - 321
    - 321
    - 1161
    - elaboration
  - !!python/tuple
    - 321
    - 338
    - 338
    - 1161
    - list
  - !!python/tuple
    - 321
    - 323
    - 323
    - 338
    - elaboration
  - !!python/tuple
    - 338
    - 1161
    - 321
    - 338
    - list
  - !!python/tuple
    - 338
    - 350
    - 350
    - 354
    - elaboration
  - !!python/tuple
    - 338
    - 354
    - 354
    - 399
    - elaboration
  - !!python/tuple
    - 354
    - 359
    - 359
    - 399
    - elaboration
  - !!python/tuple
    - 359
    - 377
    - 377
    - 399
    - elaboration
  - !!python/tuple
    - 338
    - 399
    - 399
    - 1161
    - elaboration
  - !!python/tuple
    - 399
    - 405
    - 405
    - 419
    - elaboration
  - !!python/tuple
    - 399
    - 419
    - 419
    - 444
    - elaboration
  - !!python/tuple
    - 430
    - 444
    - 419
    - 430
    - attribution
  - !!python/tuple
    - 430
    - 439
    - 439
    - 444
    - elaboration
  - !!python/tuple
    - 399
    - 444
    - 444
    - 1161
    - elaboration
  - !!python/tuple
    - 448
    - 463
    - 444
    - 448
    - attribution
  - !!python/tuple
    - 448
    - 459
    - 459
    - 463
    - elaboration
  - !!python/tuple
    - 444
    - 463
    - 463
    - 1161
    - elaboration
  - !!python/tuple
    - 463
    - 475
    - 475
    - 503
    - same_unit
  - !!python/tuple
    - 463
    - 465
    - 465
    - 475
    - elaboration
  - !!python/tuple
    - 475
    - 503
    - 463
    - 475
    - same_unit
  - !!python/tuple
    - 475
    - 490
    - 490
    - 503
    - list
  - !!python/tuple
    - 475
    - 484
    - 484
    - 490
    - elaboration
  - !!python/tuple
    - 490
    - 503
    - 475
    - 490
    - list
  - !!python/tuple
    - 463
    - 503
    - 503
    - 1161
    - elaboration
  - !!python/tuple
    - 503
    - 512
    - 512
    - 524
    - elaboration
  - !!python/tuple
    - 503
    - 524
    - 524
    - 529
    - elaboration
  - !!python/tuple
    - 503
    - 529
    - 529
    - 552
    - elaboration
  - !!python/tuple
    - 529
    - 542
    - 542
    - 552
    - elaboration
  - !!python/tuple
    - 542
    - 548
    - 548
    - 552
    - elaboration
  - !!python/tuple
    - 503
    - 552
    - 552
    - 1161
    - elaboration
  - !!python/tuple
    - 552
    - 565
    - 565
    - 1161
    - elaboration
  - !!python/tuple
    - 565
    - 597
    - 597
    - 619
    - elaboration
  - !!python/tuple
    - 565
    - 619
    - 619
    - 620
    - elaboration
  - !!python/tuple
    - 565
    - 620
    - 620
    - 1161
    - circumstance
  - !!python/tuple
    - 620
    - 811
    - 811
    - 1161
    - topic
  - !!python/tuple
    - 620
    - 643
    - 643
    - 653
    - elaboration
  - !!python/tuple
    - 620
    - 653
    - 653
    - 811
    - elaboration
  - !!python/tuple
    - 653
    - 666
    - 666
    - 811
    - elaboration
  - !!python/tuple
    - 666
    - 688
    - 688
    - 700
    - same_unit
  - !!python/tuple
    - 666
    - 682
    - 682
    - 688
    - elaboration
  - !!python/tuple
    - 688
    - 700
    - 666
    - 688
    - same_unit
  - !!python/tuple
    - 688
    - 693
    - 693
    - 700
    - elaboration
  - !!python/tuple
    - 666
    - 700
    - 700
    - 811
    - elaboration
  - !!python/tuple
    - 700
    - 705
    - 705
    - 717
    - elaboration
  - !!python/tuple
    - 700
    - 717
    - 717
    - 738
    - elaboration
  - !!python/tuple
    - 700
    - 738
    - 738
    - 811
    - elaboration
  - !!python/tuple
    - 738
    - 755
    - 755
    - 773
    - purpose
  - !!python/tuple
    - 755
    - 761
    - 761
    - 773
    - elaboration
  - !!python/tuple
    - 761
    - 762
    - 762
    - 773
    - purpose
  - !!python/tuple
    - 762
    - 769
    - 769
    - 773
    - elaboration
  - !!python/tuple
    - 738
    - 773
    - 773
    - 811
    - elaboration
  - !!python/tuple
    - 773
    - 778
    - 778
    - 788
    - elaboration
  - !!python/tuple
    - 778
    - 785
    - 785
    - 788
    - purpose
  - !!python/tuple
    - 773
    - 788
    - 788
    - 811
    - elaboration
  - !!python/tuple
    - 788
    - 792
    - 792
    - 811
    - elaboration
  - !!python/tuple
    - 792
    - 796
    - 796
    - 801
    - elaboration
  - !!python/tuple
    - 792
    - 801
    - 801
    - 811
    - elaboration
  - !!python/tuple
    - 801
    - 808
    - 808
    - 811
    - elaboration
  - !!python/tuple
    - 811
    - 1161
    - 620
    - 811
    - topic
  - !!python/tuple
    - 811
    - 814
    - 814
    - 1161
    - elaboration
  - !!python/tuple
    - 814
    - 825
    - 825
    - 844
    - means
  - !!python/tuple
    - 825
    - 829
    - 829
    - 837
    - elaboration
  - !!python/tuple
    - 825
    - 837
    - 837
    - 844
    - elaboration
  - !!python/tuple
    - 814
    - 844
    - 844
    - 1161
    - explanation
  - !!python/tuple
    - 844
    - 853
    - 853
    - 865
    - elaboration
  - !!python/tuple
    - 844
    - 865
    - 865
    - 1161
    - elaboration
  - !!python/tuple
    - 865
    - 871
    - 871
    - 1161
    - textualorganization
  - !!python/tuple
    - 867
    - 871
    - 865
    - 867
    - attribution
  - !!python/tuple
    - 871
    - 1161
    - 865
    - 871
    - textualorganization
  - !!python/tuple
    - 871
    - 880
    - 880
    - 898
    - elaboration
  - !!python/tuple
    - 871
    - 898
    - 898
    - 1161
    - elaboration
  - !!python/tuple
    - 898
    - 977
    - 977
    - 1161
    - list
  - !!python/tuple
    - 898
    - 903
    - 903
    - 977
    - elaboration
  - !!python/tuple
    - 903
    - 922
    - 922
    - 928
    - elaboration
  - !!python/tuple
    - 903
    - 928
    - 928
    - 977
    - elaboration
  - !!python/tuple
    - 928
    - 930
    - 930
    - 956
    - elaboration
  - !!python/tuple
    - 930
    - 935
    - 935
    - 956
    - elaboration
  - !!python/tuple
    - 935
    - 939
    - 939
    - 956
    - elaboration
  - !!python/tuple
    - 928
    - 956
    - 956
    - 977
    - elaboration
  - !!python/tuple
    - 956
    - 964
    - 964
    - 977
    - elaboration
  - !!python/tuple
    - 977
    - 1161
    - 898
    - 977
    - list
  - !!python/tuple
    - 977
    - 991
    - 991
    - 1037
    - elaboration
  - !!python/tuple
    - 991
    - 1025
    - 1025
    - 1037
    - same_unit
  - !!python/tuple
    - 991
    - 999
    - 999
    - 1025
    - elaboration
  - !!python/tuple
    - 1025
    - 1037
    - 991
    - 1025
    - same_unit
  - !!python/tuple
    - 1036
    - 1037
    - 1025
    - 1036
    - attribution
  - !!python/tuple
    - 977
    - 1037
    - 1037
    - 1161
    - elaboration
  - !!python/tuple
    - 1037
    - 1042
    - 1042
    - 1161
    - elaboration
  - !!python/tuple
    - 1042
    - 1065
    - 1065
    - 1069
    - elaboration
  - !!python/tuple
    - 1042
    - 1069
    - 1069
    - 1095
    - elaboration
  - !!python/tuple
    - 1042
    - 1095
    - 1095
    - 1161
    - elaboration
  - !!python/tuple
    - 1095
    - 1129
    - 1129
    - 1139
    - elaboration
  - !!python/tuple
    - 1095
    - 1139
    - 1139
    - 1161
    - elaboration
  - !!python/tuple
    - 1139
    - 1151
    - 1151
    - 1161
    - elaboration
  tokens:
  - We
  - first
  - thank
  - the
  - reviewers
  - for
  - their
  - reviews
  - and
  - now
  - answer
  - individually
  - to
  - the
  - concerns
  - raised
  - .
  - '-'
  - Review
  - '#'
  - '1'
  - ':'
  - Thanks
  - a
  - lot
  - .
  - We
  - are
  - glad
  - that
  - the
  - interest
  - of
  - our
  - contributions
  - has
  - been
  - understood
  - and
  - appreciated
  - .
  - 'On'
  - top
  - of
  - the
  - contributions
  - mentioned
  - ','
  - we
  - also
  - would
  - like
  - to
  - mention
  - the
  - sound
  - probabilistic
  - formulation
  - of
  - our
  - model
  - which
  - we
  - think
  - can
  - be
  - valuable
  - since
  - the
  - probabilistic
  - interpretation
  - of
  - the
  - attributes
  - allows
  - to
  - perform
  - meaningful
  - interpolations
  - and
  - to
  - introduce
  - a
  - new
  - sampling
  - procedure
  - .
  - Indeed
  - ','
  - understanding
  - the
  - interaction
  - and
  - the
  - dynamics
  - between
  - all
  - the
  - elements
  - highly
  - interests
  - us
  - and
  - we
  - will
  - work
  - 'on'
  - that
  - in
  - the
  - future
  - ':'
  - Especially
  - ','
  - we
  - would
  - like
  - to
  - investigate
  - how
  - the
  - dimensionality
  - of
  - the
  - attribute
  - space
  - affects
  - the
  - learnt
  - attributes
  - and
  - how
  - we
  - can
  - shape
  - these
  - attributes
  - by
  - providing
  - ','
  - for
  - instance
  - ','
  - weaker
  - attribute
  - functions
  - .
  - '-'
  - Review
  - '#'
  - '2'
  - ':'
  - We
  - are
  - sorry
  - that
  - you
  - find
  - our
  - paper
  - hard
  - to
  - follow
  - .
  - Can
  - you
  - be
  - more
  - specific
  - or
  - provide
  - us
  - with
  - ideas
  - for
  - improvement
  - '?'
  - We
  - are
  - quite
  - surprised
  - about
  - this
  - statement
  - since
  - we
  - tried
  - to
  - be
  - as
  - detailed
  - as
  - possible
  - ':'
  - all
  - aspects
  - of
  - the
  - model
  - are
  - discussed
  - ','
  - motivated
  - and
  - progressively
  - introduced
  - ;
  - we
  - also
  - provided
  - a
  - detailed
  - algorithm
  - together
  - with
  - a
  - figure
  - of
  - our
  - architecture
  - .
  - The
  - experimental
  - part
  - illustrates
  - the
  - different
  - and
  - novel
  - sampling
  - schemes
  - offered
  - by
  - VarNet
  - 'on'
  - a
  - well-known
  - and
  - simple
  - dataset
  - .
  - It
  - is
  - here
  - for
  - illustration
  - purposes
  - and
  - it
  - is
  - not
  - intended
  - to
  - prove
  - anything
  - .
  - Our
  - paper
  - is
  - not
  - about
  - a
  - specific
  - model
  - or
  - implementation
  - .
  - We
  - chose
  - MNIST
  - because
  - it
  - allows
  - to
  - easily
  - understand
  - what
  - this
  - framework
  - provides
  - ','
  - without
  - the
  - need
  - to
  - focus
  - 'on'
  - the
  - implementation
  - of
  - the
  - encoder
  - ','
  - decoder
  - and
  - attribute
  - function
  - .
  - That
  - '''s'
  - why
  - we
  - chose
  - simple
  - MLPs
  - for
  - the
  - encoder
  - ','
  - decoder
  - and
  - attribute
  - function
  - and
  - put
  - this
  - experimental
  - part
  - in
  - appendix
  - .
  - The
  - paper
  - you
  - propose
  - is
  - indeed
  - relevant
  - and
  - we
  - will
  - include
  - it
  - in
  - the
  - related
  - works
  - .
  - But
  - this
  - approach
  - ','
  - like
  - the
  - Fader
  - networks
  - ','
  - requires
  - known
  - attributes
  - -LRB-
  - appearance
  - -RRB-
  - .
  - Concerning
  - the
  - last
  - question
  - ','
  - what
  - prevents
  - z
  - '*'
  - to
  - be
  - independent
  - of
  - x
  - is
  - that
  - the
  - attribute
  - space
  - is
  - of
  - low
  - dimensionality
  - -LRB-
  - as
  - in
  - the
  - Style
  - Tokens
  - paper
  - -RRB-
  - ','
  - so
  - you
  - can
  - not
  - fully
  - reconstruct
  - x
  - by
  - only
  - considering
  - its
  - attributes
  - .
  - In
  - the
  - degenerate
  - case
  - ','
  - z
  - '*'
  - is
  - a
  - noise
  - independent
  - of
  - x
  - and
  - VarNet
  - amounts
  - to
  - a
  - WAE
  - .
  - '-'
  - Review
  - '#'
  - '3'
  - ':'
  - See
  - reply
  - to
  - reviewer
  - '#'
  - '2'
  - concerning
  - the
  - clarity
  - of
  - our
  - paper
  - or
  - the
  - discussion
  - concerning
  - the
  - experimental
  - part
  - .
  - Indeed
  - ','
  - we
  - believe
  - that
  - the
  - part
  - in
  - appendix
  - is
  - optional
  - and
  - is
  - just
  - here
  - for
  - illustrative
  - purposes
  - .
  - \
  - phi
  - -LRB-
  - x
  - ','
  - m
  - -RRB-
  - is
  - just
  - any
  - neural
  - network
  - '``'
  - This
  - attribute
  - function
  - is
  - a
  - deterministic
  - neural
  - network
  - that
  - will
  - be
  - learned
  - during
  - training
  - and
  - whose
  - aim
  - is
  - to
  - compute
  - attributes
  - of
  - x
  - ''''''
  - Sect.
  - '2.1'
  - .
  - In
  - the
  - experiments
  - ','
  - it
  - is
  - just
  - a
  - MLP
  - -LRB-
  - depending
  - 'on'
  - x
  - only
  - or
  - 'on'
  - x
  - and
  - m
  - -RRB-
  - .
  - We
  - will
  - precise
  - that
  - .
  - Concerning
  - your
  - 3rd
  - paragraph
  - ','
  - we
  - would
  - like
  - to
  - stress
  - upon
  - the
  - fact
  - that
  - there
  - is
  - 'no'
  - ground
  - truth
  - for
  - the
  - attributes
  - .
  - The
  - purpose
  - of
  - this
  - framework
  - is
  - not
  - about
  - that
  - nor
  - about
  - disentanglement
  - .
  - Also
  - ','
  - this
  - is
  - a
  - framework
  - to
  - devise
  - generative
  - models
  - with
  - novel
  - sampling
  - properties
  - ','
  - not
  - about
  - a
  - specific
  - implementation
  - ','
  - so
  - there
  - is
  - 'no'
  - point
  - in
  - showing
  - log-likelihood
  - of
  - the
  - reconstructions
  - -LRB-
  - even
  - if
  - we
  - take
  - the
  - same
  - encoder
  - and
  - decoder
  - networks
  - ','
  - how
  - to
  - fairly
  - compare
  - this
  - with
  - a
  - VAE
  - '?'
  - -RRB-
  - .
  - When
  - applied
  - bluntly
  - 'on'
  - CelebA
  - and
  - by
  - considering
  - the
  - '``'
  - Eyeglasses
  - ''''''
  - attribute
  - ','
  - we
  - are
  - for
  - instance
  - capable
  - of
  - adding
  - different
  - style
  - of
  - glasses
  - 'on'
  - the
  - same
  - face
  - simply
  - by
  - sampling
  - .
  - The
  - attribute
  - function
  - learns
  - in
  - this
  - case
  - some
  - kind
  - of
  - color
  - palette
  - .
  - 'On'
  - Dsprites
  - ','
  - we
  - can
  - obtain
  - two-dimensional
  - planes
  - of
  - variations
  - accounting
  - for
  - the
  - scale
  - and
  - y-position
  - -LRB-
  - controlled
  - by
  - the
  - x-axis
  - -RRB-
  - and
  - the
  - rotation
  - and
  - x-position
  - -LRB-
  - controlled
  - by
  - the
  - y-axis
  - -RRB-
  - .
  - There
  - is
  - indeed
  - 'no'
  - reason
  - why
  - we
  - could
  - obtain
  - a
  - dimension
  - accounting
  - for
  - only
  - one
  - attribute
  - .
  - We
  - also
  - applied
  - this
  - framework
  - to
  - the
  - generation
  - of
  - sequences
  - of
  - discrete
  - symbols
  - and
  - 'on'
  - sound
  - generation
  - with
  - successful
  - results
  - .
  - Since
  - these
  - experiments
  - require
  - more
  - tuning
  - about
  - the
  - encoder
  - ','
  - decoder
  - and
  - attribute
  - functions
  - ','
  - we
  - preferred
  - to
  - only
  - display
  - the
  - simplest
  - experiment
  - allowing
  - to
  - understand
  - and
  - focus
  - 'on'
  - the
  - possibilities
  - offered
  - by
  - VarNet
  - .
  - Here
  - there
  - are
  - some
  - specifics
  - about
  - why
  - I
  - found
  - the
  - paper
  - difficult
  - to
  - follow
  - .
  - There
  - are
  - isolated
  - statements
  - that
  - lack
  - a
  - motivation
  - that
  - can
  - guide
  - the
  - reader
  - about
  - why
  - this
  - was
  - a
  - logical
  - step
  - to
  - do
  - .
  - Two
  - examples
  - ':'
  - '``'
  - The
  - idea
  - is
  - then
  - to
  - compute
  - z
  - from
  - z
  - CD
  - by
  - applying
  - a
  - transformation
  - parametrized
  - only
  - by
  - the
  - feature
  - space
  - NN
  - ''''''
  - --
  - '>'
  - what
  - is
  - the
  - motivation
  - '?'
  - '``'
  - We
  - now
  - consider
  - the
  - regularization
  - over
  - Z
  - .
  - This
  - regularization
  - is
  - in
  - fact
  - superfluous
  - and
  - could
  - be
  - removed
  - .
  - ''''''
  - --
  - '>'
  - why
  - is
  - that
  - '?'
  - The
  - is
  - a
  - lack
  - of
  - justification
  - in
  - many
  - places
  - in
  - which
  - many
  - things
  - are
  - taken
  - for
  - granted
  - ','
  - or
  - there
  - is
  - not
  - a
  - clear
  - cause-effect
  - flow
  - .
  - To
  - mention
  - three
  - examples
  - ':'
  - '-'
  - '``'
  - However
  - ','
  - there
  - is
  - 'no'
  - reason
  - ','
  - for
  - a
  - random
  - attribute
  - NN
  - CD
  - NN
  - /
  - '='
  - NN
  - -LRB-
  - x
  - ','
  - m
  - -RRB-
  - ','
  - that
  - p
  - -LRB-
  - x
  - '|'
  - z
  - -RRB-
  - where
  - z
  - FW
  - q
  - -LRB-
  - z
  - '|'
  - x
  - ','
  - NN
  - -RRB-
  - generates
  - variations
  - of
  - the
  - original
  - x
  - with
  - features
  - VBP
  - ''''''
  - --
  - '>'
  - what
  - is
  - the
  - role
  - of
  - NN
  - given
  - that
  - it
  - is
  - not
  - mentioned
  - in
  - the
  - rest
  - of
  - the
  - sentence
  - '?'
  - '-'
  - Regarding
  - 2.3.1
  - ','
  - it
  - is
  - not
  - clear
  - why
  - the
  - distribution
  - _
  - turns
  - out
  - to
  - be
  - similar
  - to
  - the
  - outputs
  - of
  - _i
  - -LRB-
  - even
  - more
  - so
  - if
  - the
  - discriminator
  - encourages
  - them
  - to
  - be
  - turned
  - apart
  - ','
  - and
  - so
  - making
  - it
  - easy
  - to
  - separate
  - z
  - '*'
  - and
  - NN
  - ','
  - while
  - allowing
  - z
  - '*'
  - to
  - contain
  - all
  - information
  - about
  - x
  - -RRB-
  - .
  - '-'
  - In
  - sec.
  - '3.1'
  - .
  - '``'
  - This
  - degenerate
  - behavior
  - is
  - a
  - side-effect
  - of
  - our
  - adversarial
  - regularization
  - since
  - stochastic
  - encoders
  - have
  - been
  - successfully
  - used
  - in
  - WAEs
  - Rubenstein
  - et
  - al.
  - -LRB-
  - '2018'
  - -RRB-
  - ''''''
  - it
  - is
  - not
  - clear
  - what
  - adversarial
  - regularization
  - has
  - to
  - do
  - with
  - the
  - degenerate
  - behavior
  - ','
  - and
  - how
  - the
  - reference
  - gives
  - any
  - support
  - to
  - that
  - claim
  - .
  - Overall
  - ','
  - given
  - that
  - the
  - proposed
  - model
  - has
  - a
  - fair
  - degree
  - of
  - complexity
  - and
  - thus
  - is
  - difficult
  - to
  - explain
  - ','
  - it
  - may
  - be
  - helpful
  - to
  - illustrate
  - and
  - motivate
  - its
  - parts
  - with
  - a
  - specific
  - example
  - -LRB-
  - e.g.
  - an
  - image
  - of
  - a
  - particular
  - thing
  - -RRB-
  - ','
  - describing
  - for
  - each
  - element
  - of
  - the
  - approach
  - ','
  - the
  - kind
  - of
  - information
  - it
  - is
  - supposed
  - to
  - contain
  - for
  - that
  - particular
  - case
  - .
- comment_id: B1ehE56B0X
  rels:
  - !!python/tuple
    - 0
    - 256
    - 256
    - 1116
    - textualorganization
  - !!python/tuple
    - 0
    - 2
    - 2
    - 256
    - textualorganization
  - !!python/tuple
    - 2
    - 256
    - 0
    - 2
    - textualorganization
  - !!python/tuple
    - 2
    - 7
    - 7
    - 21
    - elaboration
  - !!python/tuple
    - 2
    - 21
    - 21
    - 256
    - elaboration
  - !!python/tuple
    - 21
    - 26
    - 26
    - 38
    - same_unit
  - !!python/tuple
    - 21
    - 25
    - 25
    - 26
    - elaboration
  - !!python/tuple
    - 26
    - 38
    - 21
    - 26
    - same_unit
  - !!python/tuple
    - 26
    - 33
    - 33
    - 38
    - elaboration
  - !!python/tuple
    - 21
    - 38
    - 38
    - 256
    - elaboration
  - !!python/tuple
    - 38
    - 59
    - 59
    - 71
    - elaboration
  - !!python/tuple
    - 38
    - 71
    - 71
    - 83
    - elaboration
  - !!python/tuple
    - 38
    - 83
    - 83
    - 256
    - elaboration
  - !!python/tuple
    - 83
    - 85
    - 85
    - 92
    - elaboration
  - !!python/tuple
    - 83
    - 92
    - 92
    - 128
    - elaboration
  - !!python/tuple
    - 92
    - 97
    - 97
    - 128
    - elaboration
  - !!python/tuple
    - 97
    - 110
    - 110
    - 111
    - elaboration
  - !!python/tuple
    - 97
    - 111
    - 111
    - 115
    - elaboration
  - !!python/tuple
    - 97
    - 115
    - 115
    - 128
    - elaboration
  - !!python/tuple
    - 115
    - 118
    - 118
    - 128
    - elaboration
  - !!python/tuple
    - 118
    - 127
    - 127
    - 128
    - same_unit
  - !!python/tuple
    - 118
    - 123
    - 123
    - 127
    - elaboration
  - !!python/tuple
    - 127
    - 128
    - 118
    - 127
    - same_unit
  - !!python/tuple
    - 83
    - 128
    - 128
    - 256
    - elaboration
  - !!python/tuple
    - 128
    - 131
    - 131
    - 256
    - elaboration
  - !!python/tuple
    - 131
    - 133
    - 133
    - 140
    - elaboration
  - !!python/tuple
    - 131
    - 140
    - 140
    - 256
    - elaboration
  - !!python/tuple
    - 140
    - 154
    - 154
    - 159
    - same_unit
  - !!python/tuple
    - 140
    - 148
    - 148
    - 154
    - elaboration
  - !!python/tuple
    - 154
    - 159
    - 140
    - 154
    - same_unit
  - !!python/tuple
    - 154
    - 155
    - 155
    - 159
    - elaboration
  - !!python/tuple
    - 140
    - 159
    - 159
    - 256
    - elaboration
  - !!python/tuple
    - 159
    - 173
    - 173
    - 182
    - elaboration
  - !!python/tuple
    - 159
    - 182
    - 182
    - 201
    - elaboration
  - !!python/tuple
    - 159
    - 201
    - 201
    - 256
    - elaboration
  - !!python/tuple
    - 201
    - 237
    - 237
    - 256
    - same_unit
  - !!python/tuple
    - 201
    - 213
    - 213
    - 237
    - elaboration
  - !!python/tuple
    - 213
    - 217
    - 217
    - 237
    - elaboration
  - !!python/tuple
    - 217
    - 221
    - 221
    - 237
    - elaboration
  - !!python/tuple
    - 221
    - 231
    - 231
    - 237
    - elaboration
  - !!python/tuple
    - 237
    - 256
    - 201
    - 237
    - same_unit
  - !!python/tuple
    - 237
    - 248
    - 248
    - 256
    - elaboration
  - !!python/tuple
    - 248
    - 252
    - 252
    - 256
    - elaboration
  - !!python/tuple
    - 256
    - 1116
    - 0
    - 256
    - textualorganization
  - !!python/tuple
    - 256
    - 280
    - 280
    - 299
    - sequence
  - !!python/tuple
    - 256
    - 276
    - 276
    - 280
    - elaboration
  - !!python/tuple
    - 280
    - 299
    - 256
    - 280
    - sequence
  - !!python/tuple
    - 256
    - 299
    - 299
    - 1116
    - circumstance
  - !!python/tuple
    - 299
    - 318
    - 318
    - 1116
    - list
  - !!python/tuple
    - 318
    - 1116
    - 299
    - 318
    - list
  - !!python/tuple
    - 318
    - 335
    - 335
    - 1116
    - elaboration
  - !!python/tuple
    - 335
    - 355
    - 355
    - 1116
    - list
  - !!python/tuple
    - 335
    - 337
    - 337
    - 355
    - elaboration
  - !!python/tuple
    - 337
    - 345
    - 345
    - 355
    - purpose
  - !!python/tuple
    - 345
    - 348
    - 348
    - 355
    - comparison
  - !!python/tuple
    - 355
    - 1116
    - 335
    - 355
    - list
  - !!python/tuple
    - 355
    - 366
    - 366
    - 1116
    - list
  - !!python/tuple
    - 366
    - 1116
    - 355
    - 366
    - list
  - !!python/tuple
    - 366
    - 375
    - 375
    - 1116
    - list
  - !!python/tuple
    - 366
    - 369
    - 369
    - 375
    - purpose
  - !!python/tuple
    - 375
    - 1116
    - 366
    - 375
    - list
  - !!python/tuple
    - 375
    - 389
    - 389
    - 1116
    - elaboration
  - !!python/tuple
    - 389
    - 415
    - 415
    - 1116
    - list
  - !!python/tuple
    - 407
    - 415
    - 389
    - 407
    - attribution
  - !!python/tuple
    - 415
    - 1116
    - 389
    - 415
    - list
  - !!python/tuple
    - 415
    - 424
    - 424
    - 1116
    - elaboration
  - !!python/tuple
    - 429
    - 457
    - 424
    - 429
    - attribution
  - !!python/tuple
    - 429
    - 433
    - 433
    - 457
    - elaboration
  - !!python/tuple
    - 424
    - 457
    - 457
    - 1116
    - elaboration
  - !!python/tuple
    - 457
    - 475
    - 475
    - 1116
    - elaboration
  - !!python/tuple
    - 475
    - 484
    - 484
    - 1116
    - elaboration
  - !!python/tuple
    - 484
    - 487
    - 487
    - 495
    - purpose
  - !!python/tuple
    - 484
    - 495
    - 495
    - 1116
    - elaboration
  - !!python/tuple
    - 495
    - 504
    - 504
    - 1116
    - list
  - !!python/tuple
    - 504
    - 1116
    - 495
    - 504
    - list
  - !!python/tuple
    - 504
    - 512
    - 512
    - 1116
    - elaboration
  - !!python/tuple
    - 514
    - 524
    - 512
    - 514
    - attribution
  - !!python/tuple
    - 514
    - 520
    - 520
    - 524
    - elaboration
  - !!python/tuple
    - 512
    - 524
    - 524
    - 1116
    - elaboration
  - !!python/tuple
    - 526
    - 569
    - 524
    - 526
    - attribution
  - !!python/tuple
    - 526
    - 540
    - 540
    - 569
    - elaboration
  - !!python/tuple
    - 540
    - 556
    - 556
    - 569
    - manner
  - !!python/tuple
    - 556
    - 561
    - 561
    - 569
    - elaboration
  - !!python/tuple
    - 524
    - 569
    - 569
    - 1116
    - elaboration
  - !!python/tuple
    - 569
    - 571
    - 571
    - 589
    - elaboration
  - !!python/tuple
    - 579
    - 589
    - 571
    - 579
    - attribution
  - !!python/tuple
    - 579
    - 582
    - 582
    - 589
    - elaboration
  - !!python/tuple
    - 569
    - 589
    - 589
    - 1116
    - elaboration
  - !!python/tuple
    - 589
    - 604
    - 604
    - 622
    - elaboration
  - !!python/tuple
    - 604
    - 616
    - 616
    - 622
    - purpose
  - !!python/tuple
    - 589
    - 622
    - 622
    - 1116
    - elaboration
  - !!python/tuple
    - 622
    - 626
    - 626
    - 633
    - elaboration
  - !!python/tuple
    - 622
    - 633
    - 633
    - 652
    - elaboration
  - !!python/tuple
    - 633
    - 638
    - 638
    - 652
    - elaboration
  - !!python/tuple
    - 622
    - 652
    - 652
    - 1116
    - elaboration
  - !!python/tuple
    - 652
    - 656
    - 656
    - 1116
    - textualorganization
  - !!python/tuple
    - 656
    - 1116
    - 652
    - 656
    - textualorganization
  - !!python/tuple
    - 656
    - 671
    - 671
    - 1116
    - elaboration
  - !!python/tuple
    - 671
    - 675
    - 675
    - 684
    - purpose
  - !!python/tuple
    - 671
    - 684
    - 684
    - 1116
    - elaboration
  - !!python/tuple
    - 684
    - 693
    - 693
    - 1116
    - list
  - !!python/tuple
    - 684
    - 686
    - 686
    - 693
    - same_unit
  - !!python/tuple
    - 686
    - 693
    - 684
    - 686
    - same_unit
  - !!python/tuple
    - 693
    - 1116
    - 684
    - 693
    - list
  - !!python/tuple
    - 693
    - 707
    - 707
    - 737
    - same_unit
  - !!python/tuple
    - 693
    - 695
    - 695
    - 707
    - manner
  - !!python/tuple
    - 707
    - 737
    - 693
    - 707
    - same_unit
  - !!python/tuple
    - 707
    - 712
    - 712
    - 737
    - elaboration
  - !!python/tuple
    - 693
    - 737
    - 737
    - 1116
    - elaboration
  - !!python/tuple
    - 737
    - 746
    - 746
    - 754
    - elaboration
  - !!python/tuple
    - 746
    - 749
    - 749
    - 754
    - list
  - !!python/tuple
    - 749
    - 754
    - 746
    - 749
    - list
  - !!python/tuple
    - 749
    - 750
    - 750
    - 754
    - elaboration
  - !!python/tuple
    - 737
    - 754
    - 754
    - 1116
    - elaboration
  - !!python/tuple
    - 754
    - 761
    - 761
    - 774
    - elaboration
  - !!python/tuple
    - 761
    - 765
    - 765
    - 774
    - elaboration
  - !!python/tuple
    - 754
    - 774
    - 774
    - 1116
    - elaboration
  - !!python/tuple
    - 774
    - 796
    - 796
    - 1116
    - elaboration
  - !!python/tuple
    - 798
    - 815
    - 796
    - 798
    - attribution
  - !!python/tuple
    - 796
    - 815
    - 815
    - 1116
    - elaboration
  - !!python/tuple
    - 817
    - 844
    - 815
    - 817
    - attribution
  - !!python/tuple
    - 817
    - 818
    - 818
    - 844
    - elaboration
  - !!python/tuple
    - 815
    - 844
    - 844
    - 1116
    - elaboration
  - !!python/tuple
    - 844
    - 875
    - 875
    - 1116
    - list
  - !!python/tuple
    - 844
    - 851
    - 851
    - 858
    - elaboration
  - !!python/tuple
    - 844
    - 858
    - 858
    - 864
    - elaboration
  - !!python/tuple
    - 844
    - 864
    - 864
    - 875
    - circumstance
  - !!python/tuple
    - 864
    - 871
    - 871
    - 875
    - elaboration
  - !!python/tuple
    - 875
    - 1116
    - 844
    - 875
    - list
  - !!python/tuple
    - 875
    - 886
    - 886
    - 1116
    - elaboration
  - !!python/tuple
    - 886
    - 957
    - 957
    - 1116
    - list
  - !!python/tuple
    - 886
    - 898
    - 898
    - 957
    - elaboration
  - !!python/tuple
    - 898
    - 902
    - 902
    - 903
    - elaboration
  - !!python/tuple
    - 898
    - 903
    - 903
    - 957
    - elaboration
  - !!python/tuple
    - 903
    - 907
    - 907
    - 957
    - elaboration
  - !!python/tuple
    - 907
    - 909
    - 909
    - 957
    - elaboration
  - !!python/tuple
    - 909
    - 925
    - 925
    - 957
    - elaboration
  - !!python/tuple
    - 929
    - 957
    - 925
    - 929
    - attribution
  - !!python/tuple
    - 929
    - 941
    - 941
    - 951
    - same_unit
  - !!python/tuple
    - 929
    - 937
    - 937
    - 941
    - elaboration
  - !!python/tuple
    - 941
    - 951
    - 929
    - 941
    - same_unit
  - !!python/tuple
    - 941
    - 946
    - 946
    - 951
    - elaboration
  - !!python/tuple
    - 929
    - 951
    - 951
    - 957
    - means
  - !!python/tuple
    - 951
    - 953
    - 953
    - 957
    - elaboration
  - !!python/tuple
    - 957
    - 1116
    - 886
    - 957
    - list
  - !!python/tuple
    - 957
    - 975
    - 975
    - 1116
    - list
  - !!python/tuple
    - 959
    - 975
    - 957
    - 959
    - attribution
  - !!python/tuple
    - 975
    - 1116
    - 957
    - 975
    - list
  - !!python/tuple
    - 975
    - 1002
    - 1002
    - 1116
    - list
  - !!python/tuple
    - 977
    - 1002
    - 975
    - 977
    - attribution
  - !!python/tuple
    - 978
    - 1002
    - 977
    - 978
    - attribution
  - !!python/tuple
    - 978
    - 987
    - 987
    - 1002
    - elaboration
  - !!python/tuple
    - 987
    - 999
    - 999
    - 1002
    - example
  - !!python/tuple
    - 1002
    - 1116
    - 975
    - 1002
    - list
  - !!python/tuple
    - 1002
    - 1009
    - 1009
    - 1116
    - list
  - !!python/tuple
    - 1009
    - 1116
    - 1002
    - 1009
    - list
  - !!python/tuple
    - 1009
    - 1023
    - 1023
    - 1116
    - list
  - !!python/tuple
    - 1010
    - 1023
    - 1009
    - 1010
    - attribution
  - !!python/tuple
    - 1010
    - 1017
    - 1017
    - 1023
    - purpose
  - !!python/tuple
    - 1023
    - 1116
    - 1009
    - 1023
    - list
  - !!python/tuple
    - 1023
    - 1034
    - 1034
    - 1116
    - list
  - !!python/tuple
    - 1023
    - 1026
    - 1026
    - 1034
    - same_unit
  - !!python/tuple
    - 1023
    - 1025
    - 1025
    - 1026
    - same_unit
  - !!python/tuple
    - 1025
    - 1026
    - 1023
    - 1025
    - same_unit
  - !!python/tuple
    - 1026
    - 1034
    - 1023
    - 1026
    - same_unit
  - !!python/tuple
    - 1026
    - 1028
    - 1028
    - 1034
    - purpose
  - !!python/tuple
    - 1034
    - 1116
    - 1023
    - 1034
    - list
  - !!python/tuple
    - 1036
    - 1076
    - 1034
    - 1036
    - attribution
  - !!python/tuple
    - 1036
    - 1051
    - 1051
    - 1076
    - elaboration
  - !!python/tuple
    - 1034
    - 1076
    - 1076
    - 1116
    - elaboration
  - !!python/tuple
    - 1076
    - 1098
    - 1098
    - 1116
    - elaboration
  - !!python/tuple
    - 1098
    - 1105
    - 1105
    - 1116
    - purpose
  tokens:
  - Summary
  - ':'
  - This
  - paper
  - presents
  - a
  - way
  - to
  - combine
  - existing
  - factorized
  - second
  - order
  - representations
  - with
  - a
  - codebook
  - style
  - hard
  - assignment
  - .
  - The
  - number
  - of
  - parameters
  - required
  - to
  - produce
  - this
  - encoded
  - representation
  - is
  - shown
  - to
  - be
  - very
  - low
  - .
  - Like
  - other
  - factorized
  - representations
  - ','
  - the
  - number
  - of
  - computations
  - as
  - well
  - as
  - the
  - size
  - of
  - any
  - intermediate
  - representations
  - is
  - low
  - .
  - The
  - overall
  - embedding
  - is
  - trained
  - for
  - retrieval
  - using
  - a
  - triplet
  - loss
  - .
  - Results
  - are
  - shown
  - 'on'
  - Stanford
  - online
  - ','
  - CUB
  - and
  - Cars-196
  - datasets
  - .
  - Comments
  - ':'
  - Review
  - of
  - relevant
  - works
  - seems
  - adequate
  - .
  - The
  - results
  - seem
  - reproducible
  - .
  - The
  - only
  - contribution
  - of
  - this
  - paper
  - is
  - combining
  - the
  - factorized
  - second
  - order
  - representations
  - of
  - -LRB-
  - Kim
  - et
  - .
  - al.
  - '2017'
  - -RRB-
  - with
  - a
  - codebook
  - style
  - assignment
  - -LRB-
  - sec.
  - '3.2'
  - -RRB-
  - .
  - Seems
  - marginal
  - .
  - The
  - scheme
  - described
  - in
  - Sec.
  - '3.2'
  - needs
  - clarification
  - .
  - The
  - assignment
  - is
  - applied
  - to
  - x
  - as
  - h
  - -LRB-
  - x
  - -RRB-
  - \
  - kron
  - x
  - in
  - -LRB-
  - '7'
  - -RRB-
  - .
  - Then
  - the
  - entire
  - N
  - ^
  - '2'
  - D
  - ^
  - '2'
  - dimensional
  - second
  - order
  - descriptor
  - h
  - -LRB-
  - x
  - -RRB-
  - \
  - kron
  - x
  - \
  - kron
  - h
  - -LRB-
  - x
  - -RRB-
  - \
  - kron
  - x
  - is
  - projected
  - 'on'
  - a
  - N
  - ^
  - '2'
  - D
  - ^
  - '2'
  - dim
  - w_i
  - .
  - The
  - latter
  - is
  - factorized
  - into
  - p_i
  - ','
  - q_i
  - \
  - in
  - \
  - mathbb
  - -LCB-
  - R
  - -RCB-
  - ^
  - -LCB-
  - Nd
  - -RCB-
  - ','
  - which
  - are
  - further
  - factorized
  - into
  - codebook
  - specific
  - projections
  - u
  - _
  - -LCB-
  - i
  - ','
  - j
  - -RCB-
  - ','
  - v
  - _
  - -LCB-
  - i
  - ','
  - j
  - -RCB-
  - \
  - in
  - \
  - mathbb
  - -LCB-
  - R
  - -RCB-
  - ^
  - -LCB-
  - d
  - -RCB-
  - .
  - Is
  - this
  - different
  - from
  - classical
  - assignment
  - ','
  - where
  - x
  - is
  - hard
  - assigned
  - to
  - one
  - of
  - the
  - N
  - codewords
  - as
  - h
  - -LRB-
  - x
  - -RRB-
  - ','
  - then
  - projected
  - using
  - \
  - mathbb
  - -LCB-
  - R
  - -RCB-
  - ^
  - d
  - dimensional
  - p_i
  - ','
  - q_i
  - specific
  - to
  - that
  - codeword
  - '?'
  - In
  - section
  - '4.1'
  - and
  - Table
  - '2'
  - ','
  - is
  - the
  - HPBP
  - with
  - codebook
  - the
  - same
  - as
  - the
  - proposed
  - CHPBP
  - '?'
  - The
  - wording
  - in
  - '``'
  - Then
  - we
  - re-implement
  - '...'
  - naively
  - to
  - a
  - codebook
  - strategy
  - ''''''
  - seems
  - confusing
  - .
  - The
  - method
  - denoted
  - '``'
  - Margin
  - ''''''
  - in
  - Table
  - '4'
  - seems
  - to
  - be
  - better
  - than
  - the
  - proposed
  - approach
  - 'on'
  - CUB
  - .
  - How
  - does
  - it
  - compare
  - in
  - terms
  - of
  - efficiency
  - ','
  - memory/computation
  - '?'
  - Is
  - it
  - possible
  - to
  - see
  - any
  - classification
  - results
  - '?'
  - Most
  - of
  - the
  - relevant
  - second
  - order
  - embeddings
  - have
  - been
  - evaluated
  - in
  - that
  - setting
  - .
  - ===============
  - After
  - rebuttal
  - ===============================
  - After
  - reading
  - all
  - reviews
  - ','
  - considering
  - author
  - rebuttal
  - and
  - AC
  - inputs
  - ','
  - I
  - believe
  - my
  - initial
  - rating
  - is
  - a
  - bit
  - generous
  - .
  - I
  - would
  - like
  - to
  - downgrade
  - it
  - to
  - '4'
  - .
  - It
  - has
  - been
  - pointed
  - out
  - that
  - many
  - recent
  - works
  - that
  - are
  - of
  - a
  - similar
  - flavor
  - ','
  - published
  - in
  - CVPR
  - '2018'
  - and
  - ECCV
  - '2018'
  - ','
  - have
  - slightly
  - better
  - results
  - 'on'
  - the
  - same
  - dataset
  - .
  - Further
  - ','
  - the
  - only
  - novelty
  - of
  - this
  - work
  - is
  - the
  - proposed
  - factorization
  - and
  - not
  - the
  - encoding
  - scheme
  - .
  - This
  - alone
  - is
  - not
  - sufficient
  - to
  - merit
  - acceptance
  - .
  - We
  - would
  - like
  - to
  - thank
  - you
  - for
  - the
  - high-quality
  - review
  - .
  - We
  - clarify
  - our
  - contributions
  - and
  - the
  - pointed
  - formulation
  - .
  - Finally
  - ','
  - we
  - also
  - report
  - classification
  - results
  - .
  - Q
  - -RRB-
  - The
  - only
  - contribution
  - of
  - this
  - paper
  - '...'
  - Seems
  - marginal
  - .
  - A
  - -RRB-
  - We
  - are
  - convinced
  - by
  - the
  - novelty
  - of
  - this
  - paper
  - ','
  - as
  - the
  - few
  - methods
  - that
  - combined
  - second
  - order
  - representations
  - and
  - codebook
  - strategies
  - faced
  - the
  - very
  - high
  - dimensionality
  - of
  - the
  - representation
  - without
  - proposing
  - compact
  - factorization
  - schemes
  - -LRB-
  - as
  - shown
  - in
  - Table
  - '1'
  - -RRB-
  - .
  - E.g.
  - MFAFVNet
  - -LRB-
  - Li
  - et
  - al.
  - ','
  - 2017b
  - -RRB-
  - extend
  - this
  - second
  - order
  - pooling
  - to
  - codebook
  - with
  - factorization
  - scheme
  - .
  - However
  - ','
  - even
  - with
  - such
  - factorization
  - ','
  - their
  - proposed
  - method
  - leads
  - to
  - 500k
  - dimension
  - representations
  - -LRB-
  - i.e.
  - 2x
  - Bilinear
  - Pooling
  - -RRB-
  - ','
  - 27M
  - parameters
  - and
  - around
  - 40GOps
  - to
  - compute
  - the
  - output
  - representation
  - .
  - JCF
  - provides
  - 512d
  - representation
  - -LRB-
  - 1/1000
  - factor
  - -RRB-
  - ','
  - 4M
  - parameters
  - -LRB-
  - 1/7
  - -RRB-
  - and
  - 3GOps
  - -LRB-
  - 1/10
  - -RRB-
  - with
  - the
  - same
  - performances
  - as
  - well-known
  - second
  - order
  - factorization
  - scheme
  - .
  - To
  - sum
  - up
  - ':'
  - Combining
  - second
  - order
  - representation
  - with
  - a
  - codebook
  - is
  - not
  - trivial
  - due
  - to
  - computation
  - concerns
  - .
  - We
  - are
  - the
  - first
  - to
  - provide
  - an
  - efficient
  - scheme
  - for
  - such
  - combination
  - .
  - Q
  - -RRB-
  - Is
  - this
  - different
  - from
  - classical
  - assignment
  - '?'
  - A
  - -RRB-
  - Without
  - the
  - introduction
  - of
  - rank
  - factorization
  - ','
  - there
  - is
  - 'no'
  - difference
  - ':'
  - the
  - two
  - projections
  - u
  - _
  - -LCB-
  - i
  - ','
  - j
  - -RCB-
  - and
  - v
  - _
  - -LCB-
  - i
  - ','
  - j
  - -RCB-
  - play
  - a
  - similar
  - roles
  - as
  - intra-projection
  - in
  - VLAD
  - representation
  - for
  - example
  - .
  - This
  - insight
  - is
  - developed
  - in
  - section
  - '3.2'
  - between
  - equations
  - -LRB-
  - '9'
  - -RRB-
  - and
  - -LRB-
  - '10'
  - -RRB-
  - .
  - However
  - this
  - approach
  - is
  - not
  - tractable
  - ','
  - which
  - is
  - the
  - reason
  - we
  - proposed
  - our
  - joint
  - factorization
  - and
  - codebook
  - method
  - .
  - Moreover
  - ','
  - to
  - make
  - the
  - method
  - end-to-end
  - trainable
  - ','
  - x
  - is
  - soft-assigned
  - to
  - the
  - codebook
  - instead
  - of
  - the
  - hard-assignment
  - of
  - VLAD
  - .
  - Q
  - -RRB-
  - The
  - wording
  - in
  - '``'
  - Then
  - we
  - re-implement
  - '...'
  - naively
  - to
  - a
  - codebook
  - strategy
  - ''''''
  - seems
  - confusing
  - .
  - A
  - -RRB-
  - Thanks
  - to
  - point
  - out
  - this
  - ambiguity
  - ','
  - we
  - update
  - this
  - sentence
  - in
  - the
  - new
  - paper
  - version
  - also
  - following
  - '``'
  - Reviewer3
  - ''''''
  - remark
  - 'on'
  - the
  - terms
  - used
  - .
  - For
  - clarity
  - ','
  - we
  - implement
  - Bilinear
  - Pooling
  - -LRB-
  - BP
  - -RRB-
  - and
  - Compact
  - Bilinear
  - Pooling
  - -LRB-
  - CBP
  - -RRB-
  - and
  - not
  - HPBP
  - as
  - we
  - do
  - not
  - add
  - the
  - non-linearity
  - nor
  - the
  - projection
  - .
  - However
  - ','
  - we
  - also
  - train
  - the
  - projection
  - matrices
  - for
  - CBP
  - .
  - Then
  - ','
  - we
  - extend
  - these
  - two
  - methods
  - naively
  - to
  - codebook
  - strategy
  - ','
  - that
  - is
  - by
  - computing
  - ':'
  - '-'
  - W
  - ^
  - T
  - -LSB-
  - h
  - -LRB-
  - x
  - -RRB-
  - \
  - kron
  - x
  - \
  - kron
  - x
  - -RSB-
  - where
  - W
  - \
  - in
  - \
  - mathbb
  - -LCB-
  - R
  - -RCB-
  - ^
  - -LCB-
  - Nd
  - ^
  - '2'
  - \
  - times
  - D
  - -RCB-
  - -LRB-
  - named
  - C-BP
  - -RRB-
  - '-'
  - CBP
  - extended
  - to
  - codebook
  - -LRB-
  - named
  - C-CBP
  - -RRB-
  - ','
  - using
  - equation
  - -LRB-
  - '10'
  - -RRB-
  - .
  - Q
  - -RRB-
  - '``'
  - Margin
  - ''''''
  - in
  - Table
  - '4'
  - performs
  - better
  - 'on'
  - CUB
  - ','
  - how
  - does
  - it
  - compare
  - '?'
  - A
  - -RRB-
  - The
  - '``'
  - Margin
  - ''''''
  - method
  - proposed
  - a
  - new
  - sampling
  - strategy
  - that
  - allows
  - to
  - explore
  - much
  - more
  - informative
  - triplet
  - than
  - standard
  - strategies
  - ','
  - including
  - ours
  - .
  - Also
  - ','
  - they
  - use
  - larger
  - images
  - .
  - Note
  - that
  - we
  - can
  - exploit
  - their
  - sampling
  - method
  - to
  - improve
  - our
  - training
  - procedure
  - .
  - Q
  - -RRB-
  - Is
  - it
  - possible
  - to
  - see
  - any
  - classification
  - results
  - '?'
  - A
  - -RRB-
  - As
  - mentioned
  - in
  - the
  - general
  - comment
  - ','
  - we
  - added
  - results
  - 'on'
  - '3'
  - fine-grained
  - visual
  - classification
  - -LRB-
  - FGVC
  - -RRB-
  - datasets
  - ','
  - CUB
  - and
  - CARS
  - using
  - the
  - standard
  - split
  - in
  - FGVC
  - and
  - Aircraft
  - which
  - are
  - '3'
  - common
  - datasets
  - for
  - FGVC
  - task
  - .
  - We
  - compare
  - our
  - method
  - to
  - other
  - factorization
  - scheme
  - and
  - we
  - report
  - comparable
  - results
  - to
  - the
  - state-of-the-art
  - with
  - much
  - more
  - compact
  - representation
  - .
  - For
  - more
  - details
  - ','
  - we
  - invite
  - you
  - to
  - read
  - the
  - general
  - comment
  - and
  - the
  - new
  - paper
  - version
  - .
- comment_id: B1eNZMVcnX
  rels:
  - !!python/tuple
    - 0
    - 2
    - 2
    - 53
    - elaboration
  - !!python/tuple
    - 2
    - 5
    - 5
    - 53
    - elaboration
  - !!python/tuple
    - 5
    - 20
    - 20
    - 53
    - elaboration
  - !!python/tuple
    - 25
    - 53
    - 20
    - 25
    - attribution
  - !!python/tuple
    - 25
    - 35
    - 35
    - 53
    - same_unit
  - !!python/tuple
    - 25
    - 27
    - 27
    - 35
    - purpose
  - !!python/tuple
    - 35
    - 53
    - 25
    - 35
    - same_unit
  - !!python/tuple
    - 44
    - 53
    - 35
    - 44
    - attribution
  - !!python/tuple
    - 44
    - 49
    - 49
    - 53
    - elaboration
  - !!python/tuple
    - 0
    - 53
    - 53
    - 91
    - elaboration
  - !!python/tuple
    - 54
    - 91
    - 53
    - 54
    - attribution
  - !!python/tuple
    - 54
    - 56
    - 56
    - 91
    - elaboration
  - !!python/tuple
    - 68
    - 91
    - 56
    - 68
    - attribution
  - !!python/tuple
    - 68
    - 76
    - 76
    - 91
    - purpose
  - !!python/tuple
    - 76
    - 84
    - 84
    - 91
    - purpose
  - !!python/tuple
    - 0
    - 91
    - 91
    - 177
    - elaboration
  - !!python/tuple
    - 91
    - 92
    - 92
    - 177
    - elaboration
  - !!python/tuple
    - 100
    - 130
    - 92
    - 100
    - attribution
  - !!python/tuple
    - 100
    - 126
    - 126
    - 130
    - elaboration
  - !!python/tuple
    - 92
    - 130
    - 130
    - 163
    - elaboration
  - !!python/tuple
    - 130
    - 135
    - 135
    - 163
    - elaboration
  - !!python/tuple
    - 135
    - 139
    - 139
    - 163
    - elaboration
  - !!python/tuple
    - 139
    - 146
    - 146
    - 163
    - elaboration
  - !!python/tuple
    - 148
    - 163
    - 146
    - 148
    - attribution
  - !!python/tuple
    - 148
    - 150
    - 150
    - 163
    - purpose
  - !!python/tuple
    - 92
    - 163
    - 163
    - 177
    - elaboration
  - !!python/tuple
    - 163
    - 165
    - 165
    - 177
    - elaboration
  - !!python/tuple
    - 165
    - 170
    - 170
    - 177
    - elaboration
  tokens:
  - Hi
  - '!'
  - Interesting
  - work
  - '!'
  - This
  - question
  - is
  - more
  - 'on'
  - the
  - particular
  - writing
  - style
  - and
  - not
  - 'on'
  - the
  - method
  - .
  - About
  - the
  - claim
  - of
  - being
  - '``'
  - able
  - to
  - discover
  - both
  - convolutional
  - and
  - recurrent
  - networks
  - ''''''
  - as
  - mentioned
  - in
  - text
  - ','
  - I
  - do
  - n't
  - think
  - it
  - '''s'
  - an
  - accurate
  - way
  - to
  - remark
  - that
  - .
  - Given
  - that
  - here
  - you
  - search
  - for
  - a
  - computation
  - cell
  - ','
  - and
  - quoting
  - from
  - section
  - '2.1'
  - '``'
  - The
  - learned
  - cell
  - could
  - either
  - be
  - stacked
  - to
  - form
  - a
  - convolutional
  - network
  - or
  - recursively
  - connected
  - to
  - form
  - a
  - recurrent
  - network
  - .
  - ''''''
  - .
  - In
  - my
  - opinion
  - ','
  - this
  - does
  - n't
  - imply
  - that
  - the
  - method
  - discovered
  - recurrence
  - or
  - convolutional
  - architecture
  - ','
  - but
  - instead
  - it
  - was
  - explicitly
  - done
  - by
  - stacking
  - cells
  - in
  - a
  - recurrent
  - manner
  - or
  - providing
  - a
  - convolution
  - as
  - candidate
  - operation
  - .
  - I
  - would
  - request
  - the
  - authors
  - to
  - reconsider
  - their
  - way
  - of
  - writing
  - this
  - and
  - maybe
  - say
  - something
  - like
  - ','
  - '``'
  - able
  - to
  - discover
  - effective
  - cells
  - for
  - use
  - in
  - convolutional
  - and
  - recurrent
  - networks
  - ''''''
  - .
  - Thanks
  - '!'
  - Thanks
  - for
  - the
  - suggestion
  - .
  - We
  - will
  - revise
  - our
  - writing
  - accordingly
  - .
- comment_id: B1eQB3w2lV
  rels:
  - !!python/tuple
    - 0
    - 499
    - 499
    - 508
    - topic
  - !!python/tuple
    - 0
    - 15
    - 15
    - 27
    - same_unit
  - !!python/tuple
    - 0
    - 9
    - 9
    - 15
    - elaboration
  - !!python/tuple
    - 15
    - 27
    - 0
    - 15
    - same_unit
  - !!python/tuple
    - 15
    - 23
    - 23
    - 27
    - list
  - !!python/tuple
    - 23
    - 27
    - 15
    - 23
    - list
  - !!python/tuple
    - 0
    - 27
    - 27
    - 499
    - elaboration
  - !!python/tuple
    - 27
    - 38
    - 38
    - 45
    - elaboration
  - !!python/tuple
    - 27
    - 45
    - 45
    - 63
    - elaboration
  - !!python/tuple
    - 27
    - 63
    - 63
    - 133
    - elaboration
  - !!python/tuple
    - 63
    - 64
    - 64
    - 78
    - elaboration
  - !!python/tuple
    - 63
    - 78
    - 78
    - 133
    - elaboration
  - !!python/tuple
    - 78
    - 83
    - 83
    - 88
    - elaboration
  - !!python/tuple
    - 78
    - 88
    - 88
    - 97
    - elaboration
  - !!python/tuple
    - 78
    - 97
    - 97
    - 133
    - elaboration
  - !!python/tuple
    - 97
    - 104
    - 104
    - 117
    - elaboration
  - !!python/tuple
    - 104
    - 106
    - 106
    - 117
    - list
  - !!python/tuple
    - 106
    - 117
    - 104
    - 106
    - list
  - !!python/tuple
    - 106
    - 107
    - 107
    - 117
    - elaboration
  - !!python/tuple
    - 97
    - 117
    - 117
    - 133
    - elaboration
  - !!python/tuple
    - 117
    - 121
    - 121
    - 133
    - purpose
  - !!python/tuple
    - 27
    - 133
    - 133
    - 499
    - elaboration
  - !!python/tuple
    - 133
    - 141
    - 141
    - 150
    - purpose
  - !!python/tuple
    - 133
    - 150
    - 150
    - 499
    - elaboration
  - !!python/tuple
    - 150
    - 155
    - 155
    - 162
    - purpose
  - !!python/tuple
    - 150
    - 162
    - 162
    - 171
    - elaboration
  - !!python/tuple
    - 150
    - 171
    - 171
    - 499
    - elaboration
  - !!python/tuple
    - 171
    - 184
    - 184
    - 499
    - elaboration
  - !!python/tuple
    - 220
    - 499
    - 184
    - 220
    - circumstance
  - !!python/tuple
    - 184
    - 189
    - 189
    - 220
    - list
  - !!python/tuple
    - 189
    - 220
    - 184
    - 189
    - list
  - !!python/tuple
    - 189
    - 191
    - 191
    - 220
    - purpose
  - !!python/tuple
    - 191
    - 206
    - 206
    - 220
    - elaboration
  - !!python/tuple
    - 206
    - 210
    - 210
    - 220
    - elaboration
  - !!python/tuple
    - 220
    - 230
    - 230
    - 239
    - same_unit
  - !!python/tuple
    - 220
    - 225
    - 225
    - 230
    - elaboration
  - !!python/tuple
    - 230
    - 239
    - 220
    - 230
    - same_unit
  - !!python/tuple
    - 220
    - 239
    - 239
    - 499
    - elaboration
  - !!python/tuple
    - 239
    - 252
    - 252
    - 274
    - elaboration
  - !!python/tuple
    - 252
    - 253
    - 253
    - 274
    - elaboration
  - !!python/tuple
    - 253
    - 257
    - 257
    - 274
    - elaboration
  - !!python/tuple
    - 239
    - 274
    - 274
    - 499
    - elaboration
  - !!python/tuple
    - 274
    - 281
    - 281
    - 298
    - elaboration
  - !!python/tuple
    - 281
    - 287
    - 287
    - 298
    - elaboration
  - !!python/tuple
    - 274
    - 298
    - 298
    - 499
    - elaboration
  - !!python/tuple
    - 298
    - 300
    - 300
    - 344
    - same_unit
  - !!python/tuple
    - 300
    - 344
    - 298
    - 300
    - same_unit
  - !!python/tuple
    - 300
    - 313
    - 313
    - 329
    - elaboration
  - !!python/tuple
    - 300
    - 329
    - 329
    - 344
    - manner
  - !!python/tuple
    - 298
    - 344
    - 344
    - 499
    - example
  - !!python/tuple
    - 344
    - 360
    - 360
    - 365
    - elaboration
  - !!python/tuple
    - 344
    - 365
    - 365
    - 499
    - elaboration
  - !!python/tuple
    - 365
    - 382
    - 382
    - 499
    - elaboration
  - !!python/tuple
    - 389
    - 410
    - 382
    - 389
    - attribution
  - !!python/tuple
    - 389
    - 401
    - 401
    - 410
    - elaboration
  - !!python/tuple
    - 382
    - 410
    - 410
    - 499
    - elaboration
  - !!python/tuple
    - 410
    - 442
    - 442
    - 499
    - elaboration
  - !!python/tuple
    - 442
    - 451
    - 451
    - 467
    - elaboration
  - !!python/tuple
    - 453
    - 467
    - 451
    - 453
    - attribution
  - !!python/tuple
    - 453
    - 460
    - 460
    - 467
    - elaboration
  - !!python/tuple
    - 442
    - 467
    - 467
    - 469
    - elaboration
  - !!python/tuple
    - 442
    - 469
    - 469
    - 487
    - elaboration
  - !!python/tuple
    - 442
    - 487
    - 487
    - 499
    - elaboration
  - !!python/tuple
    - 499
    - 508
    - 0
    - 499
    - topic
  - !!python/tuple
    - 499
    - 503
    - 503
    - 508
    - elaboration
  - !!python/tuple
    - 503
    - 504
    - 504
    - 508
    - elaboration
  tokens:
  - This
  - work
  - does
  - not
  - cite
  - or
  - compare
  - with
  - work
  - that
  - appeared
  - almost
  - '6'
  - months
  - ago
  - 'on'
  - attacking
  - object
  - detectors
  - in
  - the
  - physical
  - world
  - and
  - showing
  - transferability
  - .
  - The
  - work
  - of
  - Eykholt
  - et
  - al
  - show
  - a
  - similar
  - camouflage
  - attack
  - in
  - making
  - a
  - stop
  - sign
  - disappear
  - .
  - curious
  - about
  - the
  - differences
  - to
  - this
  - paper
  - ','
  - and
  - what
  - intellectual
  - contributions
  - it
  - provides
  - over
  - existing
  - work
  - .
  - Eykholt
  - et
  - al.
  - ','
  - Physical
  - Adversarial
  - Examples
  - for
  - Object
  - Detectors
  - ','
  - USENIX
  - WOOT
  - '2018'
  - .
  - https://www.usenix.org/conference/woot18/presentation/eykholt
  - Thanks
  - for
  - the
  - pointer
  - to
  - -LSB-
  - '1'
  - -RSB-
  - .
  - We
  - will
  - add
  - it
  - to
  - our
  - final
  - version
  - .
  - We
  - did
  - have
  - cited
  - a
  - similar
  - paper
  - -LSB-
  - '2'
  - -RSB-
  - which
  - also
  - aims
  - to
  - physically
  - perturb
  - the
  - stop-sign
  - detectors
  - .
  - We
  - will
  - be
  - glad
  - to
  - discuss
  - the
  - differences
  - between
  - our
  - work
  - and
  - -LSB-
  - 1,2
  - -RSB-
  - .
  - The
  - discussion
  - will
  - also
  - facilitate
  - the
  - other
  - readers
  - to
  - understand
  - the
  - motivation
  - of
  - our
  - paper
  - better
  - .
  - Good
  - physical
  - camouflages
  - are
  - supposed
  - to
  - fail
  - object
  - detectors
  - for
  - any
  - images
  - taken
  - about
  - the
  - camouflaged
  - object
  - under
  - all
  - conditions
  - ':'
  - object-to-camera
  - distance
  - ','
  - background
  - ','
  - lighting
  - condition
  - ','
  - view
  - angle
  - ','
  - etc.
  - .
  - When
  - we
  - formalize
  - the
  - problem
  - and
  - try
  - to
  - optimize
  - with
  - respect
  - to
  - the
  - camouflage
  - ','
  - however
  - ','
  - the
  - '``'
  - imaging
  - function
  - ''''''
  - which
  - transforms
  - the
  - camouflage
  - to
  - camouflaged
  - object
  - and
  - eventually
  - various
  - images
  - is
  - unknown
  - .
  - Hence
  - ','
  - the
  - key
  - challenge
  - to
  - learning
  - the
  - physical
  - camouflage
  - is
  - how
  - to
  - tackle
  - this
  - unknown
  - imaging
  - function
  - .
  - Both
  - -LSB-
  - '1'
  - -RSB-
  - and
  - -LSB-
  - '2'
  - -RSB-
  - perturb
  - the
  - detectors
  - of
  - stop
  - signs
  - which
  - are
  - planar
  - objects
  - whose
  - images
  - ','
  - under
  - changes
  - in
  - camera
  - geometry
  - ','
  - are
  - related
  - by
  - linear
  - 2D
  - projective
  - transformations
  - .
  - This
  - is
  - in
  - contrast
  - to
  - non-planar
  - objects
  - -LRB-
  - e.g.
  - ','
  - a
  - car
  - -RRB-
  - whose
  - images
  - are
  - related
  - by
  - more
  - complex
  - range-dependent
  - nonlinear
  - transformations
  - .
  - Hence
  - ','
  - -LSB-
  - 1,2
  - -RSB-
  - are
  - able
  - to
  - simplify
  - such
  - imaging
  - function
  - to
  - projective
  - transformations
  - -LRB-
  - cf.
  - Section
  - 4.2.2
  - in
  - -LSB-
  - '1'
  - -RSB-
  - and
  - Section
  - '4.1'
  - in
  - -LSB-
  - '2'
  - -RSB-
  - -RRB-
  - without
  - breaking
  - the
  - gradient
  - chain
  - between
  - the
  - perturbation
  - and
  - the
  - detector
  - ''''
  - output
  - score
  - .
  - Complex
  - nonlinear
  - transformations
  - ','
  - however
  - ','
  - require
  - a
  - dedicated
  - 3D
  - simulation
  - ','
  - for
  - instance
  - the
  - one
  - used
  - in
  - our
  - paper
  - .
  - The
  - 3D
  - simulation
  - breaks
  - the
  - gradient
  - chain
  - and
  - turns
  - the
  - problem
  - into
  - a
  - black-box
  - optimization
  - problem
  - .
  - In
  - short
  - ','
  - the
  - non-planar
  - objects
  - break
  - -LSB-
  - 1,2
  - -RSB-
  - '''s'
  - premise
  - since
  - the
  - approaches
  - therein
  - rely
  - 'on'
  - functions
  - that
  - are
  - differentiable
  - with
  - respect
  - to
  - the
  - camouflage
  - .
  - -LSB-
  - '1'
  - -RSB-
  - Song
  - ','
  - Dawn
  - ','
  - Kevin
  - Eykholt
  - ','
  - Ivan
  - Evtimov
  - ','
  - Earlence
  - Fernandes
  - ','
  - Bo
  - Li
  - ','
  - Amir
  - Rahmati
  - ','
  - Florian
  - Tramer
  - ','
  - Atul
  - Prakash
  - ','
  - and
  - Tadayoshi
  - Kohno
  - .
  - '``'
  - Physical
  - adversarial
  - examples
  - for
  - object
  - detectors
  - .
  - ''''''
  - In
  - 12th
  - -LCB-
  - USENIX
  - -RCB-
  - Workshop
  - 'on'
  - Offensive
  - Technologies
  - -LRB-
  - -LCB-
  - WOOT
  - -RCB-
  - '18'
  - -RRB-
  - .
  - '2018'
  - .
  - -LSB-
  - '2'
  - -RSB-
  - Chen
  - ','
  - Shang-Tse
  - ','
  - Cory
  - Cornelius
  - ','
  - Jason
  - Martin
  - ','
  - and
  - Duen
  - Horng
  - Chau
  - .
  - '``'
  - Robust
  - Physical
  - Adversarial
  - Attack
  - 'on'
  - Faster
  - R-CNN
  - Object
  - Detector
  - .
  - ''''''
  - arXiv
  - preprint
  - arXiv
  - ':'
  - '1804.05810'
  - -LRB-
  - '2018'
  - -RRB-
  - .
- comment_id: B1et7uIGyV
  rels:
  - !!python/tuple
    - 0
    - 3
    - 3
    - 14
    - purpose
  - !!python/tuple
    - 0
    - 14
    - 14
    - 1631
    - elaboration
  - !!python/tuple
    - 14
    - 22
    - 22
    - 47
    - elaboration
  - !!python/tuple
    - 22
    - 42
    - 42
    - 47
    - same_unit
  - !!python/tuple
    - 22
    - 23
    - 23
    - 31
    - elaboration
  - !!python/tuple
    - 22
    - 31
    - 31
    - 42
    - purpose
  - !!python/tuple
    - 31
    - 37
    - 37
    - 42
    - elaboration
  - !!python/tuple
    - 42
    - 47
    - 22
    - 42
    - same_unit
  - !!python/tuple
    - 14
    - 47
    - 47
    - 1631
    - elaboration
  - !!python/tuple
    - 54
    - 81
    - 47
    - 54
    - concession
  - !!python/tuple
    - 54
    - 70
    - 70
    - 81
    - elaboration
  - !!python/tuple
    - 70
    - 71
    - 71
    - 81
    - elaboration
  - !!python/tuple
    - 47
    - 81
    - 81
    - 1631
    - elaboration
  - !!python/tuple
    - 81
    - 86
    - 86
    - 104
    - list
  - !!python/tuple
    - 81
    - 84
    - 84
    - 86
    - elaboration
  - !!python/tuple
    - 86
    - 104
    - 81
    - 86
    - list
  - !!python/tuple
    - 86
    - 98
    - 98
    - 104
    - same_unit
  - !!python/tuple
    - 86
    - 97
    - 97
    - 98
    - same_unit
  - !!python/tuple
    - 86
    - 88
    - 88
    - 97
    - elaboration
  - !!python/tuple
    - 88
    - 89
    - 89
    - 93
    - purpose
  - !!python/tuple
    - 88
    - 93
    - 93
    - 97
    - elaboration
  - !!python/tuple
    - 97
    - 98
    - 86
    - 97
    - same_unit
  - !!python/tuple
    - 98
    - 104
    - 86
    - 98
    - same_unit
  - !!python/tuple
    - 81
    - 104
    - 104
    - 1631
    - elaboration
  - !!python/tuple
    - 104
    - 125
    - 125
    - 1631
    - list
  - !!python/tuple
    - 104
    - 119
    - 119
    - 125
    - elaboration
  - !!python/tuple
    - 125
    - 1631
    - 104
    - 125
    - list
  - !!python/tuple
    - 125
    - 127
    - 127
    - 1631
    - list
  - !!python/tuple
    - 127
    - 1631
    - 125
    - 127
    - list
  - !!python/tuple
    - 127
    - 145
    - 145
    - 1631
    - elaboration
  - !!python/tuple
    - 145
    - 158
    - 158
    - 1631
    - list
  - !!python/tuple
    - 145
    - 153
    - 153
    - 158
    - elaboration
  - !!python/tuple
    - 158
    - 1631
    - 145
    - 158
    - list
  - !!python/tuple
    - 158
    - 196
    - 196
    - 1631
    - list
  - !!python/tuple
    - 158
    - 160
    - 160
    - 196
    - elaboration
  - !!python/tuple
    - 160
    - 168
    - 168
    - 196
    - elaboration
  - !!python/tuple
    - 168
    - 175
    - 175
    - 196
    - elaboration
  - !!python/tuple
    - 175
    - 187
    - 187
    - 196
    - same_unit
  - !!python/tuple
    - 175
    - 176
    - 176
    - 187
    - elaboration
  - !!python/tuple
    - 187
    - 196
    - 175
    - 187
    - same_unit
  - !!python/tuple
    - 187
    - 188
    - 188
    - 195
    - elaboration
  - !!python/tuple
    - 187
    - 195
    - 195
    - 196
    - elaboration
  - !!python/tuple
    - 196
    - 1631
    - 158
    - 196
    - list
  - !!python/tuple
    - 196
    - 327
    - 327
    - 1631
    - list
  - !!python/tuple
    - 196
    - 202
    - 202
    - 218
    - list
  - !!python/tuple
    - 202
    - 218
    - 196
    - 202
    - list
  - !!python/tuple
    - 202
    - 212
    - 212
    - 218
    - elaboration
  - !!python/tuple
    - 196
    - 218
    - 218
    - 327
    - condition
  - !!python/tuple
    - 232
    - 327
    - 218
    - 232
    - condition
  - !!python/tuple
    - 226
    - 232
    - 218
    - 226
    - condition
  - !!python/tuple
    - 232
    - 244
    - 244
    - 327
    - list
  - !!python/tuple
    - 244
    - 327
    - 232
    - 244
    - list
  - !!python/tuple
    - 244
    - 249
    - 249
    - 272
    - purpose
  - !!python/tuple
    - 249
    - 257
    - 257
    - 272
    - circumstance
  - !!python/tuple
    - 257
    - 262
    - 262
    - 272
    - purpose
  - !!python/tuple
    - 244
    - 272
    - 272
    - 327
    - elaboration
  - !!python/tuple
    - 272
    - 276
    - 276
    - 292
    - elaboration
  - !!python/tuple
    - 272
    - 292
    - 292
    - 327
    - example
  - !!python/tuple
    - 292
    - 298
    - 298
    - 327
    - purpose
  - !!python/tuple
    - 298
    - 301
    - 301
    - 327
    - elaboration
  - !!python/tuple
    - 301
    - 316
    - 316
    - 327
    - contrast
  - !!python/tuple
    - 301
    - 312
    - 312
    - 316
    - elaboration
  - !!python/tuple
    - 316
    - 327
    - 301
    - 316
    - contrast
  - !!python/tuple
    - 316
    - 325
    - 325
    - 327
    - elaboration
  - !!python/tuple
    - 327
    - 1631
    - 196
    - 327
    - list
  - !!python/tuple
    - 327
    - 431
    - 431
    - 1631
    - topic
  - !!python/tuple
    - 327
    - 337
    - 337
    - 431
    - elaboration
  - !!python/tuple
    - 346
    - 357
    - 337
    - 346
    - attribution
  - !!python/tuple
    - 337
    - 357
    - 357
    - 431
    - elaboration
  - !!python/tuple
    - 357
    - 371
    - 371
    - 382
    - list
  - !!python/tuple
    - 371
    - 382
    - 357
    - 371
    - list
  - !!python/tuple
    - 357
    - 382
    - 382
    - 431
    - elaboration
  - !!python/tuple
    - 382
    - 384
    - 384
    - 431
    - elaboration
  - !!python/tuple
    - 384
    - 387
    - 387
    - 393
    - elaboration
  - !!python/tuple
    - 384
    - 393
    - 393
    - 431
    - example
  - !!python/tuple
    - 393
    - 407
    - 407
    - 431
    - same_unit
  - !!python/tuple
    - 393
    - 403
    - 403
    - 407
    - elaboration
  - !!python/tuple
    - 407
    - 431
    - 393
    - 407
    - same_unit
  - !!python/tuple
    - 407
    - 416
    - 416
    - 431
    - contrast
  - !!python/tuple
    - 416
    - 431
    - 407
    - 416
    - contrast
  - !!python/tuple
    - 431
    - 1631
    - 327
    - 431
    - topic
  - !!python/tuple
    - 431
    - 438
    - 438
    - 1631
    - list
  - !!python/tuple
    - 438
    - 1631
    - 431
    - 438
    - list
  - !!python/tuple
    - 438
    - 453
    - 453
    - 1631
    - list
  - !!python/tuple
    - 453
    - 1631
    - 438
    - 453
    - list
  - !!python/tuple
    - 453
    - 455
    - 455
    - 1631
    - list
  - !!python/tuple
    - 455
    - 1631
    - 453
    - 455
    - list
  - !!python/tuple
    - 455
    - 458
    - 458
    - 1631
    - textualorganization
  - !!python/tuple
    - 458
    - 1631
    - 455
    - 458
    - textualorganization
  - !!python/tuple
    - 458
    - 469
    - 469
    - 1631
    - list
  - !!python/tuple
    - 469
    - 1631
    - 458
    - 469
    - list
  - !!python/tuple
    - 469
    - 477
    - 477
    - 1631
    - list
  - !!python/tuple
    - 470
    - 477
    - 469
    - 470
    - attribution
  - !!python/tuple
    - 477
    - 1631
    - 469
    - 477
    - list
  - !!python/tuple
    - 477
    - 497
    - 497
    - 1631
    - list
  - !!python/tuple
    - 477
    - 479
    - 479
    - 497
    - elaboration
  - !!python/tuple
    - 479
    - 492
    - 492
    - 497
    - elaboration
  - !!python/tuple
    - 497
    - 1631
    - 477
    - 497
    - list
  - !!python/tuple
    - 497
    - 502
    - 502
    - 1631
    - list
  - !!python/tuple
    - 502
    - 1631
    - 497
    - 502
    - list
  - !!python/tuple
    - 502
    - 507
    - 507
    - 1631
    - list
  - !!python/tuple
    - 507
    - 1631
    - 502
    - 507
    - list
  - !!python/tuple
    - 507
    - 524
    - 524
    - 1631
    - list
  - !!python/tuple
    - 507
    - 510
    - 510
    - 524
    - elaboration
  - !!python/tuple
    - 510
    - 519
    - 519
    - 524
    - means
  - !!python/tuple
    - 524
    - 1631
    - 507
    - 524
    - list
  - !!python/tuple
    - 524
    - 591
    - 591
    - 1631
    - topic
  - !!python/tuple
    - 524
    - 530
    - 530
    - 591
    - elaboration
  - !!python/tuple
    - 530
    - 558
    - 558
    - 591
    - same_unit
  - !!python/tuple
    - 530
    - 546
    - 546
    - 558
    - elaboration
  - !!python/tuple
    - 558
    - 591
    - 530
    - 558
    - same_unit
  - !!python/tuple
    - 576
    - 591
    - 558
    - 576
    - attribution
  - !!python/tuple
    - 576
    - 585
    - 585
    - 591
    - elaboration
  - !!python/tuple
    - 591
    - 1631
    - 524
    - 591
    - topic
  - !!python/tuple
    - 591
    - 592
    - 592
    - 1631
    - list
  - !!python/tuple
    - 592
    - 1631
    - 591
    - 592
    - list
  - !!python/tuple
    - 592
    - 595
    - 595
    - 1631
    - list
  - !!python/tuple
    - 595
    - 1631
    - 592
    - 595
    - list
  - !!python/tuple
    - 595
    - 600
    - 600
    - 637
    - elaboration
  - !!python/tuple
    - 600
    - 623
    - 623
    - 636
    - elaboration
  - !!python/tuple
    - 600
    - 636
    - 636
    - 637
    - elaboration
  - !!python/tuple
    - 595
    - 637
    - 637
    - 1631
    - elaboration
  - !!python/tuple
    - 637
    - 648
    - 648
    - 658
    - circumstance
  - !!python/tuple
    - 637
    - 658
    - 658
    - 1631
    - condition
  - !!python/tuple
    - 658
    - 667
    - 667
    - 685
    - elaboration
  - !!python/tuple
    - 667
    - 677
    - 677
    - 685
    - elaboration
  - !!python/tuple
    - 658
    - 685
    - 685
    - 1631
    - elaboration
  - !!python/tuple
    - 685
    - 697
    - 697
    - 1631
    - list
  - !!python/tuple
    - 687
    - 697
    - 685
    - 687
    - attribution
  - !!python/tuple
    - 697
    - 1631
    - 685
    - 697
    - list
  - !!python/tuple
    - 697
    - 704
    - 704
    - 723
    - elaboration
  - !!python/tuple
    - 704
    - 713
    - 713
    - 723
    - purpose
  - !!python/tuple
    - 697
    - 723
    - 723
    - 1631
    - elaboration
  - !!python/tuple
    - 723
    - 725
    - 725
    - 1631
    - list
  - !!python/tuple
    - 725
    - 1631
    - 723
    - 725
    - list
  - !!python/tuple
    - 725
    - 727
    - 727
    - 733
    - elaboration
  - !!python/tuple
    - 727
    - 728
    - 728
    - 732
    - purpose
  - !!python/tuple
    - 727
    - 732
    - 732
    - 733
    - restatement
  - !!python/tuple
    - 725
    - 733
    - 733
    - 769
    - elaboration
  - !!python/tuple
    - 733
    - 747
    - 747
    - 752
    - same_unit
  - !!python/tuple
    - 733
    - 735
    - 735
    - 747
    - elaboration
  - !!python/tuple
    - 747
    - 752
    - 733
    - 747
    - same_unit
  - !!python/tuple
    - 733
    - 752
    - 752
    - 769
    - elaboration
  - !!python/tuple
    - 725
    - 769
    - 769
    - 1631
    - elaboration
  - !!python/tuple
    - 769
    - 791
    - 791
    - 822
    - elaboration
  - !!python/tuple
    - 797
    - 814
    - 791
    - 797
    - attribution
  - !!python/tuple
    - 797
    - 810
    - 810
    - 814
    - elaboration
  - !!python/tuple
    - 791
    - 814
    - 814
    - 822
    - elaboration
  - !!python/tuple
    - 814
    - 816
    - 816
    - 822
    - elaboration
  - !!python/tuple
    - 769
    - 822
    - 822
    - 1631
    - elaboration
  - !!python/tuple
    - 822
    - 1002
    - 1002
    - 1631
    - list
  - !!python/tuple
    - 822
    - 955
    - 955
    - 1002
    - list
  - !!python/tuple
    - 822
    - 844
    - 844
    - 866
    - list
  - !!python/tuple
    - 822
    - 825
    - 825
    - 844
    - elaboration
  - !!python/tuple
    - 844
    - 866
    - 822
    - 844
    - list
  - !!python/tuple
    - 844
    - 847
    - 847
    - 866
    - reason
  - !!python/tuple
    - 847
    - 852
    - 852
    - 866
    - elaboration
  - !!python/tuple
    - 822
    - 866
    - 866
    - 955
    - elaboration
  - !!python/tuple
    - 866
    - 868
    - 868
    - 955
    - elaboration
  - !!python/tuple
    - 868
    - 876
    - 876
    - 955
    - elaboration
  - !!python/tuple
    - 876
    - 882
    - 882
    - 891
    - elaboration
  - !!python/tuple
    - 876
    - 891
    - 891
    - 955
    - elaboration
  - !!python/tuple
    - 891
    - 896
    - 896
    - 904
    - elaboration
  - !!python/tuple
    - 896
    - 897
    - 897
    - 904
    - elaboration
  - !!python/tuple
    - 891
    - 904
    - 904
    - 955
    - elaboration
  - !!python/tuple
    - 904
    - 926
    - 926
    - 940
    - same_unit
  - !!python/tuple
    - 904
    - 909
    - 909
    - 926
    - elaboration
  - !!python/tuple
    - 909
    - 910
    - 910
    - 919
    - elaboration
  - !!python/tuple
    - 909
    - 919
    - 919
    - 926
    - elaboration
  - !!python/tuple
    - 926
    - 940
    - 904
    - 926
    - same_unit
  - !!python/tuple
    - 926
    - 930
    - 930
    - 940
    - elaboration
  - !!python/tuple
    - 904
    - 940
    - 940
    - 955
    - elaboration
  - !!python/tuple
    - 940
    - 946
    - 946
    - 955
    - elaboration
  - !!python/tuple
    - 955
    - 1002
    - 822
    - 955
    - list
  - !!python/tuple
    - 956
    - 980
    - 955
    - 956
    - attribution
  - !!python/tuple
    - 962
    - 980
    - 956
    - 962
    - attribution
  - !!python/tuple
    - 962
    - 972
    - 972
    - 973
    - elaboration
  - !!python/tuple
    - 962
    - 973
    - 973
    - 980
    - elaboration
  - !!python/tuple
    - 955
    - 980
    - 980
    - 1002
    - elaboration
  - !!python/tuple
    - 980
    - 988
    - 988
    - 1002
    - elaboration
  - !!python/tuple
    - 988
    - 992
    - 992
    - 1002
    - same_unit
  - !!python/tuple
    - 988
    - 989
    - 989
    - 992
    - elaboration
  - !!python/tuple
    - 992
    - 1002
    - 988
    - 992
    - same_unit
  - !!python/tuple
    - 1002
    - 1631
    - 822
    - 1002
    - list
  - !!python/tuple
    - 1002
    - 1135
    - 1135
    - 1631
    - topic
  - !!python/tuple
    - 1002
    - 1004
    - 1004
    - 1135
    - elaboration
  - !!python/tuple
    - 1004
    - 1007
    - 1007
    - 1041
    - elaboration
  - !!python/tuple
    - 1007
    - 1033
    - 1033
    - 1041
    - elaboration
  - !!python/tuple
    - 1004
    - 1041
    - 1041
    - 1135
    - elaboration
  - !!python/tuple
    - 1041
    - 1050
    - 1050
    - 1058
    - same_unit
  - !!python/tuple
    - 1041
    - 1044
    - 1044
    - 1050
    - elaboration
  - !!python/tuple
    - 1050
    - 1058
    - 1041
    - 1050
    - same_unit
  - !!python/tuple
    - 1041
    - 1058
    - 1058
    - 1135
    - elaboration
  - !!python/tuple
    - 1058
    - 1063
    - 1063
    - 1070
    - elaboration
  - !!python/tuple
    - 1058
    - 1070
    - 1070
    - 1135
    - elaboration
  - !!python/tuple
    - 1070
    - 1084
    - 1084
    - 1135
    - same_unit
  - !!python/tuple
    - 1070
    - 1075
    - 1075
    - 1084
    - elaboration
  - !!python/tuple
    - 1084
    - 1135
    - 1070
    - 1084
    - same_unit
  - !!python/tuple
    - 1084
    - 1092
    - 1092
    - 1135
    - same_unit
  - !!python/tuple
    - 1084
    - 1091
    - 1091
    - 1092
    - elaboration
  - !!python/tuple
    - 1092
    - 1135
    - 1084
    - 1092
    - same_unit
  - !!python/tuple
    - 1092
    - 1107
    - 1107
    - 1120
    - elaboration
  - !!python/tuple
    - 1111
    - 1120
    - 1107
    - 1111
    - attribution
  - !!python/tuple
    - 1092
    - 1120
    - 1120
    - 1135
    - elaboration
  - !!python/tuple
    - 1120
    - 1130
    - 1130
    - 1133
    - elaboration
  - !!python/tuple
    - 1120
    - 1133
    - 1133
    - 1135
    - elaboration
  - !!python/tuple
    - 1135
    - 1631
    - 1002
    - 1135
    - topic
  - !!python/tuple
    - 1135
    - 1161
    - 1161
    - 1631
    - topic
  - !!python/tuple
    - 1135
    - 1138
    - 1138
    - 1161
    - elaboration
  - !!python/tuple
    - 1141
    - 1161
    - 1138
    - 1141
    - attribution
  - !!python/tuple
    - 1141
    - 1145
    - 1145
    - 1161
    - purpose
  - !!python/tuple
    - 1145
    - 1156
    - 1156
    - 1161
    - elaboration
  - !!python/tuple
    - 1161
    - 1631
    - 1135
    - 1161
    - topic
  - !!python/tuple
    - 1161
    - 1163
    - 1163
    - 1631
    - elaboration
  - !!python/tuple
    - 1163
    - 1171
    - 1171
    - 1631
    - textualorganization
  - !!python/tuple
    - 1171
    - 1631
    - 1163
    - 1171
    - textualorganization
  - !!python/tuple
    - 1171
    - 1185
    - 1185
    - 1631
    - list
  - !!python/tuple
    - 1185
    - 1631
    - 1171
    - 1185
    - list
  - !!python/tuple
    - 1185
    - 1213
    - 1213
    - 1631
    - topic
  - !!python/tuple
    - 1185
    - 1201
    - 1201
    - 1213
    - same_unit
  - !!python/tuple
    - 1185
    - 1189
    - 1189
    - 1201
    - elaboration
  - !!python/tuple
    - 1189
    - 1196
    - 1196
    - 1201
    - elaboration
  - !!python/tuple
    - 1196
    - 1200
    - 1200
    - 1201
    - elaboration
  - !!python/tuple
    - 1201
    - 1213
    - 1185
    - 1201
    - same_unit
  - !!python/tuple
    - 1202
    - 1213
    - 1201
    - 1202
    - attribution
  - !!python/tuple
    - 1213
    - 1631
    - 1185
    - 1213
    - topic
  - !!python/tuple
    - 1213
    - 1239
    - 1239
    - 1631
    - topic
  - !!python/tuple
    - 1213
    - 1223
    - 1223
    - 1225
    - elaboration
  - !!python/tuple
    - 1213
    - 1225
    - 1225
    - 1239
    - elaboration
  - !!python/tuple
    - 1227
    - 1239
    - 1225
    - 1227
    - attribution
  - !!python/tuple
    - 1229
    - 1239
    - 1227
    - 1229
    - attribution
  - !!python/tuple
    - 1229
    - 1230
    - 1230
    - 1239
    - elaboration
  - !!python/tuple
    - 1239
    - 1631
    - 1213
    - 1239
    - topic
  - !!python/tuple
    - 1239
    - 1249
    - 1249
    - 1266
    - list
  - !!python/tuple
    - 1249
    - 1266
    - 1239
    - 1249
    - list
  - !!python/tuple
    - 1239
    - 1266
    - 1266
    - 1631
    - elaboration
  - !!python/tuple
    - 1266
    - 1334
    - 1334
    - 1631
    - list
  - !!python/tuple
    - 1266
    - 1277
    - 1277
    - 1334
    - elaboration
  - !!python/tuple
    - 1300
    - 1311
    - 1277
    - 1300
    - attribution
  - !!python/tuple
    - 1277
    - 1283
    - 1283
    - 1300
    - elaboration
  - !!python/tuple
    - 1277
    - 1311
    - 1311
    - 1334
    - elaboration
  - !!python/tuple
    - 1311
    - 1314
    - 1314
    - 1334
    - elaboration
  - !!python/tuple
    - 1327
    - 1334
    - 1314
    - 1327
    - attribution
  - !!python/tuple
    - 1327
    - 1332
    - 1332
    - 1334
    - elaboration
  - !!python/tuple
    - 1334
    - 1631
    - 1266
    - 1334
    - list
  - !!python/tuple
    - 1334
    - 1338
    - 1338
    - 1369
    - elaboration
  - !!python/tuple
    - 1338
    - 1341
    - 1341
    - 1369
    - purpose
  - !!python/tuple
    - 1341
    - 1363
    - 1363
    - 1369
    - elaboration
  - !!python/tuple
    - 1334
    - 1369
    - 1369
    - 1432
    - elaboration
  - !!python/tuple
    - 1369
    - 1390
    - 1390
    - 1432
    - list
  - !!python/tuple
    - 1369
    - 1375
    - 1375
    - 1390
    - elaboration
  - !!python/tuple
    - 1390
    - 1432
    - 1369
    - 1390
    - list
  - !!python/tuple
    - 1399
    - 1408
    - 1390
    - 1399
    - attribution
  - !!python/tuple
    - 1399
    - 1404
    - 1404
    - 1408
    - elaboration
  - !!python/tuple
    - 1390
    - 1408
    - 1408
    - 1432
    - means
  - !!python/tuple
    - 1408
    - 1418
    - 1418
    - 1432
    - elaboration
  - !!python/tuple
    - 1418
    - 1423
    - 1423
    - 1432
    - elaboration
  - !!python/tuple
    - 1334
    - 1432
    - 1432
    - 1596
    - elaboration
  - !!python/tuple
    - 1432
    - 1439
    - 1439
    - 1596
    - question
  - !!python/tuple
    - 1439
    - 1596
    - 1432
    - 1439
    - question
  - !!python/tuple
    - 1439
    - 1460
    - 1460
    - 1596
    - contrast
  - !!python/tuple
    - 1439
    - 1442
    - 1442
    - 1460
    - purpose
  - !!python/tuple
    - 1442
    - 1444
    - 1444
    - 1460
    - purpose
  - !!python/tuple
    - 1444
    - 1451
    - 1451
    - 1460
    - elaboration
  - !!python/tuple
    - 1460
    - 1596
    - 1439
    - 1460
    - contrast
  - !!python/tuple
    - 1460
    - 1464
    - 1464
    - 1474
    - purpose
  - !!python/tuple
    - 1460
    - 1474
    - 1474
    - 1596
    - elaboration
  - !!python/tuple
    - 1474
    - 1487
    - 1487
    - 1501
    - elaboration
  - !!python/tuple
    - 1474
    - 1501
    - 1501
    - 1518
    - circumstance
  - !!python/tuple
    - 1501
    - 1517
    - 1517
    - 1518
    - same_unit
  - !!python/tuple
    - 1501
    - 1509
    - 1509
    - 1517
    - elaboration
  - !!python/tuple
    - 1517
    - 1518
    - 1501
    - 1517
    - same_unit
  - !!python/tuple
    - 1474
    - 1518
    - 1518
    - 1596
    - elaboration
  - !!python/tuple
    - 1518
    - 1525
    - 1525
    - 1546
    - concession
  - !!python/tuple
    - 1525
    - 1531
    - 1531
    - 1546
    - circumstance
  - !!python/tuple
    - 1531
    - 1538
    - 1538
    - 1546
    - elaboration
  - !!python/tuple
    - 1538
    - 1543
    - 1543
    - 1546
    - elaboration
  - !!python/tuple
    - 1518
    - 1546
    - 1546
    - 1596
    - elaboration
  - !!python/tuple
    - 1546
    - 1549
    - 1549
    - 1565
    - elaboration
  - !!python/tuple
    - 1549
    - 1559
    - 1559
    - 1565
    - elaboration
  - !!python/tuple
    - 1546
    - 1565
    - 1565
    - 1596
    - elaboration
  - !!python/tuple
    - 1587
    - 1596
    - 1565
    - 1587
    - attribution
  - !!python/tuple
    - 1334
    - 1596
    - 1596
    - 1631
    - elaboration
  - !!python/tuple
    - 1596
    - 1622
    - 1622
    - 1631
    - list
  - !!python/tuple
    - 1596
    - 1600
    - 1600
    - 1605
    - elaboration
  - !!python/tuple
    - 1600
    - 1601
    - 1601
    - 1605
    - elaboration
  - !!python/tuple
    - 1596
    - 1605
    - 1605
    - 1622
    - elaboration
  - !!python/tuple
    - 1615
    - 1622
    - 1605
    - 1615
    - attribution
  - !!python/tuple
    - 1622
    - 1631
    - 1596
    - 1622
    - list
  - !!python/tuple
    - 1622
    - 1626
    - 1626
    - 1631
    - elaboration
  - !!python/tuple
    - 1626
    - 1627
    - 1627
    - 1631
    - elaboration
  tokens:
  - The
  - paper
  - proposes
  - to
  - use
  - successor
  - features
  - for
  - the
  - purpose
  - of
  - option
  - discovery
  - .
  - The
  - idea
  - is
  - to
  - start
  - by
  - constructing
  - successor
  - features
  - based
  - 'on'
  - a
  - random
  - policy
  - ','
  - cluster
  - them
  - to
  - discover
  - subgoals
  - ','
  - learn
  - options
  - that
  - reach
  - these
  - subgoals
  - ','
  - then
  - iterate
  - this
  - process
  - .
  - This
  - could
  - be
  - an
  - interesting
  - proposal
  - ','
  - but
  - there
  - are
  - several
  - conceptual
  - problems
  - with
  - the
  - paper
  - ','
  - and
  - then
  - many
  - more
  - minor
  - issues
  - ','
  - which
  - put
  - it
  - below
  - the
  - threshold
  - at
  - the
  - moment
  - .
  - Bigger
  - comments
  - ':'
  - '1'
  - .
  - The
  - reward
  - used
  - to
  - train
  - the
  - options
  - -LRB-
  - eq
  - '5'
  - -RRB-
  - could
  - be
  - either
  - positive
  - or
  - negative
  - .
  - Hence
  - ','
  - it
  - is
  - not
  - clear
  - how
  - or
  - why
  - this
  - is
  - related
  - to
  - getting
  - options
  - that
  - go
  - to
  - a
  - goal
  - .
  - '2'
  - .
  - Computing
  - SRs
  - only
  - for
  - a
  - random
  - policy
  - seems
  - like
  - it
  - will
  - waste
  - potentially
  - a
  - lot
  - of
  - data
  - .
  - Why
  - not
  - do
  - off-policy
  - learning
  - of
  - the
  - SR
  - while
  - performing
  - the
  - option
  - '?'
  - '3'
  - .
  - The
  - candidate
  - states
  - formula
  - seems
  - very
  - heuristic
  - .
  - It
  - does
  - not
  - favour
  - reaching
  - many
  - places
  - necessarily
  - -LRB-
  - eg
  - going
  - to
  - one
  - next
  - state
  - would
  - give
  - a
  - '1'
  - /
  - -LRB-
  - 1-gamma
  - -RRB-
  - SR
  - value
  - -RRB-
  - '4'
  - .
  - Fig
  - '5'
  - is
  - very
  - confusing
  - .
  - There
  - are
  - large
  - regions
  - of
  - all
  - subgoals
  - and
  - then
  - subgoals
  - that
  - are
  - very
  - spread
  - out
  - .
  - If
  - so
  - many
  - subgoals
  - are
  - close
  - by
  - ','
  - why
  - would
  - an
  - agent
  - explore
  - '?'
  - It
  - could
  - just
  - jump
  - randomly
  - in
  - that
  - region
  - for
  - a
  - while
  - .
  - It
  - would
  - have
  - been
  - useful
  - to
  - plot
  - the
  - trajectory
  - distribution
  - of
  - the
  - agent
  - when
  - using
  - the
  - learned
  - options
  - to
  - see
  - what
  - exactly
  - the
  - agent
  - is
  - doing
  - '5'
  - .
  - There
  - are
  - some
  - hacks
  - that
  - detract
  - from
  - the
  - clarity
  - of
  - the
  - results
  - and
  - the
  - merits
  - of
  - the
  - proposed
  - method
  - .
  - For
  - example
  - ','
  - options
  - are
  - supposed
  - to
  - be
  - good
  - for
  - exploration
  - ','
  - so
  - sampling
  - them
  - less
  - would
  - defeat
  - the
  - purpose
  - of
  - constructing
  - them
  - ','
  - but
  - that
  - is
  - exactly
  - what
  - the
  - authors
  - end
  - up
  - doing
  - .
  - This
  - is
  - very
  - strange
  - and
  - seems
  - like
  - a
  - hack
  - .
  - Similarly
  - ','
  - the
  - use
  - of
  - auxiliary
  - tasks
  - makes
  - it
  - unclear
  - what
  - is
  - the
  - relative
  - merit
  - of
  - the
  - proposed
  - method
  - .
  - It
  - would
  - have
  - been
  - very
  - useful
  - to
  - avoid
  - using
  - all
  - these
  - bells
  - and
  - whistles
  - and
  - stick
  - as
  - closely
  - as
  - possible
  - to
  - the
  - stated
  - idea
  - .
  - '6'
  - .
  - The
  - experiments
  - need
  - to
  - be
  - described
  - much
  - better
  - .
  - For
  - example
  - ','
  - in
  - the
  - grid
  - worlds
  - are
  - action
  - effects
  - deterministic
  - or
  - stochastic
  - '?'
  - Are
  - start
  - state
  - and
  - goal
  - state
  - drawn
  - at
  - random
  - but
  - maintained
  - fixed
  - across
  - the
  - learning
  - ','
  - or
  - each
  - run
  - has
  - a
  - different
  - pair
  - '?'
  - Are
  - parameters
  - optimized
  - for
  - each
  - algorithm
  - '?'
  - In
  - the
  - plots
  - for
  - the
  - DM
  - Lab
  - experiments
  - ','
  - what
  - are
  - we
  - looking
  - at
  - '?'
  - Policies
  - '?'
  - End
  - states
  - '?'
  - How
  - do
  - options
  - compare
  - to
  - Q-learning
  - only
  - in
  - this
  - case
  - '?'
  - Do
  - you
  - still
  - do
  - the
  - non-unit
  - exploration
  - '?'
  - The
  - network
  - used
  - seems
  - gigantic
  - ','
  - was
  - this
  - optimized
  - or
  - was
  - this
  - the
  - first
  - choice
  - that
  - came
  - to
  - mind
  - '?'
  - Would
  - this
  - not
  - overfit
  - '?'
  - What
  - is
  - the
  - nonlinearity
  - '?'
  - Small
  - comments
  - ':'
  - '-'
  - This
  - synergy
  - enables
  - the
  - rapid
  - learning
  - Successor
  - representations
  - by
  - improving
  - sample
  - efficiency
  - .
  - '-'
  - Argmax
  - a
  - ''''
  - before
  - eq
  - '1'
  - '-'
  - Inconsistent
  - notation
  - for
  - the
  - transition
  - probability
  - p
  - '-'
  - Eq
  - '3'
  - and
  - '4'
  - are
  - incorrect
  - -LRB-
  - you
  - seem
  - to
  - be
  - one-off
  - in
  - the
  - feature
  - vectors
  - used
  - -RRB-
  - '-'
  - Figure
  - '2'
  - is
  - unclear
  - ','
  - it
  - requires
  - more
  - explanation
  - '-'
  - Eq
  - '6'
  - does
  - not
  - correspond
  - to
  - eq
  - '5'
  - '-'
  - In
  - Fig
  - '6'
  - are
  - the
  - '4'
  - panes
  - corresponding
  - top
  - the
  - '4'
  - envs
  - .
  - '?'
  - Please
  - explain
  - .
  - Also
  - this
  - figure
  - needs
  - error
  - bars
  - '-'
  - It
  - would
  - be
  - useful
  - to
  - plot
  - not
  - just
  - AUC
  - ','
  - but
  - actual
  - learning
  - curves
  - ','
  - in
  - order
  - to
  - see
  - their
  - shape
  - -LRB-
  - eg
  - rising
  - faster
  - and
  - asymptoting
  - lower
  - may
  - give
  - a
  - better
  - AUC
  - -RRB-
  - .
  - '-'
  - Does
  - primitive
  - Q-learning
  - get
  - the
  - same
  - number
  - of
  - time
  - steps
  - as
  - '*'
  - all
  - '*'
  - stages
  - of
  - the
  - proposed
  - algorithm
  - '?'
  - If
  - not
  - ','
  - it
  - is
  - not
  - a
  - fair
  - comparison
  - '-'
  - It
  - would
  - be
  - nice
  - to
  - also
  - have
  - quantitative
  - results
  - corresponding
  - to
  - the
  - experiments
  - in
  - Fig
  - '7'
  - .
  - Thank
  - you
  - for
  - going
  - through
  - our
  - work
  - and
  - providing
  - valuable
  - feedback
  - .
  - We
  - hopefully
  - address
  - the
  - concerns
  - and
  - questions
  - raised
  - in
  - this
  - response
  - and
  - we
  - would
  - be
  - happy
  - to
  - expand
  - 'on'
  - any
  - point
  - unclear
  - in
  - this
  - response
  - .
  - '1'
  - .
  - The
  - reward
  - used
  - to
  - train
  - the
  - options
  - ':'
  - One
  - way
  - to
  - understand
  - the
  - proposed
  - reward
  - function
  - is
  - to
  - look
  - at
  - Figure
  - '22'
  - -LRB-
  - Appendix
  - O
  - -RRB-
  - .
  - This
  - reward
  - function
  - corresponds
  - to
  - one
  - single
  - option
  - and
  - dictates
  - the
  - intra-option
  - policy
  - for
  - the
  - same
  - .
  - The
  - difference
  - between
  - the
  - values
  - printed
  - 'on'
  - the
  - individual
  - states
  - corresponds
  - to
  - the
  - reward
  - for
  - a
  - transition
  - between
  - the
  - two
  - states
  - .
  - Hence
  - the
  - designed
  - reward
  - function
  - ensures
  - that
  - we
  - reach
  - a
  - state
  - with
  - the
  - largest
  - value
  - of
  - $
  - \
  - phi
  - -LRB-
  - s
  - -RRB-
  - .
  - '|'
  - Psi
  - -LRB-
  - Sub-goal
  - -RRB-
  - $
  - '2'
  - .
  - Off-policy
  - SR
  - ':'
  - Learning
  - the
  - SR
  - in
  - an
  - off-policy
  - manner
  - is
  - extremely
  - hard
  - and
  - we
  - are
  - unaware
  - of
  - any
  - such
  - formulations
  - .
  - This
  - is
  - primarily
  - because
  - it
  - is
  - very
  - difficult
  - to
  - relate
  - the
  - discounted
  - visitation
  - counts
  - of
  - one
  - policy
  - to
  - that
  - of
  - another
  - .
  - '3'
  - .
  - The
  - candidate
  - states
  - formula
  - seems
  - very
  - heuristic
  - .
  - We
  - have
  - a
  - very
  - good
  - reason
  - for
  - the
  - choice
  - of
  - the
  - candidate
  - states
  - formula
  - .
  - The
  - candidate
  - states
  - are
  - those
  - states
  - which
  - have
  - a
  - moderately
  - developed
  - SR.
  - .
  - The
  - formula
  - uses
  - the
  - L1
  - norm
  - which
  - encodes
  - the
  - visitation
  - count
  - of
  - a
  - particular
  - state
  - -LRB-
  - See
  - -LSB-
  - '1'
  - -RSB-
  - -RRB-
  - ','
  - hence
  - encoding
  - the
  - degree
  - to
  - which
  - the
  - SR
  - of
  - a
  - state
  - is
  - developed
  - .
  - The
  - clustering
  - from
  - the
  - SR-options
  - ensures
  - that
  - the
  - chosen
  - sub-goals
  - are
  - sufficiently
  - spread
  - apart
  - .
  - Can
  - you
  - kindly
  - clarify
  - the
  - comment
  - ':'
  - '``'
  - going
  - to
  - one
  - next
  - state
  - would
  - give
  - a
  - '1'
  - /
  - -LRB-
  - 1-gamma
  - -RRB-
  - SR
  - value
  - ''''''
  - .
  - The
  - maximum
  - magnitude
  - of
  - the
  - SR
  - is
  - '1'
  - /
  - -LRB-
  - 1-gamma
  - -RRB-
  - so
  - the
  - statement
  - is
  - n't
  - too
  - clear
  - to
  - us
  - .
  - '4'
  - .
  - Fig
  - '5'
  - ':'
  - As
  - stated
  - in
  - the
  - image
  - caption
  - ','
  - the
  - images
  - 'on'
  - the
  - left
  - are
  - for
  - the
  - Successor
  - Options
  - and
  - the
  - images
  - 'on'
  - the
  - right
  - are
  - for
  - Eigen-options
  - -LRB-
  - the
  - method
  - we
  - compare
  - to
  - -RRB-
  - .
  - The
  - coloured
  - states
  - -LRB-
  - yellow
  - to
  - light
  - blue
  - -RRB-
  - are
  - the
  - termination
  - states
  - of
  - all
  - options
  - .
  - This
  - clearly
  - emphasizes
  - the
  - point
  - that
  - the
  - Eigen-options
  - have
  - nearby
  - sub-goals
  - .
  - With
  - regards
  - to
  - the
  - statement
  - -LRB-
  - If
  - so
  - many
  - subgoals
  - are
  - close
  - by
  - ','
  - why
  - would
  - an
  - agent
  - explore
  - '?'
  - -RRB-
  - ','
  - this
  - is
  - precisely
  - the
  - problem
  - with
  - the
  - Eigen-options
  - framework
  - ','
  - and
  - this
  - is
  - the
  - basis
  - 'on'
  - which
  - we
  - claim
  - that
  - Successor
  - options
  - will
  - exhibit
  - better
  - empirical
  - performance
  - .
  - With
  - regards
  - to
  - the
  - trajectory
  - distribution
  - ','
  - Appendix
  - J
  - addresses
  - the
  - same
  - .
  - '5'
  - .
  - Experimental
  - Evaluation
  - ':'
  - The
  - reviewer
  - claims
  - that
  - we
  - use
  - tricks
  - to
  - detract
  - from
  - the
  - clarity
  - of
  - the
  - paper
  - ','
  - an
  - assessment
  - we
  - politely
  - disagree
  - with
  - .
  - 5a
  - .
  - '``'
  - Auxiliary
  - tasks
  - make
  - relative
  - merit
  - unclear
  - ':'
  - Can
  - the
  - reviewer
  - kindly
  - clarify
  - which
  - auxiliary
  - tasks
  - are
  - being
  - referred
  - to
  - here
  - '?'
  - The
  - only
  - auxiliary
  - loss
  - we
  - use
  - is
  - the
  - image
  - reconstruction
  - loss
  - -LRB-
  - auto-encoder
  - loss
  - -RRB-
  - which
  - ensures
  - that
  - the
  - Successor
  - Features
  - do
  - not
  - learn
  - the
  - 'null'
  - vector
  - .
  - This
  - is
  - a
  - very
  - commonly
  - used
  - loss
  - in
  - Successor
  - Representation
  - based
  - papers
  - -LRB-
  - See
  - -LSB-
  - '2'
  - -RSB-
  - which
  - justify
  - the
  - usage
  - of
  - the
  - same
  - -RRB-
  - .
  - This
  - loss
  - is
  - very
  - much
  - part
  - of
  - our
  - described
  - framework
  - and
  - does
  - not
  - in
  - an
  - any
  - form
  - ','
  - hide
  - the
  - merit
  - of
  - the
  - reported
  - results
  - 5b
  - .
  - Sampling
  - options
  - less
  - would
  - defeat
  - the
  - purpose
  - of
  - constructing
  - them
  - ':'
  - We
  - have
  - demonstrated
  - evaluations
  - for
  - Q-learning
  - -LRB-
  - only
  - actions
  - -RRB-
  - ','
  - and
  - SR-options
  - with
  - uniform
  - and
  - non-uniform
  - exploration
  - and
  - hence
  - we
  - do
  - not
  - '``'
  - detract
  - the
  - clarity
  - of
  - the
  - results
  - ''''''
  - using
  - hacks
  - .
  - We
  - have
  - attempted
  - to
  - be
  - as
  - transparent
  - as
  - possible
  - in
  - the
  - reported
  - results
  - and
  - we
  - believe
  - we
  - stick
  - to
  - the
  - method
  - described
  - .
  - There
  - are
  - two
  - points
  - we
  - would
  - like
  - to
  - make
  - '1'
  - -RRB-
  - The
  - primary
  - focus
  - of
  - this
  - work
  - is
  - 'on'
  - Option-discovery
  - and
  - hence
  - we
  - do
  - not
  - delve
  - deeply
  - into
  - areas
  - related
  - to
  - learning
  - with
  - options
  - .
  - There
  - are
  - a
  - plethora
  - of
  - possibilities
  - worth
  - exploring
  - and
  - this
  - does
  - not
  - detract
  - from
  - the
  - utility
  - of
  - the
  - options
  - themselves
  - .
  - '2'
  - -RRB-
  - Sampling
  - the
  - option
  - less
  - does
  - not
  - mean
  - that
  - there
  - is
  - 'no'
  - utility
  - in
  - constructing
  - them
  - .
  - By
  - that
  - logic
  - ','
  - one
  - should
  - expect
  - a
  - naive
  - Q-learner
  - to
  - have
  - the
  - best
  - performance
  - -LRB-
  - which
  - is
  - clearly
  - not
  - the
  - case
  - -RRB-
  - .
  - So
  - why
  - is
  - this
  - scheme
  - useful
  - '?'
  - Options
  - are
  - used
  - to
  - navigate
  - to
  - key
  - parts
  - of
  - the
  - state
  - space
  - -LRB-
  - that
  - primitive
  - actions
  - are
  - unable
  - to
  - -RRB-
  - ','
  - while
  - actions
  - are
  - used
  - to
  - explore
  - the
  - newly
  - discovered
  - region
  - in
  - state
  - space
  - .
  - This
  - is
  - analogous
  - to
  - using
  - an
  - airplane
  - to
  - travel
  - to
  - a
  - new
  - city
  - -LRB-
  - using
  - an
  - option
  - to
  - land
  - in
  - new
  - parts
  - of
  - state
  - space
  - -RRB-
  - ','
  - following
  - which
  - one
  - explores
  - the
  - city
  - 'on'
  - foot
  - -LRB-
  - each
  - step
  - is
  - a
  - primitive
  - action
  - -RRB-
  - .
  - Hence
  - the
  - options
  - are
  - still
  - essential
  - ','
  - even
  - if
  - they
  - are
  - sampled
  - infrequently
  - since
  - they
  - perform
  - a
  - sequence
  - of
  - transition
  - that
  - has
  - a
  - negligible
  - probability
  - of
  - happening
  - .
  - Sampling
  - the
  - options
  - frequently
  - would
  - translate
  - to
  - jumping
  - between
  - regions
  - in
  - state
  - space
  - without
  - exploring
  - any
  - one
  - region
  - .
  - See
  - Appendix
  - J
  - for
  - more
  - details
  - -LSB-
  - '1'
  - -RSB-
  - Machado
  - ','
  - Marlos
  - C.
  - ','
  - Marc
  - G.
  - Bellemare
  - ','
  - and
  - Michael
  - Bowling
  - .
  - '``'
  - Count-based
  - exploration
  - with
  - the
  - successor
  - representation
  - .
  - ''''''
  - arXiv
  - preprint
  - arXiv
  - ':'
  - '1807.11622'
  - -LRB-
  - '2018'
  - -RRB-
  - .
  - -LSB-
  - '2'
  - -RSB-
  - Kulkarni
  - ','
  - Tejas
  - D.
  - ','
  - et
  - al.
  - '``'
  - Deep
  - successor
  - reinforcement
  - learning
  - .
  - ''''''
  - arXiv
  - preprint
  - arXiv
  - ':'
  - '1606.02396'
  - -LRB-
  - '2016'
  - -RRB-
  - .
- comment_id: B1ejRvrkam
  rels:
  - !!python/tuple
    - 0
    - 8
    - 8
    - 22
    - elaboration
  - !!python/tuple
    - 8
    - 17
    - 17
    - 22
    - elaboration
  - !!python/tuple
    - 0
    - 22
    - 22
    - 191
    - elaboration
  - !!python/tuple
    - 22
    - 30
    - 30
    - 41
    - elaboration
  - !!python/tuple
    - 32
    - 41
    - 30
    - 32
    - attribution
  - !!python/tuple
    - 22
    - 41
    - 41
    - 191
    - elaboration
  - !!python/tuple
    - 41
    - 48
    - 48
    - 68
    - same_unit
  - !!python/tuple
    - 41
    - 47
    - 47
    - 48
    - elaboration
  - !!python/tuple
    - 48
    - 68
    - 41
    - 48
    - same_unit
  - !!python/tuple
    - 48
    - 67
    - 67
    - 68
    - same_unit
  - !!python/tuple
    - 48
    - 61
    - 61
    - 67
    - elaboration
  - !!python/tuple
    - 67
    - 68
    - 48
    - 67
    - same_unit
  - !!python/tuple
    - 41
    - 68
    - 68
    - 191
    - elaboration
  - !!python/tuple
    - 68
    - 74
    - 74
    - 86
    - purpose
  - !!python/tuple
    - 68
    - 86
    - 86
    - 191
    - elaboration
  - !!python/tuple
    - 86
    - 112
    - 112
    - 191
    - elaboration
  - !!python/tuple
    - 112
    - 123
    - 123
    - 191
    - elaboration
  - !!python/tuple
    - 123
    - 132
    - 132
    - 148
    - elaboration
  - !!python/tuple
    - 132
    - 136
    - 136
    - 148
    - elaboration
  - !!python/tuple
    - 136
    - 137
    - 137
    - 148
    - elaboration
  - !!python/tuple
    - 123
    - 148
    - 148
    - 191
    - elaboration
  - !!python/tuple
    - 148
    - 151
    - 151
    - 177
    - purpose
  - !!python/tuple
    - 151
    - 155
    - 155
    - 177
    - elaboration
  - !!python/tuple
    - 155
    - 172
    - 172
    - 177
    - elaboration
  - !!python/tuple
    - 148
    - 177
    - 177
    - 191
    - elaboration
  - !!python/tuple
    - 177
    - 179
    - 179
    - 191
    - list
  - !!python/tuple
    - 179
    - 191
    - 177
    - 179
    - list
  - !!python/tuple
    - 184
    - 191
    - 179
    - 184
    - attribution
  tokens:
  - The
  - paper
  - proposes
  - a
  - simple
  - but
  - effective
  - method
  - for
  - controlling
  - the
  - location
  - of
  - objects
  - in
  - image
  - generation
  - using
  - generative
  - adversarial
  - networks
  - .
  - Experiments
  - 'on'
  - MNIST
  - and
  - CLEVR
  - are
  - toy
  - examples
  - but
  - illustrate
  - that
  - the
  - model
  - is
  - indeed
  - performing
  - as
  - expected
  - .
  - The
  - experiments
  - 'on'
  - COCO
  - produce
  - results
  - that
  - while
  - containing
  - obvious
  - artefacts
  - are
  - producing
  - output
  - consistent
  - with
  - the
  - input
  - control
  - signal
  - -LRB-
  - i.e.
  - ','
  - bounding
  - boxes
  - -RRB-
  - .
  - It
  - would
  - however
  - have
  - been
  - interesting
  - to
  - see
  - more
  - varied
  - bounding
  - box
  - locations
  - for
  - the
  - same
  - caption
  - .
  - In
  - short
  - ','
  - the
  - paper
  - makes
  - an
  - interesting
  - addition
  - to
  - image
  - generation
  - works
  - and
  - likely
  - to
  - be
  - incorporated
  - into
  - future
  - image
  - generation
  - and
  - inpainting
  - methods
  - .
  - Dear
  - reviewer
  - ','
  - thank
  - you
  - very
  - much
  - for
  - your
  - review
  - .
  - We
  - will
  - update
  - our
  - submission
  - with
  - examples
  - of
  - images
  - based
  - 'on'
  - MS-COCO
  - captions
  - in
  - which
  - we
  - vary
  - the
  - location
  - of
  - the
  - various
  - bounding
  - boxes
  - .
  - We
  - will
  - work
  - to
  - implement
  - the
  - feedback
  - we
  - got
  - and
  - will
  - post
  - an
  - updated
  - version
  - of
  - our
  - submission
  - by
  - the
  - end
  - of
  - next
  - week
  - -LRB-
  - latest
  - 'on'
  - '16'
  - .
  - November
  - -RRB-
  - and
  - will
  - let
  - you
  - know
  - once
  - the
  - updated
  - version
  - is
  - online
  - .
- comment_id: B1e21iAhA7
  rels:
  - !!python/tuple
    - 0
    - 2
    - 2
    - 1382
    - textualorganization
  - !!python/tuple
    - 2
    - 1382
    - 0
    - 2
    - textualorganization
  - !!python/tuple
    - 2
    - 4
    - 4
    - 26
    - elaboration
  - !!python/tuple
    - 2
    - 26
    - 26
    - 1382
    - elaboration
  - !!python/tuple
    - 26
    - 35
    - 35
    - 46
    - elaboration
  - !!python/tuple
    - 26
    - 46
    - 46
    - 1382
    - elaboration
  - !!python/tuple
    - 46
    - 51
    - 51
    - 68
    - list
  - !!python/tuple
    - 51
    - 68
    - 46
    - 51
    - list
  - !!python/tuple
    - 51
    - 52
    - 52
    - 68
    - manner
  - !!python/tuple
    - 52
    - 59
    - 59
    - 68
    - elaboration
  - !!python/tuple
    - 59
    - 60
    - 60
    - 68
    - elaboration
  - !!python/tuple
    - 46
    - 68
    - 68
    - 1382
    - elaboration
  - !!python/tuple
    - 68
    - 77
    - 77
    - 86
    - elaboration
  - !!python/tuple
    - 68
    - 86
    - 86
    - 1382
    - elaboration
  - !!python/tuple
    - 86
    - 94
    - 94
    - 114
    - same_unit
  - !!python/tuple
    - 86
    - 90
    - 90
    - 94
    - elaboration
  - !!python/tuple
    - 94
    - 114
    - 86
    - 94
    - same_unit
  - !!python/tuple
    - 94
    - 98
    - 98
    - 114
    - same_unit
  - !!python/tuple
    - 98
    - 114
    - 94
    - 98
    - same_unit
  - !!python/tuple
    - 86
    - 114
    - 114
    - 1382
    - elaboration
  - !!python/tuple
    - 114
    - 126
    - 126
    - 134
    - list
  - !!python/tuple
    - 126
    - 134
    - 114
    - 126
    - list
  - !!python/tuple
    - 114
    - 134
    - 134
    - 1382
    - elaboration
  - !!python/tuple
    - 134
    - 144
    - 144
    - 158
    - manner
  - !!python/tuple
    - 144
    - 154
    - 154
    - 158
    - same_unit
  - !!python/tuple
    - 144
    - 146
    - 146
    - 154
    - elaboration
  - !!python/tuple
    - 154
    - 158
    - 144
    - 154
    - same_unit
  - !!python/tuple
    - 134
    - 158
    - 158
    - 1382
    - elaboration
  - !!python/tuple
    - 167
    - 175
    - 158
    - 167
    - attribution
  - !!python/tuple
    - 158
    - 175
    - 175
    - 1382
    - elaboration
  - !!python/tuple
    - 175
    - 190
    - 190
    - 1382
    - question
  - !!python/tuple
    - 175
    - 186
    - 186
    - 190
    - comparison
  - !!python/tuple
    - 190
    - 1382
    - 175
    - 190
    - question
  - !!python/tuple
    - 192
    - 200
    - 190
    - 192
    - attribution
  - !!python/tuple
    - 192
    - 197
    - 197
    - 200
    - comparison
  - !!python/tuple
    - 190
    - 200
    - 200
    - 1382
    - elaboration
  - !!python/tuple
    - 200
    - 227
    - 227
    - 1382
    - list
  - !!python/tuple
    - 227
    - 1382
    - 200
    - 227
    - list
  - !!python/tuple
    - 227
    - 242
    - 242
    - 1382
    - list
  - !!python/tuple
    - 230
    - 242
    - 227
    - 230
    - attribution
  - !!python/tuple
    - 242
    - 1382
    - 227
    - 242
    - list
  - !!python/tuple
    - 242
    - 254
    - 254
    - 1382
    - elaboration
  - !!python/tuple
    - 254
    - 269
    - 269
    - 1382
    - list
  - !!python/tuple
    - 269
    - 1382
    - 254
    - 269
    - list
  - !!python/tuple
    - 269
    - 271
    - 271
    - 281
    - elaboration
  - !!python/tuple
    - 273
    - 281
    - 271
    - 273
    - attribution
  - !!python/tuple
    - 269
    - 281
    - 281
    - 1382
    - elaboration
  - !!python/tuple
    - 281
    - 289
    - 289
    - 302
    - elaboration
  - !!python/tuple
    - 281
    - 302
    - 302
    - 1382
    - elaboration
  - !!python/tuple
    - 302
    - 314
    - 314
    - 1382
    - elaboration
  - !!python/tuple
    - 342
    - 1382
    - 314
    - 342
    - antithesis
  - !!python/tuple
    - 329
    - 342
    - 314
    - 329
    - attribution
  - !!python/tuple
    - 329
    - 334
    - 334
    - 342
    - elaboration
  - !!python/tuple
    - 348
    - 353
    - 342
    - 348
    - attribution
  - !!python/tuple
    - 342
    - 353
    - 353
    - 1382
    - elaboration
  - !!python/tuple
    - 353
    - 361
    - 361
    - 367
    - elaboration
  - !!python/tuple
    - 353
    - 367
    - 367
    - 1382
    - elaboration
  - !!python/tuple
    - 367
    - 391
    - 391
    - 1382
    - list
  - !!python/tuple
    - 367
    - 374
    - 374
    - 391
    - elaboration
  - !!python/tuple
    - 391
    - 1382
    - 367
    - 391
    - list
  - !!python/tuple
    - 391
    - 466
    - 466
    - 1382
    - list
  - !!python/tuple
    - 391
    - 394
    - 394
    - 466
    - textualorganization
  - !!python/tuple
    - 394
    - 466
    - 391
    - 394
    - textualorganization
  - !!python/tuple
    - 396
    - 410
    - 394
    - 396
    - attribution
  - !!python/tuple
    - 396
    - 402
    - 402
    - 410
    - manner
  - !!python/tuple
    - 402
    - 404
    - 404
    - 410
    - purpose
  - !!python/tuple
    - 394
    - 410
    - 410
    - 466
    - elaboration
  - !!python/tuple
    - 410
    - 419
    - 419
    - 423
    - purpose
  - !!python/tuple
    - 410
    - 423
    - 423
    - 466
    - elaboration
  - !!python/tuple
    - 425
    - 445
    - 423
    - 425
    - attribution
  - !!python/tuple
    - 425
    - 436
    - 436
    - 445
    - purpose
  - !!python/tuple
    - 423
    - 445
    - 445
    - 466
    - elaboration
  - !!python/tuple
    - 466
    - 1382
    - 391
    - 466
    - list
  - !!python/tuple
    - 466
    - 469
    - 469
    - 1382
    - textualorganization
  - !!python/tuple
    - 469
    - 1382
    - 466
    - 469
    - textualorganization
  - !!python/tuple
    - 469
    - 485
    - 485
    - 1382
    - elaboration
  - !!python/tuple
    - 485
    - 489
    - 489
    - 538
    - elaboration
  - !!python/tuple
    - 489
    - 504
    - 504
    - 538
    - same_unit
  - !!python/tuple
    - 489
    - 493
    - 493
    - 504
    - elaboration
  - !!python/tuple
    - 504
    - 538
    - 489
    - 504
    - same_unit
  - !!python/tuple
    - 504
    - 518
    - 518
    - 524
    - elaboration
  - !!python/tuple
    - 504
    - 524
    - 524
    - 529
    - elaboration
  - !!python/tuple
    - 504
    - 529
    - 529
    - 538
    - elaboration
  - !!python/tuple
    - 485
    - 538
    - 538
    - 1382
    - elaboration
  - !!python/tuple
    - 544
    - 556
    - 538
    - 544
    - attribution
  - !!python/tuple
    - 538
    - 556
    - 556
    - 1382
    - elaboration
  - !!python/tuple
    - 556
    - 564
    - 564
    - 583
    - purpose
  - !!python/tuple
    - 564
    - 569
    - 569
    - 583
    - circumstance
  - !!python/tuple
    - 556
    - 583
    - 583
    - 1382
    - elaboration
  - !!python/tuple
    - 594
    - 602
    - 583
    - 594
    - attribution
  - !!python/tuple
    - 594
    - 599
    - 599
    - 602
    - manner
  - !!python/tuple
    - 583
    - 602
    - 602
    - 1382
    - example
  - !!python/tuple
    - 606
    - 640
    - 602
    - 606
    - attribution
  - !!python/tuple
    - 617
    - 640
    - 606
    - 617
    - attribution
  - !!python/tuple
    - 617
    - 623
    - 623
    - 640
    - elaboration
  - !!python/tuple
    - 602
    - 640
    - 640
    - 1382
    - elaboration
  - !!python/tuple
    - 640
    - 652
    - 652
    - 1382
    - list
  - !!python/tuple
    - 640
    - 646
    - 646
    - 652
    - same_unit
  - !!python/tuple
    - 640
    - 642
    - 642
    - 646
    - elaboration
  - !!python/tuple
    - 646
    - 652
    - 640
    - 646
    - same_unit
  - !!python/tuple
    - 648
    - 652
    - 646
    - 648
    - attribution
  - !!python/tuple
    - 652
    - 1382
    - 640
    - 652
    - list
  - !!python/tuple
    - 652
    - 725
    - 725
    - 1382
    - list
  - !!python/tuple
    - 652
    - 654
    - 654
    - 725
    - elaboration
  - !!python/tuple
    - 654
    - 662
    - 662
    - 682
    - same_unit
  - !!python/tuple
    - 654
    - 658
    - 658
    - 662
    - elaboration
  - !!python/tuple
    - 662
    - 682
    - 654
    - 662
    - same_unit
  - !!python/tuple
    - 662
    - 666
    - 666
    - 682
    - same_unit
  - !!python/tuple
    - 666
    - 682
    - 662
    - 666
    - same_unit
  - !!python/tuple
    - 654
    - 682
    - 682
    - 725
    - elaboration
  - !!python/tuple
    - 682
    - 695
    - 695
    - 696
    - same_unit
  - !!python/tuple
    - 682
    - 688
    - 688
    - 695
    - list
  - !!python/tuple
    - 688
    - 695
    - 682
    - 688
    - list
  - !!python/tuple
    - 688
    - 692
    - 692
    - 695
    - elaboration
  - !!python/tuple
    - 695
    - 696
    - 682
    - 695
    - same_unit
  - !!python/tuple
    - 682
    - 696
    - 696
    - 699
    - elaboration
  - !!python/tuple
    - 682
    - 699
    - 699
    - 725
    - elaboration
  - !!python/tuple
    - 703
    - 725
    - 699
    - 703
    - attribution
  - !!python/tuple
    - 703
    - 711
    - 711
    - 725
    - circumstance
  - !!python/tuple
    - 711
    - 712
    - 712
    - 725
    - elaboration
  - !!python/tuple
    - 712
    - 716
    - 716
    - 725
    - same_unit
  - !!python/tuple
    - 712
    - 713
    - 713
    - 716
    - elaboration
  - !!python/tuple
    - 716
    - 725
    - 712
    - 716
    - same_unit
  - !!python/tuple
    - 725
    - 1382
    - 652
    - 725
    - list
  - !!python/tuple
    - 725
    - 727
    - 727
    - 1382
    - list
  - !!python/tuple
    - 727
    - 1382
    - 725
    - 727
    - list
  - !!python/tuple
    - 727
    - 747
    - 747
    - 1382
    - list
  - !!python/tuple
    - 747
    - 1382
    - 727
    - 747
    - list
  - !!python/tuple
    - 747
    - 757
    - 757
    - 1382
    - list
  - !!python/tuple
    - 757
    - 1382
    - 747
    - 757
    - list
  - !!python/tuple
    - 757
    - 760
    - 760
    - 790
    - elaboration
  - !!python/tuple
    - 760
    - 771
    - 771
    - 790
    - elaboration
  - !!python/tuple
    - 757
    - 790
    - 790
    - 802
    - elaboration
  - !!python/tuple
    - 793
    - 802
    - 790
    - 793
    - attribution
  - !!python/tuple
    - 793
    - 798
    - 798
    - 802
    - same_unit
  - !!python/tuple
    - 793
    - 794
    - 794
    - 798
    - same_unit
  - !!python/tuple
    - 794
    - 798
    - 793
    - 794
    - same_unit
  - !!python/tuple
    - 798
    - 802
    - 793
    - 798
    - same_unit
  - !!python/tuple
    - 757
    - 802
    - 802
    - 1382
    - temporal
  - !!python/tuple
    - 802
    - 813
    - 813
    - 1382
    - textualorganization
  - !!python/tuple
    - 802
    - 807
    - 807
    - 813
    - same_unit
  - !!python/tuple
    - 802
    - 803
    - 803
    - 807
    - elaboration
  - !!python/tuple
    - 807
    - 813
    - 802
    - 807
    - same_unit
  - !!python/tuple
    - 813
    - 1382
    - 802
    - 813
    - textualorganization
  - !!python/tuple
    - 820
    - 839
    - 813
    - 820
    - attribution
  - !!python/tuple
    - 813
    - 839
    - 839
    - 1382
    - elaboration
  - !!python/tuple
    - 839
    - 855
    - 855
    - 1382
    - topic
  - !!python/tuple
    - 841
    - 855
    - 839
    - 841
    - attribution
  - !!python/tuple
    - 845
    - 855
    - 841
    - 845
    - attribution
  - !!python/tuple
    - 855
    - 1382
    - 839
    - 855
    - topic
  - !!python/tuple
    - 855
    - 874
    - 874
    - 1382
    - list
  - !!python/tuple
    - 874
    - 1382
    - 855
    - 874
    - list
  - !!python/tuple
    - 874
    - 876
    - 876
    - 887
    - elaboration
  - !!python/tuple
    - 874
    - 887
    - 887
    - 1382
    - elaboration
  - !!python/tuple
    - 887
    - 891
    - 891
    - 902
    - elaboration
  - !!python/tuple
    - 887
    - 902
    - 902
    - 1382
    - elaboration
  - !!python/tuple
    - 902
    - 912
    - 912
    - 916
    - elaboration
  - !!python/tuple
    - 902
    - 916
    - 916
    - 1382
    - elaboration
  - !!python/tuple
    - 916
    - 931
    - 931
    - 937
    - same_unit
  - !!python/tuple
    - 916
    - 927
    - 927
    - 931
    - elaboration
  - !!python/tuple
    - 931
    - 937
    - 916
    - 931
    - same_unit
  - !!python/tuple
    - 931
    - 934
    - 934
    - 937
    - manner
  - !!python/tuple
    - 916
    - 937
    - 937
    - 1382
    - elaboration
  - !!python/tuple
    - 949
    - 957
    - 937
    - 949
    - attribution
  - !!python/tuple
    - 937
    - 946
    - 946
    - 949
    - same_unit
  - !!python/tuple
    - 937
    - 943
    - 943
    - 946
    - elaboration
  - !!python/tuple
    - 946
    - 949
    - 937
    - 946
    - same_unit
  - !!python/tuple
    - 937
    - 957
    - 957
    - 1382
    - elaboration
  - !!python/tuple
    - 957
    - 970
    - 970
    - 1103
    - elaboration
  - !!python/tuple
    - 970
    - 980
    - 980
    - 994
    - purpose
  - !!python/tuple
    - 970
    - 994
    - 994
    - 1103
    - elaboration
  - !!python/tuple
    - 994
    - 1001
    - 1001
    - 1044
    - elaboration
  - !!python/tuple
    - 1001
    - 1006
    - 1006
    - 1044
    - same_unit
  - !!python/tuple
    - 1001
    - 1005
    - 1005
    - 1006
    - list
  - !!python/tuple
    - 1005
    - 1006
    - 1001
    - 1005
    - list
  - !!python/tuple
    - 1006
    - 1044
    - 1001
    - 1006
    - same_unit
  - !!python/tuple
    - 1006
    - 1010
    - 1010
    - 1044
    - elaboration
  - !!python/tuple
    - 1010
    - 1029
    - 1029
    - 1044
    - elaboration
  - !!python/tuple
    - 1029
    - 1032
    - 1032
    - 1044
    - reason
  - !!python/tuple
    - 994
    - 1044
    - 1044
    - 1103
    - elaboration
  - !!python/tuple
    - 1048
    - 1069
    - 1044
    - 1048
    - attribution
  - !!python/tuple
    - 1048
    - 1054
    - 1054
    - 1069
    - circumstance
  - !!python/tuple
    - 1054
    - 1057
    - 1057
    - 1069
    - purpose
  - !!python/tuple
    - 1044
    - 1069
    - 1069
    - 1103
    - example
  - !!python/tuple
    - 1069
    - 1083
    - 1083
    - 1088
    - same_unit
  - !!python/tuple
    - 1069
    - 1076
    - 1076
    - 1083
    - elaboration
  - !!python/tuple
    - 1083
    - 1088
    - 1069
    - 1083
    - same_unit
  - !!python/tuple
    - 1069
    - 1088
    - 1088
    - 1103
    - elaboration
  - !!python/tuple
    - 1088
    - 1092
    - 1092
    - 1103
    - elaboration
  - !!python/tuple
    - 1092
    - 1099
    - 1099
    - 1103
    - attribution
  - !!python/tuple
    - 957
    - 1103
    - 1103
    - 1382
    - elaboration
  - !!python/tuple
    - 1103
    - 1111
    - 1111
    - 1382
    - elaboration
  - !!python/tuple
    - 1111
    - 1125
    - 1125
    - 1382
    - elaboration
  - !!python/tuple
    - 1125
    - 1129
    - 1129
    - 1142
    - means
  - !!python/tuple
    - 1125
    - 1142
    - 1142
    - 1382
    - elaboration
  - !!python/tuple
    - 1142
    - 1150
    - 1150
    - 1159
    - elaboration
  - !!python/tuple
    - 1150
    - 1154
    - 1154
    - 1159
    - same_unit
  - !!python/tuple
    - 1150
    - 1151
    - 1151
    - 1154
    - elaboration
  - !!python/tuple
    - 1154
    - 1159
    - 1150
    - 1154
    - same_unit
  - !!python/tuple
    - 1154
    - 1155
    - 1155
    - 1159
    - elaboration
  - !!python/tuple
    - 1142
    - 1159
    - 1159
    - 1382
    - elaboration
  - !!python/tuple
    - 1159
    - 1179
    - 1179
    - 1382
    - elaboration
  - !!python/tuple
    - 1179
    - 1198
    - 1198
    - 1382
    - list
  - !!python/tuple
    - 1198
    - 1382
    - 1179
    - 1198
    - list
  - !!python/tuple
    - 1202
    - 1236
    - 1198
    - 1202
    - attribution
  - !!python/tuple
    - 1202
    - 1218
    - 1218
    - 1236
    - same_unit
  - !!python/tuple
    - 1202
    - 1217
    - 1217
    - 1218
    - purpose
  - !!python/tuple
    - 1218
    - 1236
    - 1202
    - 1218
    - same_unit
  - !!python/tuple
    - 1218
    - 1228
    - 1228
    - 1236
    - elaboration
  - !!python/tuple
    - 1198
    - 1236
    - 1236
    - 1382
    - elaboration
  - !!python/tuple
    - 1236
    - 1242
    - 1242
    - 1265
    - condition
  - !!python/tuple
    - 1242
    - 1253
    - 1253
    - 1265
    - elaboration
  - !!python/tuple
    - 1236
    - 1265
    - 1265
    - 1382
    - elaboration
  - !!python/tuple
    - 1265
    - 1282
    - 1282
    - 1382
    - list
  - !!python/tuple
    - 1265
    - 1267
    - 1267
    - 1282
    - elaboration
  - !!python/tuple
    - 1267
    - 1271
    - 1271
    - 1282
    - elaboration
  - !!python/tuple
    - 1282
    - 1382
    - 1265
    - 1282
    - list
  - !!python/tuple
    - 1282
    - 1286
    - 1286
    - 1295
    - elaboration
  - !!python/tuple
    - 1282
    - 1295
    - 1295
    - 1382
    - elaboration
  - !!python/tuple
    - 1295
    - 1305
    - 1305
    - 1382
    - list
  - !!python/tuple
    - 1305
    - 1382
    - 1295
    - 1305
    - list
  - !!python/tuple
    - 1305
    - 1322
    - 1322
    - 1382
    - list
  - !!python/tuple
    - 1308
    - 1322
    - 1305
    - 1308
    - attribution
  - !!python/tuple
    - 1308
    - 1309
    - 1309
    - 1322
    - elaboration
  - !!python/tuple
    - 1316
    - 1322
    - 1309
    - 1316
    - attribution
  - !!python/tuple
    - 1321
    - 1322
    - 1316
    - 1321
    - attribution
  - !!python/tuple
    - 1322
    - 1382
    - 1305
    - 1322
    - list
  - !!python/tuple
    - 1322
    - 1340
    - 1340
    - 1382
    - list
  - !!python/tuple
    - 1324
    - 1340
    - 1322
    - 1324
    - attribution
  - !!python/tuple
    - 1324
    - 1333
    - 1333
    - 1340
    - elaboration
  - !!python/tuple
    - 1333
    - 1334
    - 1334
    - 1340
    - attribution
  - !!python/tuple
    - 1340
    - 1382
    - 1322
    - 1340
    - list
  - !!python/tuple
    - 1340
    - 1345
    - 1345
    - 1363
    - purpose
  - !!python/tuple
    - 1345
    - 1350
    - 1350
    - 1363
    - elaboration
  - !!python/tuple
    - 1350
    - 1354
    - 1354
    - 1363
    - elaboration
  - !!python/tuple
    - 1354
    - 1358
    - 1358
    - 1363
    - elaboration
  - !!python/tuple
    - 1340
    - 1363
    - 1363
    - 1382
    - elaboration
  - !!python/tuple
    - 1367
    - 1382
    - 1363
    - 1367
    - attribution
  - !!python/tuple
    - 1367
    - 1370
    - 1370
    - 1382
    - list
  - !!python/tuple
    - 1370
    - 1382
    - 1367
    - 1370
    - list
  tokens:
  - Comments
  - ':'
  - The
  - author
  - -LRB-
  - s
  - -RRB-
  - provide
  - stability
  - and
  - generalization
  - bounds
  - for
  - SGD
  - with
  - momentum
  - for
  - strongly
  - convex
  - ','
  - smooth
  - ','
  - and
  - Lipschitz
  - losses
  - .
  - This
  - paper
  - basically
  - follows
  - and
  - extends
  - the
  - results
  - from
  - -LRB-
  - Hardt
  - ','
  - Recht
  - ','
  - and
  - Singer
  - ','
  - '2016'
  - -RRB-
  - .
  - Section
  - '2'
  - is
  - quite
  - identical
  - but
  - without
  - mentioning
  - the
  - overlap
  - from
  - Section
  - '2'
  - in
  - -LRB-
  - Hardt
  - et
  - al
  - ','
  - '2016'
  - -RRB-
  - .
  - The
  - analysis
  - closely
  - follows
  - the
  - approach
  - from
  - there
  - .
  - The
  - proof
  - of
  - Theorem
  - '2'
  - has
  - some
  - issues
  - .
  - The
  - set
  - of
  - assumptions
  - -LRB-
  - smooth
  - ','
  - Lipschitz
  - and
  - strongly
  - convex
  - -RRB-
  - is
  - not
  - valid
  - 'on'
  - the
  - whole
  - set
  - R
  - ^
  - d
  - ','
  - for
  - example
  - quadratic
  - function
  - .
  - In
  - this
  - case
  - ','
  - your
  - Lipschitz
  - constant
  - L
  - would
  - be
  - arbitrarily
  - large
  - and
  - could
  - be
  - damaged
  - your
  - theoretical
  - result
  - .
  - To
  - consider
  - projected
  - step
  - is
  - 'true'
  - ','
  - but
  - the
  - proof
  - without
  - projection
  - -LRB-
  - and
  - then
  - explaining
  - in
  - the
  - end
  - -RRB-
  - should
  - have
  - troubles
  - .
  - From
  - the
  - theoretical
  - results
  - ','
  - it
  - is
  - not
  - clear
  - that
  - momentum
  - parameter
  - affects
  - positively
  - or
  - negatively
  - .
  - In
  - Theorem
  - '3'
  - ','
  - what
  - is
  - the
  - advantage
  - of
  - this
  - convergence
  - compared
  - to
  - SGD
  - '?'
  - It
  - seems
  - that
  - it
  - is
  - not
  - better
  - than
  - SGD
  - .
  - Moreover
  - ','
  - if
  - \
  - mu
  - '='
  - '0'
  - and
  - \
  - gamma
  - '>'
  - '0'
  - ','
  - it
  - seems
  - not
  - able
  - to
  - recover
  - the
  - linear
  - convergence
  - to
  - neighborhood
  - of
  - SGD
  - .
  - Please
  - also
  - notice
  - that
  - ','
  - in
  - this
  - situation
  - ','
  - L
  - also
  - could
  - be
  - large
  - .
  - The
  - topic
  - could
  - be
  - interesting
  - but
  - the
  - contributions
  - are
  - very
  - incremental
  - .
  - At
  - the
  - current
  - state
  - ','
  - I
  - do
  - not
  - support
  - the
  - publications
  - of
  - this
  - paper
  - .
  - R2C1
  - ':'
  - We
  - believe
  - that
  - our
  - results
  - are
  - substantial
  - and
  - important
  - .
  - Our
  - analysis
  - involves
  - some
  - subtle
  - but
  - important
  - steps
  - in
  - dealing
  - with
  - the
  - momentum
  - term
  - in
  - the
  - recursion
  - in
  - Section
  - '4'
  - .
  - This
  - method
  - was
  - not
  - conceived
  - in
  - prior
  - attempts
  - 'on'
  - this
  - problem
  - .
  - We
  - reproduce
  - the
  - following
  - statement
  - from
  - -LSB-
  - Hardt
  - et
  - al.
  - ','
  - Section
  - '7'
  - -RSB-
  - ':'
  - '``'
  - One
  - very
  - important
  - technique
  - that
  - we
  - did
  - not
  - discuss
  - is
  - momentum
  - .
  - However
  - ','
  - it
  - is
  - not
  - clear
  - that
  - momentum
  - adds
  - stability
  - .
  - It
  - is
  - possible
  - that
  - momentum
  - speeds
  - up
  - training
  - but
  - adversely
  - impacts
  - generalization
  - .
  - ''''''
  - Our
  - work
  - is
  - the
  - first
  - successful
  - attempt
  - that
  - establishes
  - that
  - SGMM
  - generalizes
  - ','
  - for
  - the
  - practically
  - important
  - class
  - of
  - strongly
  - convex
  - loss
  - functions
  - .
  - '------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
  - R2C2
  - ':'
  - Please
  - note
  - that
  - we
  - first
  - discuss
  - the
  - proofs
  - without
  - projection
  - to
  - keep
  - the
  - notation
  - uncluttered
  - .
  - We
  - then
  - explain
  - how
  - the
  - proofs
  - can
  - be
  - modified
  - to
  - accommodate
  - projection
  - .
  - We
  - believe
  - this
  - approach
  - is
  - technically
  - sound
  - ','
  - and
  - it
  - helps
  - the
  - readers
  - to
  - better
  - understand
  - the
  - insights
  - in
  - our
  - proofs
  - .
  - We
  - respectfully
  - request
  - that
  - the
  - reviewer
  - point
  - out
  - any
  - specific
  - issue
  - in
  - our
  - proofs
  - such
  - that
  - we
  - can
  - fix
  - it
  - .
  - '------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
  - R2C3
  - ':'
  - To
  - the
  - best
  - of
  - our
  - understanding
  - ','
  - linear
  - convergence
  - happens
  - under
  - a
  - very
  - stringent
  - condition
  - ':'
  - $
  - \
  - Pr
  - \
  - -LCB-
  - \
  - nabla
  - f_i
  - -LRB-
  - x
  - ^
  - '*'
  - -RRB-
  - '='
  - '0'
  - \
  - -RCB-
  - '='
  - '1'
  - $
  - ','
  - \
  - ie
  - $
  - x
  - ^
  - '*'
  - $
  - is
  - a
  - simultaneous
  - minimizer
  - of
  - -LRB-
  - almost
  - -RRB-
  - all
  - $
  - f_i
  - -LRB-
  - x
  - ^
  - '*'
  - -RRB-
  - $
  - -LSB-
  - Needell
  - et
  - al.
  - ','
  - '2014'
  - -RSB-
  - .
  - Such
  - a
  - condition
  - would
  - artificially
  - force
  - that
  - the
  - loss
  - function
  - be
  - simultaneously
  - minimized
  - 'on'
  - each
  - training
  - example
  - .
  - In
  - absence
  - of
  - this
  - condition
  - ','
  - SGD
  - appears
  - to
  - exhibit
  - similar
  - convergence
  - rate
  - as
  - our
  - paper
  - ','
  - albeit
  - under
  - somewhat
  - different
  - assumptions
  - 'on'
  - the
  - loss
  - function
  - .
  - Moreover
  - ','
  - in
  - terms
  - of
  - convergence
  - ','
  - we
  - can
  - not
  - claim
  - that
  - SGMM
  - always
  - outperforms
  - SGM
  - without
  - momentum
  - .
  - For
  - example
  - ','
  - in
  - -LSB-
  - Kidambi
  - et
  - al.
  - ','
  - '2018'
  - -RSB-
  - ','
  - the
  - authors
  - show
  - that
  - there
  - exists
  - linear
  - regression
  - problems
  - for
  - which
  - SGM
  - outperforms
  - SGMM
  - in
  - terms
  - of
  - convergence
  - for
  - any
  - learning
  - rate
  - and
  - momentum
  - parameter
  - .
  - Dear
  - author
  - -LRB-
  - s
  - -RRB-
  - ','
  - Thank
  - you
  - for
  - your
  - response
  - '!'
  - '1'
  - .
  - The
  - set
  - of
  - assumptions
  - -LRB-
  - smooth
  - ','
  - Lipschitz
  - and
  - strongly
  - convex
  - -RRB-
  - is
  - not
  - valid
  - 'on'
  - the
  - whole
  - set
  - R
  - ^
  - d
  - ','
  - for
  - example
  - quadratic
  - function
  - .
  - Your
  - L
  - may
  - be
  - arbitrarily
  - large
  - and
  - your
  - bound
  - in
  - -LRB-
  - '14'
  - -RRB-
  - could
  - be
  - damaged
  - .
  - I
  - do
  - not
  - think
  - you
  - can
  - properly
  - apply
  - the
  - projection
  - step
  - here
  - after
  - deriving
  - -LRB-
  - '14'
  - -RRB-
  - in
  - case
  - of
  - L
  - '-'
  - '>'
  - \
  - infty
  - .
  - '2'
  - .
  - I
  - was
  - asking
  - about
  - the
  - linear
  - convergence
  - to
  - '``'
  - neighborhood
  - ''''''
  - ','
  - not
  - to
  - the
  - '``'
  - optimal
  - solution
  - ''''''
  - .
  - Your
  - theory
  - seems
  - not
  - able
  - to
  - cover
  - this
  - case
  - .
  - Dear
  - reviewer
  - ','
  - '-'
  - To
  - clarify
  - the
  - correctness
  - of
  - our
  - proof
  - ','
  - please
  - note
  - that
  - L
  - in
  - the
  - theorem
  - statement
  - and
  - the
  - proof
  - is
  - bounded
  - due
  - to
  - compactness
  - of
  - the
  - parameter
  - space
  - .
  - We
  - have
  - shown
  - that
  - -LRB-
  - '19'
  - -RRB-
  - holds
  - for
  - projected
  - SGMM
  - .
  - Before
  - -LRB-
  - '19'
  - -RRB-
  - ','
  - we
  - have
  - not
  - used
  - L-Lipschitz
  - .
  - '-'
  - Regarding
  - convergence
  - analysis
  - ','
  - please
  - note
  - that
  - our
  - goal
  - is
  - to
  - find
  - a
  - '*'
  - global
  - '*'
  - convergence
  - bound
  - not
  - a
  - '``'
  - local
  - ''''''
  - one
  - .
  - We
  - know
  - that
  - classical
  - results
  - show
  - that
  - heavy
  - ball
  - momentum
  - achieves
  - linear
  - convergence
  - rate
  - locally
  - .
  - However
  - ','
  - those
  - results
  - are
  - for
  - batch
  - gradient
  - descent
  - not
  - stochastic
  - gradient
  - descent
  - for
  - a
  - general
  - strongly-convex
  - function
  - .
  - Dear
  - author
  - -LRB-
  - s
  - -RRB-
  - ','
  - You
  - are
  - using
  - a
  - stochastic
  - algorithm
  - .
  - There
  - is
  - nothing
  - guarantee
  - that
  - all
  - of
  - your
  - updated
  - iterations
  - are
  - in
  - compact
  - set
  - .
  - It
  - is
  - 'true'
  - that
  - you
  - consider
  - projected
  - step
  - as
  - in
  - -LRB-
  - '5'
  - -RRB-
  - .
  - However
  - ','
  - your
  - proof
  - in
  - Lemma
  - '1'
  - from
  - the
  - beginning
  - to
  - -LRB-
  - '14'
  - -RRB-
  - ','
  - you
  - are
  - using
  - without
  - projection
  - .
  - It
  - would
  - damage
  - your
  - bound
  - in
  - -LRB-
  - '14'
  - -RRB-
  - as
  - I
  - mentioned
  - before
  - that
  - L
  - could
  - be
  - arbitrarily
  - large
  - .
  - Your
  - explanation
  - in
  - the
  - last
  - paragraph
  - of
  - Lemma
  - '1'
  - is
  - not
  - convincing
  - .
  - In
  - order
  - to
  - fix
  - this
  - issue
  - ','
  - you
  - may
  - need
  - to
  - consider
  - your
  - derivation
  - with
  - projected
  - step
  - from
  - the
  - beginning
  - of
  - the
  - proof
  - .
  - Dear
  - reviewer
  - ','
  - Please
  - note
  - that
  - inequalities
  - -LRB-
  - A.
  - '2'
  - -RRB-
  - and
  - -LRB-
  - A.
  - '3'
  - -RRB-
  - -LRB-
  - shown
  - in
  - the
  - proof
  - of
  - Lemma
  - '1'
  - in
  - the
  - supplementary
  - document
  - -RRB-
  - hold
  - for
  - the
  - projected
  - SGMM
  - update
  - -LRB-
  - '5'
  - -RRB-
  - because
  - Euclidean
  - projection
  - does
  - not
  - increase
  - the
  - distance
  - between
  - projected
  - points
  - .
  - We
  - are
  - quite
  - certain
  - that
  - our
  - proof
  - is
  - correct
  - ','
  - since
  - our
  - approach
  - to
  - handle
  - projection
  - is
  - a
  - commonly
  - used
  - technique
  - in
  - existing
  - work
  - .
  - For
  - example
  - ','
  - it
  - is
  - used
  - in
  - -LRB-
  - Hardt
  - et
  - al.
  - ','
  - '2016'
  - -RRB-
  - -LSB-
  - Section
  - '3.4'
  - -RSB-
  - .
  - I
  - am
  - not
  - sure
  - where
  - I
  - could
  - find
  - the
  - supplementary
  - document
  - as
  - you
  - said
  - .
  - The
  - pdf
  - file
  - only
  - contains
  - '9'
  - pages
  - .
  - Dear
  - reviewer
  - ','
  - The
  - supplementary
  - document
  - was
  - submitted
  - along
  - with
  - the
  - original
  - submission
  - .
  - It
  - can
  - be
  - found
  - by
  - clicking
  - the
  - '``'
  - Show
  - revisions
  - ''''''
  - link
  - below
  - the
  - paper
  - title
  - .
  - 'Yes'
  - ','
  - it
  - should
  - hold
  - for
  - projection
  - steps
  - in
  - -LRB-
  - A2
  - -RRB-
  - and
  - -LRB-
  - A3
  - -RRB-
  - .
  - But
  - even
  - so
  - ','
  - your
  - constant
  - L
  - must
  - depend
  - 'on'
  - the
  - radius
  - of
  - the
  - compact
  - set
  - of
  - the
  - parameters
  - .
  - How
  - could
  - you
  - guarantee
  - the
  - optimal
  - solution
  - of
  - the
  - strongly
  - convex
  - problem
  - is
  - always
  - in
  - that
  - compact
  - set
  - '?'
  - In
  - order
  - to
  - increase
  - '``'
  - the
  - chance
  - ''''''
  - of
  - the
  - compact
  - set
  - containing
  - the
  - optimal
  - solution
  - ','
  - you
  - need
  - to
  - '``'
  - increase
  - ''''''
  - the
  - size
  - of
  - the
  - compact
  - set
  - ','
  - which
  - implicitly
  - pushes
  - L
  - be
  - arbitrarily
  - large
  - .
  - Therefore
  - ','
  - it
  - is
  - only
  - possible
  - if
  - you
  - just
  - find
  - the
  - solution
  - from
  - the
  - small
  - compact
  - set
  - -LRB-
  - from
  - which
  - the
  - optimal
  - solution
  - may
  - be
  - very
  - far
  - -RRB-
  - .
  - Our
  - problem
  - setting
  - involves
  - constrained
  - optimization
  - where
  - we
  - seek
  - the
  - optimal
  - solution
  - within
  - a
  - compact
  - set
  - .
  - This
  - constraint
  - is
  - assumed
  - to
  - be
  - given
  - apriori
  - in
  - the
  - problem
  - definition
  - .
  - Such
  - setting
  - have
  - been
  - widely
  - considered
  - in
  - the
  - literature
  - .
  - See
  - for
  - example
  - ':'
  - -LRB-
  - Hardt
  - et
  - al.
  - ','
  - '2016'
  - -RRB-
  - -LSB-
  - Section
  - '3.4'
  - -RSB-
  - -RRB-
  - .
  - Please
  - note
  - that
  - we
  - do
  - not
  - address
  - the
  - unconstrained
  - optimization
  - problem
  - that
  - you
  - mention
  - in
  - your
  - response
  - .
  - Thus
  - we
  - do
  - not
  - need
  - to
  - design
  - the
  - compact
  - set
  - that
  - increases
  - the
  - chance
  - of
  - the
  - compact
  - set
  - containing
  - the
  - optimal
  - solution
  - .
  - We
  - hope
  - this
  - clarifies
  - our
  - problem
  - setup
  - and
  - you
  - are
  - convinced
  - by
  - the
  - technical
  - soundness
  - of
  - our
  - work
  - .
- comment_id: B1eSC5-K0X
  rels:
  - !!python/tuple
    - 0
    - 33
    - 33
    - 45
    - circumstance
  - !!python/tuple
    - 0
    - 45
    - 45
    - 903
    - elaboration
  - !!python/tuple
    - 45
    - 105
    - 105
    - 903
    - topic
  - !!python/tuple
    - 45
    - 59
    - 59
    - 70
    - elaboration
  - !!python/tuple
    - 45
    - 70
    - 70
    - 105
    - elaboration
  - !!python/tuple
    - 105
    - 903
    - 45
    - 105
    - topic
  - !!python/tuple
    - 148
    - 903
    - 105
    - 148
    - concession
  - !!python/tuple
    - 148
    - 158
    - 158
    - 903
    - elaboration
  - !!python/tuple
    - 158
    - 166
    - 166
    - 178
    - elaboration
  - !!python/tuple
    - 166
    - 167
    - 167
    - 178
    - elaboration
  - !!python/tuple
    - 168
    - 178
    - 167
    - 168
    - attribution
  - !!python/tuple
    - 168
    - 175
    - 175
    - 178
    - same_unit
  - !!python/tuple
    - 168
    - 171
    - 171
    - 175
    - elaboration
  - !!python/tuple
    - 175
    - 178
    - 168
    - 175
    - same_unit
  - !!python/tuple
    - 158
    - 178
    - 178
    - 903
    - example
  - !!python/tuple
    - 178
    - 190
    - 190
    - 196
    - elaboration
  - !!python/tuple
    - 178
    - 196
    - 196
    - 903
    - elaboration
  - !!python/tuple
    - 196
    - 237
    - 237
    - 903
    - elaboration
  - !!python/tuple
    - 250
    - 269
    - 237
    - 250
    - concession
  - !!python/tuple
    - 237
    - 245
    - 245
    - 250
    - elaboration
  - !!python/tuple
    - 237
    - 269
    - 269
    - 903
    - elaboration
  - !!python/tuple
    - 269
    - 276
    - 276
    - 285
    - purpose
  - !!python/tuple
    - 269
    - 285
    - 285
    - 903
    - example
  - !!python/tuple
    - 285
    - 307
    - 307
    - 336
    - elaboration
  - !!python/tuple
    - 285
    - 336
    - 336
    - 903
    - elaboration
  - !!python/tuple
    - 336
    - 348
    - 348
    - 359
    - elaboration
  - !!python/tuple
    - 348
    - 350
    - 350
    - 359
    - circumstance
  - !!python/tuple
    - 336
    - 359
    - 359
    - 903
    - elaboration
  - !!python/tuple
    - 359
    - 372
    - 372
    - 379
    - elaboration
  - !!python/tuple
    - 372
    - 378
    - 378
    - 379
    - elaboration
  - !!python/tuple
    - 359
    - 379
    - 379
    - 391
    - elaboration
  - !!python/tuple
    - 379
    - 384
    - 384
    - 391
    - elaboration
  - !!python/tuple
    - 359
    - 391
    - 391
    - 903
    - elaboration
  - !!python/tuple
    - 391
    - 429
    - 429
    - 903
    - contrast
  - !!python/tuple
    - 391
    - 399
    - 399
    - 429
    - purpose
  - !!python/tuple
    - 399
    - 407
    - 407
    - 429
    - same_unit
  - !!python/tuple
    - 407
    - 429
    - 399
    - 407
    - same_unit
  - !!python/tuple
    - 408
    - 429
    - 407
    - 408
    - attribution
  - !!python/tuple
    - 408
    - 422
    - 422
    - 429
    - list
  - !!python/tuple
    - 422
    - 429
    - 408
    - 422
    - list
  - !!python/tuple
    - 429
    - 903
    - 391
    - 429
    - contrast
  - !!python/tuple
    - 429
    - 444
    - 444
    - 903
    - elaboration
  - !!python/tuple
    - 444
    - 473
    - 473
    - 903
    - list
  - !!python/tuple
    - 444
    - 450
    - 450
    - 473
    - elaboration
  - !!python/tuple
    - 450
    - 459
    - 459
    - 473
    - elaboration
  - !!python/tuple
    - 459
    - 465
    - 465
    - 473
    - elaboration
  - !!python/tuple
    - 473
    - 903
    - 444
    - 473
    - list
  - !!python/tuple
    - 473
    - 476
    - 476
    - 903
    - textualorganization
  - !!python/tuple
    - 476
    - 903
    - 473
    - 476
    - textualorganization
  - !!python/tuple
    - 476
    - 483
    - 483
    - 512
    - elaboration
  - !!python/tuple
    - 488
    - 512
    - 483
    - 488
    - attribution
  - !!python/tuple
    - 488
    - 498
    - 498
    - 512
    - elaboration
  - !!python/tuple
    - 476
    - 512
    - 512
    - 903
    - elaboration
  - !!python/tuple
    - 520
    - 545
    - 512
    - 520
    - concession
  - !!python/tuple
    - 520
    - 531
    - 531
    - 545
    - same_unit
  - !!python/tuple
    - 531
    - 545
    - 520
    - 531
    - same_unit
  - !!python/tuple
    - 531
    - 532
    - 532
    - 545
    - circumstance
  - !!python/tuple
    - 532
    - 538
    - 538
    - 545
    - elaboration
  - !!python/tuple
    - 512
    - 545
    - 545
    - 903
    - condition
  - !!python/tuple
    - 576
    - 903
    - 545
    - 576
    - condition
  - !!python/tuple
    - 560
    - 576
    - 545
    - 560
    - condition
  - !!python/tuple
    - 560
    - 561
    - 561
    - 576
    - elaboration
  - !!python/tuple
    - 576
    - 624
    - 624
    - 903
    - textualorganization
  - !!python/tuple
    - 576
    - 579
    - 579
    - 624
    - elaboration
  - !!python/tuple
    - 624
    - 903
    - 576
    - 624
    - textualorganization
  - !!python/tuple
    - 624
    - 630
    - 630
    - 665
    - example
  - !!python/tuple
    - 630
    - 632
    - 632
    - 665
    - elaboration
  - !!python/tuple
    - 632
    - 639
    - 639
    - 665
    - elaboration
  - !!python/tuple
    - 639
    - 645
    - 645
    - 665
    - elaboration
  - !!python/tuple
    - 646
    - 665
    - 645
    - 646
    - attribution
  - !!python/tuple
    - 646
    - 653
    - 653
    - 665
    - same_unit
  - !!python/tuple
    - 653
    - 665
    - 646
    - 653
    - same_unit
  - !!python/tuple
    - 655
    - 665
    - 653
    - 655
    - attribution
  - !!python/tuple
    - 624
    - 665
    - 665
    - 903
    - elaboration
  - !!python/tuple
    - 705
    - 903
    - 665
    - 705
    - concession
  - !!python/tuple
    - 665
    - 666
    - 666
    - 705
    - elaboration
  - !!python/tuple
    - 719
    - 744
    - 705
    - 719
    - attribution
  - !!python/tuple
    - 730
    - 744
    - 719
    - 730
    - concession
  - !!python/tuple
    - 730
    - 736
    - 736
    - 744
    - elaboration
  - !!python/tuple
    - 705
    - 744
    - 744
    - 903
    - elaboration
  - !!python/tuple
    - 749
    - 753
    - 744
    - 749
    - attribution
  - !!python/tuple
    - 744
    - 753
    - 753
    - 903
    - elaboration
  - !!python/tuple
    - 753
    - 759
    - 759
    - 781
    - elaboration
  - !!python/tuple
    - 759
    - 773
    - 773
    - 781
    - elaboration
  - !!python/tuple
    - 753
    - 781
    - 781
    - 801
    - elaboration
  - !!python/tuple
    - 753
    - 801
    - 801
    - 903
    - elaboration
  - !!python/tuple
    - 801
    - 816
    - 816
    - 903
    - elaboration
  - !!python/tuple
    - 822
    - 853
    - 816
    - 822
    - attribution
  - !!python/tuple
    - 822
    - 830
    - 830
    - 853
    - elaboration
  - !!python/tuple
    - 830
    - 841
    - 841
    - 853
    - circumstance
  - !!python/tuple
    - 816
    - 853
    - 853
    - 903
    - elaboration
  - !!python/tuple
    - 853
    - 887
    - 887
    - 903
    - elaboration
  tokens:
  - The
  - primary
  - technical
  - contribution
  - comes
  - from
  - Section
  - '2'
  - ','
  - where
  - it
  - is
  - demonstrated
  - that
  - the
  - normalized
  - back-propagated
  - gradients
  - obtained
  - from
  - a
  - BN
  - layer
  - can
  - be
  - viewed
  - as
  - the
  - residuals
  - of
  - the
  - gradients
  - obtained
  - without
  - BN
  - regressed
  - via
  - a
  - simple
  - two-parameter
  - model
  - of
  - the
  - activations
  - .
  - In
  - some
  - sense
  - though
  - this
  - result
  - is
  - to
  - be
  - expected
  - ','
  - since
  - centering
  - data
  - -LRB-
  - i.e.
  - ','
  - removing
  - the
  - mean
  - as
  - in
  - BN
  - -RRB-
  - can
  - be
  - generically
  - viewed
  - as
  - computing
  - the
  - residuals
  - after
  - a
  - least
  - squares
  - fit
  - of
  - a
  - single
  - constant
  - ','
  - and
  - similarly
  - for
  - de-trending
  - with
  - respect
  - to
  - a
  - single
  - independent
  - variable
  - ','
  - in
  - this
  - case
  - the
  - activations
  - .
  - So
  - I
  - '''m'
  - not
  - sure
  - that
  - Theorem
  - '1'
  - is
  - really
  - that
  - much
  - of
  - an
  - insightful
  - breakthrough
  - ','
  - even
  - if
  - it
  - may
  - be
  - nice
  - to
  - work
  - through
  - the
  - precise
  - details
  - in
  - the
  - specific
  - case
  - of
  - a
  - BN
  - layer
  - and
  - the
  - relationship
  - to
  - gradients
  - .
  - But
  - beyond
  - this
  - a
  - larger
  - issue
  - is
  - as
  - follows
  - ':'
  - This
  - paper
  - is
  - framed
  - as
  - taking
  - a
  - step
  - in
  - explaining
  - why
  - batch
  - normalization
  - -LRB-
  - BN
  - -RRB-
  - works
  - so
  - well
  - .
  - For
  - example
  - ','
  - even
  - the
  - abstract
  - mentions
  - this
  - as
  - an
  - unsettled
  - issue
  - in
  - motivating
  - the
  - proposed
  - analysis
  - .
  - However
  - ','
  - to
  - me
  - the
  - interpretation
  - of
  - BN
  - as
  - introducing
  - a
  - form
  - of
  - least
  - squares
  - fit
  - does
  - not
  - really
  - extend
  - our
  - understanding
  - of
  - why
  - it
  - actually
  - works
  - better
  - in
  - practice
  - ','
  - and
  - this
  - is
  - the
  - biggest
  - disconnect
  - of
  - the
  - paper
  - .
  - The
  - new
  - perspective
  - presented
  - might
  - be
  - another
  - way
  - to
  - interpret
  - BN
  - layers
  - ','
  - but
  - it
  - unfortunately
  - remains
  - mostly
  - unanswered
  - exactly
  - why
  - this
  - new
  - perspective
  - is
  - relevant
  - in
  - actually
  - explaining
  - BN
  - behavior
  - .
  - The
  - presented
  - normalization
  - theory
  - is
  - also
  - used
  - to
  - motivate
  - heuristic
  - modifications
  - to
  - standard
  - BN
  - schemes
  - .
  - For
  - example
  - ','
  - the
  - paper
  - proposed
  - concatenating
  - BN
  - with
  - a
  - layer
  - normalization
  - layer
  - ','
  - demonstrating
  - some
  - modest
  - improvement
  - 'on'
  - CIFAR-10
  - data
  - .
  - But
  - again
  - ','
  - I
  - do
  - n't
  - see
  - how
  - viewing
  - these
  - normalization
  - schemes
  - as
  - least-squares
  - residuals
  - motivates
  - such
  - concatenation
  - any
  - more
  - than
  - the
  - merits
  - of
  - the
  - original
  - versions
  - themselves
  - .
  - Moreover
  - ','
  - it
  - is
  - not
  - even
  - clear
  - that
  - BN+LN
  - is
  - in
  - fact
  - generally
  - better
  - since
  - only
  - a
  - single
  - data
  - set
  - is
  - considered
  - .
  - There
  - are
  - also
  - 'no'
  - comparisons
  - against
  - competing
  - BN
  - modifications
  - such
  - as
  - switch
  - normalization
  - -LRB-
  - Luo
  - et
  - al.
  - '2018'
  - -RRB-
  - which
  - also
  - involves
  - a
  - hybrid
  - method
  - combining
  - aspects
  - of
  - LN
  - and
  - BN
  - .
  - Why
  - not
  - compare
  - against
  - approaches
  - like
  - this
  - '?'
  - To
  - conclude
  - ','
  - in
  - Section
  - '6'
  - the
  - paper
  - asks
  - '``'
  - Why
  - do
  - empirical
  - improvements
  - in
  - neural
  - networks
  - with
  - BN
  - keep
  - the
  - gradient-least-squares
  - residuals
  - and
  - drop
  - the
  - explained
  - portion
  - '?'
  - ''''''
  - But
  - this
  - question
  - is
  - not
  - at
  - all
  - answered
  - but
  - rather
  - deferred
  - to
  - future
  - work
  - .
  - For
  - me
  - this
  - was
  - a
  - disappointment
  - as
  - this
  - would
  - seem
  - to
  - be
  - an
  - essential
  - ingredient
  - for
  - actually
  - developing
  - a
  - meaningful
  - theory
  - for
  - why
  - BN
  - is
  - helpful
  - in
  - practice
  - .
  - Other
  - comments
  - ':'
  - '*'
  - The
  - analysis
  - from
  - Section
  - '2'
  - ','
  - including
  - Theorem
  - '1'
  - ','
  - assume
  - that
  - the
  - BN
  - parameters
  - c
  - and
  - b
  - can
  - be
  - ignored
  - -LRB-
  - presumably
  - this
  - means
  - fixing
  - c
  - '='
  - '1'
  - and
  - b
  - '='
  - '0'
  - -RRB-
  - .
  - I
  - did
  - not
  - carefully
  - check
  - the
  - details
  - ','
  - but
  - do
  - all
  - the
  - same
  - derivations
  - and
  - conclusions
  - still
  - seamlessly
  - go
  - through
  - when
  - these
  - parameters
  - have
  - general
  - values
  - that
  - deviate
  - from
  - this
  - standard
  - initialization
  - '?'
  - If
  - not
  - ','
  - then
  - I
  - do
  - n't
  - really
  - see
  - what
  - is
  - the
  - practical
  - relevance
  - ','
  - since
  - once
  - learning
  - begins
  - ','
  - both
  - b
  - and
  - c
  - will
  - typically
  - shift
  - to
  - arbitrary
  - values
  - .
  - Below
  - eq
  - .
  - -LRB-
  - '1'
  - -RRB-
  - it
  - states
  - that
  - c
  - and
  - b
  - are
  - only
  - ignored
  - for
  - clarity
  - ','
  - but
  - then
  - later
  - I
  - did
  - not
  - see
  - any
  - subsequent
  - discussion
  - to
  - handle
  - the
  - general
  - case
  - ','
  - which
  - is
  - what
  - would
  - be
  - actually
  - needed
  - for
  - explaining
  - BN
  - behavior
  - in
  - practice
  - .
  - '*'
  - Please
  - run
  - a
  - speck-checker
  - .
  - Example
  - ','
  - '``'
  - 'On'
  - some
  - leve
  - ','
  - the
  - matrix
  - gradient
  - '...'
  - ''''''
  - '*'
  - The
  - paper
  - cites
  - -LRB-
  - Lipton
  - and
  - Steinhardt
  - ','
  - '2018'
  - -RRB-
  - in
  - arguing
  - that
  - reasons
  - for
  - the
  - effectiveness
  - of
  - BN
  - are
  - lacking
  - .
  - Indeed
  - -LRB-
  - Lipton
  - and
  - Steinhardt
  - ','
  - '2018'
  - -RRB-
  - criticize
  - the
  - original
  - BN
  - paper
  - for
  - conflating
  - speculation
  - with
  - explanation
  - ','
  - or
  - more
  - precisely
  - ','
  - framing
  - speculation
  - about
  - why
  - BN
  - should
  - be
  - helpful
  - as
  - an
  - actual
  - 'true'
  - explanation
  - without
  - clear
  - evidence
  - .
  - But
  - to
  - me
  - this
  - submission
  - is
  - hovering
  - somewhere
  - in
  - the
  - same
  - category
  - ','
  - speculating
  - that
  - regressing
  - away
  - certain
  - portions
  - of
  - the
  - gradient
  - could
  - be
  - useful
  - but
  - never
  - really
  - providing
  - concrete
  - evidence
  - for
  - why
  - this
  - should
  - offer
  - an
  - improvement
  - .
  - Dear
  - Paper923
  - AnonReviewer2
  - Thank
  - you
  - for
  - your
  - criticisms
  - .
  - We
  - have
  - dialed
  - back
  - our
  - language
  - in
  - the
  - abstract
  - ','
  - the
  - TLDR
  - ','
  - and
  - the
  - main
  - body
  - of
  - the
  - text
  - to
  - reflect
  - your
  - perspective
  - 'on'
  - our
  - work
  - .
  - We
  - apologize
  - for
  - the
  - typos
  - in
  - the
  - earlier
  - version
  - ','
  - and
  - we
  - have
  - been
  - more
  - diligent
  - in
  - this
  - update
  - .
  - Also
  - ','
  - we
  - have
  - clarified
  - some
  - of
  - the
  - language
  - around
  - the
  - downstream
  - affine
  - transformation
  - .
  - Ignoring
  - the
  - affine
  - transform
  - is
  - done
  - without
  - loss
  - of
  - generality
  - ','
  - in
  - the
  - sense
  - that
  - they
  - can
  - be
  - absorbed
  - into
  - the
  - rest
  - of
  - the
  - network
  - without
  - impacting
  - our
  - view
  - of
  - the
  - gradients
  - of
  - the
  - gaussian
  - normalization
  - .
  - Our
  - experiments
  - were
  - meant
  - to
  - be
  - toy-examples
  - of
  - how
  - one
  - might
  - better
  - understand
  - what
  - happens
  - to
  - the
  - gradient
  - regression
  - under
  - adjustments
  - to
  - BN
  - ;
  - ideally
  - ','
  - we
  - would
  - like
  - to
  - design
  - a
  - new
  - normalization
  - that
  - outperforms
  - switch
  - normalization
  - ','
  - but
  - we
  - have
  - not
  - been
  - able
  - to
  - do
  - that
  - here
  - .
- comment_id: B1ekBnl3yV
  rels:
  - !!python/tuple
    - 0
    - 10
    - 10
    - 15
    - elaboration
  - !!python/tuple
    - 0
    - 15
    - 15
    - 27
    - elaboration
  - !!python/tuple
    - 15
    - 19
    - 19
    - 27
    - purpose
  - !!python/tuple
    - 0
    - 27
    - 27
    - 983
    - elaboration
  - !!python/tuple
    - 27
    - 32
    - 32
    - 61
    - elaboration
  - !!python/tuple
    - 32
    - 41
    - 41
    - 61
    - elaboration
  - !!python/tuple
    - 41
    - 57
    - 57
    - 61
    - elaboration
  - !!python/tuple
    - 27
    - 61
    - 61
    - 983
    - elaboration
  - !!python/tuple
    - 61
    - 67
    - 67
    - 81
    - elaboration
  - !!python/tuple
    - 67
    - 72
    - 72
    - 81
    - elaboration
  - !!python/tuple
    - 61
    - 81
    - 81
    - 983
    - elaboration
  - !!python/tuple
    - 88
    - 117
    - 81
    - 88
    - attribution
  - !!python/tuple
    - 100
    - 117
    - 88
    - 100
    - attribution
  - !!python/tuple
    - 100
    - 112
    - 112
    - 117
    - elaboration
  - !!python/tuple
    - 81
    - 117
    - 117
    - 983
    - elaboration
  - !!python/tuple
    - 120
    - 135
    - 117
    - 120
    - attribution
  - !!python/tuple
    - 117
    - 135
    - 135
    - 983
    - elaboration
  - !!python/tuple
    - 135
    - 141
    - 141
    - 147
    - elaboration
  - !!python/tuple
    - 135
    - 147
    - 147
    - 983
    - elaboration
  - !!python/tuple
    - 147
    - 165
    - 165
    - 191
    - elaboration
  - !!python/tuple
    - 165
    - 169
    - 169
    - 191
    - elaboration
  - !!python/tuple
    - 147
    - 191
    - 191
    - 213
    - elaboration
  - !!python/tuple
    - 191
    - 192
    - 192
    - 213
    - elaboration
  - !!python/tuple
    - 192
    - 203
    - 203
    - 213
    - elaboration
  - !!python/tuple
    - 203
    - 204
    - 204
    - 213
    - elaboration
  - !!python/tuple
    - 147
    - 213
    - 213
    - 983
    - elaboration
  - !!python/tuple
    - 213
    - 231
    - 231
    - 254
    - elaboration
  - !!python/tuple
    - 231
    - 238
    - 238
    - 254
    - elaboration
  - !!python/tuple
    - 238
    - 246
    - 246
    - 254
    - list
  - !!python/tuple
    - 246
    - 254
    - 238
    - 246
    - list
  - !!python/tuple
    - 213
    - 254
    - 254
    - 983
    - elaboration
  - !!python/tuple
    - 254
    - 266
    - 266
    - 286
    - same_unit
  - !!python/tuple
    - 254
    - 260
    - 260
    - 266
    - elaboration
  - !!python/tuple
    - 266
    - 286
    - 254
    - 266
    - same_unit
  - !!python/tuple
    - 266
    - 281
    - 281
    - 286
    - purpose
  - !!python/tuple
    - 254
    - 286
    - 286
    - 983
    - elaboration
  - !!python/tuple
    - 286
    - 287
    - 287
    - 290
    - elaboration
  - !!python/tuple
    - 286
    - 290
    - 290
    - 303
    - elaboration
  - !!python/tuple
    - 286
    - 303
    - 303
    - 315
    - elaboration
  - !!python/tuple
    - 303
    - 311
    - 311
    - 315
    - elaboration
  - !!python/tuple
    - 286
    - 315
    - 315
    - 983
    - elaboration
  - !!python/tuple
    - 315
    - 329
    - 329
    - 347
    - same_unit
  - !!python/tuple
    - 315
    - 325
    - 325
    - 329
    - elaboration
  - !!python/tuple
    - 329
    - 347
    - 315
    - 329
    - same_unit
  - !!python/tuple
    - 329
    - 330
    - 330
    - 347
    - elaboration
  - !!python/tuple
    - 330
    - 341
    - 341
    - 347
    - elaboration
  - !!python/tuple
    - 315
    - 347
    - 347
    - 983
    - elaboration
  - !!python/tuple
    - 347
    - 353
    - 353
    - 386
    - condition
  - !!python/tuple
    - 362
    - 386
    - 353
    - 362
    - condition
  - !!python/tuple
    - 362
    - 385
    - 385
    - 386
    - same_unit
  - !!python/tuple
    - 362
    - 373
    - 373
    - 385
    - elaboration
  - !!python/tuple
    - 385
    - 386
    - 362
    - 385
    - same_unit
  - !!python/tuple
    - 347
    - 386
    - 386
    - 983
    - elaboration
  - !!python/tuple
    - 391
    - 399
    - 386
    - 391
    - attribution
  - !!python/tuple
    - 386
    - 399
    - 399
    - 983
    - elaboration
  - !!python/tuple
    - 399
    - 408
    - 408
    - 426
    - purpose
  - !!python/tuple
    - 408
    - 416
    - 416
    - 426
    - elaboration
  - !!python/tuple
    - 416
    - 425
    - 425
    - 426
    - purpose
  - !!python/tuple
    - 399
    - 426
    - 426
    - 434
    - elaboration
  - !!python/tuple
    - 427
    - 434
    - 426
    - 427
    - attribution
  - !!python/tuple
    - 399
    - 434
    - 434
    - 983
    - elaboration
  - !!python/tuple
    - 434
    - 442
    - 442
    - 983
    - reason
  - !!python/tuple
    - 442
    - 451
    - 451
    - 462
    - elaboration
  - !!python/tuple
    - 442
    - 462
    - 462
    - 983
    - elaboration
  - !!python/tuple
    - 462
    - 467
    - 467
    - 477
    - purpose
  - !!python/tuple
    - 467
    - 470
    - 470
    - 477
    - elaboration
  - !!python/tuple
    - 462
    - 477
    - 477
    - 983
    - elaboration
  - !!python/tuple
    - 480
    - 495
    - 477
    - 480
    - attribution
  - !!python/tuple
    - 480
    - 489
    - 489
    - 495
    - list
  - !!python/tuple
    - 489
    - 495
    - 480
    - 489
    - list
  - !!python/tuple
    - 477
    - 495
    - 495
    - 983
    - elaboration
  - !!python/tuple
    - 495
    - 499
    - 499
    - 507
    - elaboration
  - !!python/tuple
    - 495
    - 507
    - 507
    - 983
    - elaboration
  - !!python/tuple
    - 507
    - 514
    - 514
    - 557
    - elaboration
  - !!python/tuple
    - 514
    - 520
    - 520
    - 557
    - elaboration
  - !!python/tuple
    - 507
    - 557
    - 557
    - 983
    - elaboration
  - !!python/tuple
    - 557
    - 578
    - 578
    - 983
    - list
  - !!python/tuple
    - 557
    - 562
    - 562
    - 578
    - purpose
  - !!python/tuple
    - 562
    - 567
    - 567
    - 578
    - list
  - !!python/tuple
    - 567
    - 578
    - 562
    - 567
    - list
  - !!python/tuple
    - 569
    - 578
    - 567
    - 569
    - attribution
  - !!python/tuple
    - 578
    - 983
    - 557
    - 578
    - list
  - !!python/tuple
    - 578
    - 585
    - 585
    - 983
    - list
  - !!python/tuple
    - 579
    - 585
    - 578
    - 579
    - attribution
  - !!python/tuple
    - 585
    - 983
    - 578
    - 585
    - list
  - !!python/tuple
    - 585
    - 590
    - 590
    - 983
    - list
  - !!python/tuple
    - 590
    - 983
    - 585
    - 590
    - list
  - !!python/tuple
    - 590
    - 592
    - 592
    - 617
    - elaboration
  - !!python/tuple
    - 592
    - 599
    - 599
    - 617
    - purpose
  - !!python/tuple
    - 599
    - 604
    - 604
    - 617
    - elaboration
  - !!python/tuple
    - 604
    - 606
    - 606
    - 617
    - elaboration
  - !!python/tuple
    - 590
    - 617
    - 617
    - 983
    - elaboration
  - !!python/tuple
    - 617
    - 629
    - 629
    - 669
    - list
  - !!python/tuple
    - 617
    - 625
    - 625
    - 629
    - elaboration
  - !!python/tuple
    - 629
    - 669
    - 617
    - 629
    - list
  - !!python/tuple
    - 629
    - 640
    - 640
    - 669
    - list
  - !!python/tuple
    - 640
    - 669
    - 629
    - 640
    - list
  - !!python/tuple
    - 640
    - 641
    - 641
    - 669
    - elaboration
  - !!python/tuple
    - 617
    - 669
    - 669
    - 983
    - elaboration
  - !!python/tuple
    - 669
    - 702
    - 702
    - 983
    - elaboration
  - !!python/tuple
    - 702
    - 807
    - 807
    - 983
    - list
  - !!python/tuple
    - 702
    - 704
    - 704
    - 729
    - elaboration
  - !!python/tuple
    - 721
    - 729
    - 704
    - 721
    - attribution
  - !!python/tuple
    - 704
    - 709
    - 709
    - 721
    - elaboration
  - !!python/tuple
    - 721
    - 725
    - 725
    - 729
    - circumstance
  - !!python/tuple
    - 702
    - 729
    - 729
    - 807
    - elaboration
  - !!python/tuple
    - 729
    - 731
    - 731
    - 742
    - elaboration
  - !!python/tuple
    - 729
    - 742
    - 742
    - 807
    - elaboration
  - !!python/tuple
    - 742
    - 764
    - 764
    - 770
    - elaboration
  - !!python/tuple
    - 742
    - 770
    - 770
    - 807
    - elaboration
  - !!python/tuple
    - 770
    - 779
    - 779
    - 807
    - elaboration
  - !!python/tuple
    - 779
    - 785
    - 785
    - 807
    - elaboration
  - !!python/tuple
    - 807
    - 983
    - 702
    - 807
    - list
  - !!python/tuple
    - 807
    - 816
    - 816
    - 983
    - list
  - !!python/tuple
    - 807
    - 809
    - 809
    - 816
    - elaboration
  - !!python/tuple
    - 816
    - 983
    - 807
    - 816
    - list
  - !!python/tuple
    - 816
    - 838
    - 838
    - 983
    - list
  - !!python/tuple
    - 816
    - 818
    - 818
    - 838
    - elaboration
  - !!python/tuple
    - 838
    - 983
    - 816
    - 838
    - list
  - !!python/tuple
    - 838
    - 862
    - 862
    - 983
    - list
  - !!python/tuple
    - 862
    - 983
    - 838
    - 862
    - list
  - !!python/tuple
    - 862
    - 863
    - 863
    - 864
    - elaboration
  - !!python/tuple
    - 862
    - 864
    - 864
    - 872
    - elaboration
  - !!python/tuple
    - 862
    - 872
    - 872
    - 983
    - elaboration
  - !!python/tuple
    - 872
    - 876
    - 876
    - 983
    - textualorganization
  - !!python/tuple
    - 876
    - 983
    - 872
    - 876
    - textualorganization
  - !!python/tuple
    - 876
    - 896
    - 896
    - 983
    - list
  - !!python/tuple
    - 876
    - 886
    - 886
    - 896
    - elaboration
  - !!python/tuple
    - 896
    - 983
    - 876
    - 896
    - list
  - !!python/tuple
    - 896
    - 897
    - 897
    - 902
    - elaboration
  - !!python/tuple
    - 896
    - 902
    - 902
    - 983
    - elaboration
  - !!python/tuple
    - 902
    - 906
    - 906
    - 922
    - purpose
  - !!python/tuple
    - 906
    - 912
    - 912
    - 922
    - elaboration
  - !!python/tuple
    - 902
    - 922
    - 922
    - 983
    - elaboration
  - !!python/tuple
    - 922
    - 924
    - 924
    - 955
    - elaboration
  - !!python/tuple
    - 924
    - 941
    - 941
    - 955
    - same_unit
  - !!python/tuple
    - 924
    - 934
    - 934
    - 941
    - elaboration
  - !!python/tuple
    - 941
    - 955
    - 924
    - 941
    - same_unit
  - !!python/tuple
    - 941
    - 946
    - 946
    - 955
    - elaboration
  - !!python/tuple
    - 922
    - 955
    - 955
    - 983
    - elaboration
  - !!python/tuple
    - 955
    - 957
    - 957
    - 965
    - purpose
  - !!python/tuple
    - 957
    - 961
    - 961
    - 965
    - elaboration
  - !!python/tuple
    - 955
    - 965
    - 965
    - 983
    - elaboration
  - !!python/tuple
    - 965
    - 969
    - 969
    - 983
    - elaboration
  - !!python/tuple
    - 969
    - 974
    - 974
    - 983
    - same_unit
  - !!python/tuple
    - 971
    - 974
    - 969
    - 971
    - attribution
  - !!python/tuple
    - 974
    - 983
    - 969
    - 974
    - same_unit
  tokens:
  - '#'
  - Summary
  - This
  - paper
  - proposes
  - a
  - simple
  - regularizer
  - for
  - RL
  - which
  - encourages
  - the
  - state
  - representations
  - learned
  - by
  - neural
  - networks
  - to
  - be
  - more
  - discriminative
  - across
  - different
  - observations
  - .
  - The
  - main
  - idea
  - is
  - to
  - -LRB-
  - implicitly
  - -RRB-
  - measure
  - the
  - rank
  - of
  - the
  - matrix
  - which
  - is
  - constructed
  - from
  - a
  - sequence
  - of
  - observations
  - and
  - state
  - feature
  - vectors
  - and
  - encourage
  - the
  - rank
  - to
  - be
  - high
  - .
  - This
  - paper
  - introduces
  - three
  - different
  - objectives
  - to
  - implement
  - the
  - same
  - idea
  - -LRB-
  - increasing
  - the
  - rank
  - of
  - the
  - matrix
  - -RRB-
  - .
  - The
  - experimental
  - results
  - 'on'
  - Atari
  - games
  - show
  - that
  - this
  - regularizer
  - improves
  - A3C
  - 'on'
  - most
  - of
  - the
  - games
  - and
  - show
  - that
  - the
  - learned
  - representations
  - with
  - the
  - proposed
  - regularizer
  - has
  - a
  - high
  - rank
  - compared
  - to
  - the
  - baseline
  - .
  - -LSB-
  - Pros
  - -RSB-
  - '-'
  - Makes
  - an
  - interesting
  - point
  - about
  - the
  - correlation
  - between
  - RL
  - performance
  - and
  - state
  - representations
  - .
  - '-'
  - Proposes
  - a
  - simple
  - objective
  - function
  - that
  - gives
  - strong
  - empirical
  - results
  - .
  - -LSB-
  - Cons
  - -RSB-
  - '-'
  - Needs
  - more
  - empirical
  - evidences
  - to
  - better
  - support
  - the
  - main
  - hypothesis
  - of
  - the
  - paper
  - .
  - '#'
  - Novelty
  - and
  - Significance
  - '-'
  - This
  - paper
  - makes
  - an
  - interesting
  - observation
  - about
  - the
  - correlation
  - between
  - the
  - RL
  - performance
  - and
  - the
  - how
  - discriminative
  - the
  - learned
  - features
  - .
  - '-'
  - To
  - verify
  - it
  - ','
  - this
  - paper
  - proposes
  - a
  - new
  - and
  - simple
  - regularizer
  - which
  - improves
  - the
  - performance
  - across
  - many
  - Atari
  - games
  - .
  - '#'
  - Quality
  - and
  - Experiment
  - '-'
  - The
  - main
  - hypothesis
  - of
  - this
  - paper
  - is
  - that
  - the
  - expressiveness
  - of
  - the
  - features
  - -LRB-
  - specifically
  - the
  - rank
  - of
  - the
  - matrix
  - that
  - consists
  - of
  - a
  - sequence
  - of
  - features
  - -RRB-
  - and
  - its
  - RL
  - performance
  - is
  - highly
  - correlated
  - .
  - Although
  - this
  - paper
  - showed
  - some
  - plots
  - -LRB-
  - Figure
  - '2'
  - ','
  - '6'
  - -RRB-
  - to
  - verify
  - this
  - ','
  - a
  - more
  - extensive
  - statistical
  - test
  - or
  - experiments
  - would
  - be
  - more
  - convincing
  - to
  - show
  - the
  - hypothesis
  - .
  - Examples
  - would
  - be
  - ':'
  - '1'
  - -RRB-
  - Measuring
  - the
  - correlation
  - between
  - the
  - two
  - across
  - all
  - Atari
  - games
  - .
  - '2'
  - -RRB-
  - An
  - ablation
  - study
  - 'on'
  - the
  - hyperparameter
  - -LRB-
  - alpha
  - -RRB-
  - .
  - '3'
  - -RRB-
  - Learning
  - state
  - representations
  - just
  - from
  - the
  - reconstruction
  - task
  - -LRB-
  - without
  - RL
  - -RRB-
  - with/without
  - the
  - proposed
  - regularizer
  - and
  - separately
  - learning
  - policies
  - 'on'
  - top
  - of
  - that
  - -LRB-
  - with
  - fixed
  - representations
  - -RRB-
  - .
  - It
  - would
  - be
  - much
  - more
  - convincing
  - if
  - the
  - regularizer
  - helps
  - even
  - in
  - this
  - setup
  - ','
  - because
  - this
  - would
  - show
  - the
  - general
  - effect
  - of
  - the
  - expressiveness
  - term
  - -LRB-
  - by
  - removing
  - the
  - effect
  - of
  - RL
  - algorithm
  - 'on'
  - the
  - representation
  - -RRB-
  - .
  - '-'
  - This
  - paper
  - only
  - reports
  - '``'
  - relative
  - ''''''
  - performances
  - to
  - the
  - baseline
  - .
  - Though
  - it
  - looks
  - strong
  - ','
  - it
  - is
  - also
  - important
  - to
  - report
  - the
  - absolute
  - performances
  - in
  - Atari
  - games
  - -LRB-
  - e.g.
  - ','
  - median
  - human-normalized
  - score
  - ','
  - etc
  - -RRB-
  - to
  - show
  - how
  - significant
  - the
  - performance
  - gap
  - is
  - .
  - '-'
  - The
  - results
  - with
  - DQN
  - are
  - not
  - convincing
  - because
  - the
  - agents
  - are
  - trained
  - only
  - for
  - 20M
  - frames
  - -LRB-
  - compared
  - to
  - 200M
  - frames
  - in
  - many
  - other
  - papers
  - -RRB-
  - .
  - It
  - is
  - not
  - much
  - meaningful
  - to
  - compare
  - performances
  - 'on'
  - such
  - a
  - short
  - training
  - regime
  - .
  - I
  - would
  - suggest
  - running
  - longer
  - or
  - removing
  - this
  - result
  - from
  - the
  - paper
  - and
  - focusing
  - 'on'
  - more
  - analysis
  - .
  - '#'
  - Clarity
  - and
  - Presentation
  - '-'
  - Figure
  - 1a
  - is
  - not
  - much
  - insightful
  - .
  - It
  - is
  - not
  - surprising
  - that
  - the
  - representations
  - that
  - led
  - to
  - a
  - poor
  - policy
  - -LRB-
  - which
  - achieves
  - '0'
  - reward
  - -RRB-
  - are
  - much
  - less
  - discriminative
  - given
  - five
  - situations
  - with
  - five
  - distinct
  - optimal
  - actions
  - ','
  - because
  - the
  - the
  - policy
  - has
  - 'no'
  - idea
  - which
  - action
  - is
  - better
  - than
  - the
  - other
  - in
  - such
  - situations
  - .
  - It
  - would
  - be
  - more
  - informative
  - to
  - pick
  - N
  - consecutive
  - frames
  - and
  - show
  - how
  - scattered
  - they
  - are
  - in
  - the
  - embedding
  - space
  - .
  - Thank
  - you
  - for
  - the
  - helpful
  - comments
  - .
  - Here
  - are
  - our
  - responses
  - .
  - Q1
  - ':'
  - A
  - more
  - extensive
  - statistical
  - test
  - or
  - experiments
  - to
  - convince
  - the
  - hypothesis
  - ':'
  - a
  - -RRB-
  - Measuring
  - the
  - correlation
  - between
  - the
  - two
  - across
  - all
  - Atari
  - games
  - .
  - b
  - -RRB-
  - An
  - ablation
  - study
  - 'on'
  - the
  - hyperparameter
  - -LRB-
  - alpha
  - -RRB-
  - .
  - c
  - -RRB-
  - Learning
  - state
  - representations
  - separately
  - '...'
  - A1
  - ':'
  - a
  - -RRB-
  - Curves
  - tracking
  - testing
  - rewards
  - and
  - the
  - expressiveness
  - in
  - Fig.
  - '2'
  - and
  - Fig.
  - '6'
  - have
  - demonstrated
  - their
  - correlation
  - over
  - multiple
  - Atari
  - games
  - ','
  - which
  - have
  - shown
  - some
  - statistical
  - significance
  - .
  - In
  - order
  - to
  - make
  - the
  - results
  - more
  - convincing
  - ','
  - we
  - have
  - added
  - the
  - study
  - about
  - relationship
  - between
  - model
  - performance
  - and
  - representations
  - across
  - all
  - Atari
  - games
  - in
  - Section
  - 2.2.1
  - in
  - our
  - updated
  - version
  - .
  - b
  - -RRB-
  - The
  - ablation
  - study
  - 'on'
  - hyperparameter
  - -LRB-
  - alpha
  - -RRB-
  - is
  - shown
  - in
  - Appendix
  - B
  - now
  - ','
  - which
  - shows
  - that
  - improvements
  - are
  - stable
  - with
  - different
  - alpha
  - .
  - c
  - -RRB-
  - Our
  - proposed
  - method
  - is
  - designed
  - and
  - applied
  - to
  - RL
  - algorithms
  - .
  - Although
  - one
  - RL
  - agent
  - model
  - can
  - be
  - viewed
  - as
  - the
  - state
  - extractor
  - and
  - the
  - policy
  - learning
  - part
  - ','
  - they
  - are
  - trained
  - end-to-end
  - within
  - the
  - RL
  - algorithm
  - loop
  - .
  - These
  - two
  - parts
  - influence
  - each
  - other
  - during
  - training
  - .
  - Besides
  - ','
  - we
  - do
  - not
  - emphasis
  - which
  - part
  - works
  - in
  - our
  - proposed
  - method
  - indeed
  - ','
  - we
  - just
  - make
  - a
  - correlation
  - between
  - performances
  - and
  - the
  - expressiveness
  - of
  - representations
  - .
  - Q2
  - ':'
  - Detailed
  - questions
  - for
  - the
  - experimental
  - results
  - .
  - A2
  - ':'
  - '1'
  - -RRB-
  - We
  - also
  - provide
  - the
  - absolute
  - performances
  - of
  - all
  - Atari
  - games
  - in
  - Appendix
  - C
  - in
  - the
  - updated
  - version
  - .
  - '2'
  - -RRB-
  - For
  - DQN
  - ','
  - we
  - finished
  - the
  - experiments
  - for
  - 200M
  - frames
  - and
  - updated
  - the
  - results
  - in
  - Section
  - '4.4'
  - in
  - the
  - new
  - version
  - .
  - Q3
  - ':'
  - '``'
  - Figure
  - 1a
  - is
  - not
  - much
  - insightful
  - .
  - '...'
  - ''''''
  - A3
  - ':'
  - Fig.
  - 1a
  - has
  - been
  - refined
  - in
  - the
  - following
  - way
  - ':'
  - selected
  - frames
  - and
  - their
  - corresponding
  - embedding
  - points
  - are
  - removed
  - .
  - Thanks
  - for
  - addressing
  - my
  - questions
  - .
  - The
  - revised
  - paper
  - seems
  - to
  - provide
  - a
  - bit
  - more
  - evidence
  - about
  - the
  - relationship
  - between
  - expressiveness
  - and
  - performance
  - in
  - RL
  - .
  - One
  - thing
  - that
  - I
  - realized
  - is
  - that
  - the
  - authors
  - used
  - different
  - hyperparameters
  - -LRB-
  - rank
  - regularization
  - term
  - ','
  - alpha
  - -RRB-
  - for
  - each
  - Atari
  - game
  - ','
  - which
  - makes
  - the
  - proposed
  - algorithm
  - look
  - less
  - practical
  - .
  - I
  - decided
  - to
  - keep
  - my
  - score
  - -LRB-
  - '6'
  - -RRB-
  - .
  - One
  - minor
  - comment
  - ':'
  - Please
  - compute
  - '``'
  - human
  - ''''''
  - '-'
  - normalized
  - score
  - in
  - Table
  - '5'
  - and
  - '6'
  - .
- comment_id: B1e3qRw3p7
  rels:
  - !!python/tuple
    - 0
    - 1077
    - 1077
    - 1227
    - topic
  - !!python/tuple
    - 0
    - 8
    - 8
    - 18
    - means
  - !!python/tuple
    - 0
    - 18
    - 18
    - 1077
    - elaboration
  - !!python/tuple
    - 18
    - 29
    - 29
    - 38
    - elaboration
  - !!python/tuple
    - 18
    - 38
    - 38
    - 1077
    - elaboration
  - !!python/tuple
    - 38
    - 42
    - 42
    - 53
    - circumstance
  - !!python/tuple
    - 42
    - 47
    - 47
    - 53
    - elaboration
  - !!python/tuple
    - 38
    - 53
    - 53
    - 1077
    - elaboration
  - !!python/tuple
    - 56
    - 69
    - 53
    - 56
    - attribution
  - !!python/tuple
    - 53
    - 69
    - 69
    - 1077
    - elaboration
  - !!python/tuple
    - 69
    - 88
    - 88
    - 105
    - elaboration
  - !!python/tuple
    - 88
    - 92
    - 92
    - 105
    - purpose
  - !!python/tuple
    - 92
    - 97
    - 97
    - 105
    - condition
  - !!python/tuple
    - 69
    - 105
    - 105
    - 1077
    - elaboration
  - !!python/tuple
    - 106
    - 119
    - 105
    - 106
    - attribution
  - !!python/tuple
    - 106
    - 110
    - 110
    - 119
    - same_unit
  - !!python/tuple
    - 106
    - 107
    - 107
    - 110
    - elaboration
  - !!python/tuple
    - 110
    - 119
    - 106
    - 110
    - same_unit
  - !!python/tuple
    - 105
    - 119
    - 119
    - 1077
    - elaboration
  - !!python/tuple
    - 125
    - 139
    - 119
    - 125
    - attribution
  - !!python/tuple
    - 125
    - 132
    - 132
    - 139
    - purpose
  - !!python/tuple
    - 119
    - 139
    - 139
    - 1077
    - elaboration
  - !!python/tuple
    - 139
    - 141
    - 141
    - 1077
    - elaboration
  - !!python/tuple
    - 141
    - 168
    - 168
    - 183
    - elaboration
  - !!python/tuple
    - 141
    - 183
    - 183
    - 209
    - elaboration
  - !!python/tuple
    - 141
    - 209
    - 209
    - 1077
    - elaboration
  - !!python/tuple
    - 209
    - 217
    - 217
    - 1077
    - elaboration
  - !!python/tuple
    - 217
    - 220
    - 220
    - 251
    - elaboration
  - !!python/tuple
    - 220
    - 242
    - 242
    - 251
    - elaboration
  - !!python/tuple
    - 217
    - 251
    - 251
    - 1077
    - elaboration
  - !!python/tuple
    - 251
    - 260
    - 260
    - 270
    - purpose
  - !!python/tuple
    - 251
    - 270
    - 270
    - 1077
    - elaboration
  - !!python/tuple
    - 271
    - 284
    - 270
    - 271
    - attribution
  - !!python/tuple
    - 271
    - 276
    - 276
    - 284
    - condition
  - !!python/tuple
    - 270
    - 284
    - 284
    - 1077
    - elaboration
  - !!python/tuple
    - 284
    - 301
    - 301
    - 1077
    - elaboration
  - !!python/tuple
    - 301
    - 308
    - 308
    - 1077
    - elaboration
  - !!python/tuple
    - 308
    - 312
    - 312
    - 327
    - purpose
  - !!python/tuple
    - 308
    - 327
    - 327
    - 1077
    - elaboration
  - !!python/tuple
    - 332
    - 342
    - 327
    - 332
    - attribution
  - !!python/tuple
    - 332
    - 337
    - 337
    - 342
    - circumstance
  - !!python/tuple
    - 327
    - 342
    - 342
    - 426
    - elaboration
  - !!python/tuple
    - 342
    - 365
    - 365
    - 426
    - elaboration
  - !!python/tuple
    - 365
    - 385
    - 385
    - 397
    - same_unit
  - !!python/tuple
    - 365
    - 369
    - 369
    - 385
    - elaboration
  - !!python/tuple
    - 385
    - 397
    - 365
    - 385
    - same_unit
  - !!python/tuple
    - 365
    - 397
    - 397
    - 426
    - elaboration
  - !!python/tuple
    - 400
    - 426
    - 397
    - 400
    - attribution
  - !!python/tuple
    - 400
    - 419
    - 419
    - 426
    - elaboration
  - !!python/tuple
    - 327
    - 426
    - 426
    - 1077
    - elaboration
  - !!python/tuple
    - 426
    - 436
    - 436
    - 1077
    - elaboration
  - !!python/tuple
    - 436
    - 448
    - 448
    - 1077
    - elaboration
  - !!python/tuple
    - 448
    - 459
    - 459
    - 469
    - elaboration
  - !!python/tuple
    - 448
    - 469
    - 469
    - 1077
    - elaboration
  - !!python/tuple
    - 469
    - 482
    - 482
    - 1077
    - elaboration
  - !!python/tuple
    - 482
    - 484
    - 484
    - 497
    - elaboration
  - !!python/tuple
    - 482
    - 497
    - 497
    - 1077
    - elaboration
  - !!python/tuple
    - 497
    - 508
    - 508
    - 549
    - elaboration
  - !!python/tuple
    - 508
    - 519
    - 519
    - 549
    - elaboration
  - !!python/tuple
    - 519
    - 521
    - 521
    - 549
    - elaboration
  - !!python/tuple
    - 521
    - 529
    - 529
    - 549
    - elaboration
  - !!python/tuple
    - 497
    - 549
    - 549
    - 1077
    - elaboration
  - !!python/tuple
    - 549
    - 556
    - 556
    - 570
    - elaboration
  - !!python/tuple
    - 556
    - 558
    - 558
    - 570
    - elaboration
  - !!python/tuple
    - 558
    - 559
    - 559
    - 570
    - elaboration
  - !!python/tuple
    - 559
    - 565
    - 565
    - 570
    - elaboration
  - !!python/tuple
    - 549
    - 570
    - 570
    - 1077
    - elaboration
  - !!python/tuple
    - 570
    - 581
    - 581
    - 1077
    - elaboration
  - !!python/tuple
    - 581
    - 591
    - 591
    - 618
    - elaboration
  - !!python/tuple
    - 591
    - 614
    - 614
    - 618
    - elaboration
  - !!python/tuple
    - 581
    - 618
    - 618
    - 1077
    - elaboration
  - !!python/tuple
    - 618
    - 627
    - 627
    - 645
    - elaboration
  - !!python/tuple
    - 627
    - 633
    - 633
    - 645
    - attribution
  - !!python/tuple
    - 633
    - 636
    - 636
    - 645
    - purpose
  - !!python/tuple
    - 618
    - 645
    - 645
    - 1077
    - elaboration
  - !!python/tuple
    - 647
    - 651
    - 645
    - 647
    - attribution
  - !!python/tuple
    - 645
    - 651
    - 651
    - 1077
    - elaboration
  - !!python/tuple
    - 651
    - 652
    - 652
    - 653
    - elaboration
  - !!python/tuple
    - 651
    - 653
    - 653
    - 667
    - elaboration
  - !!python/tuple
    - 653
    - 659
    - 659
    - 667
    - same_unit
  - !!python/tuple
    - 659
    - 667
    - 653
    - 659
    - same_unit
  - !!python/tuple
    - 651
    - 667
    - 667
    - 682
    - elaboration
  - !!python/tuple
    - 651
    - 682
    - 682
    - 1077
    - elaboration
  - !!python/tuple
    - 682
    - 683
    - 683
    - 684
    - elaboration
  - !!python/tuple
    - 682
    - 684
    - 684
    - 708
    - elaboration
  - !!python/tuple
    - 682
    - 708
    - 708
    - 1077
    - elaboration
  - !!python/tuple
    - 708
    - 716
    - 716
    - 741
    - elaboration
  - !!python/tuple
    - 716
    - 729
    - 729
    - 741
    - elaboration
  - !!python/tuple
    - 729
    - 734
    - 734
    - 741
    - elaboration
  - !!python/tuple
    - 708
    - 741
    - 741
    - 1077
    - elaboration
  - !!python/tuple
    - 741
    - 760
    - 760
    - 763
    - elaboration
  - !!python/tuple
    - 741
    - 763
    - 763
    - 828
    - elaboration
  - !!python/tuple
    - 763
    - 784
    - 784
    - 796
    - same_unit
  - !!python/tuple
    - 763
    - 778
    - 778
    - 784
    - elaboration
  - !!python/tuple
    - 784
    - 796
    - 763
    - 784
    - same_unit
  - !!python/tuple
    - 763
    - 796
    - 796
    - 828
    - elaboration
  - !!python/tuple
    - 796
    - 805
    - 805
    - 828
    - list
  - !!python/tuple
    - 796
    - 800
    - 800
    - 805
    - elaboration
  - !!python/tuple
    - 805
    - 828
    - 796
    - 805
    - list
  - !!python/tuple
    - 805
    - 822
    - 822
    - 828
    - attribution
  - !!python/tuple
    - 741
    - 828
    - 828
    - 1077
    - elaboration
  - !!python/tuple
    - 828
    - 851
    - 851
    - 870
    - list
  - !!python/tuple
    - 828
    - 837
    - 837
    - 851
    - elaboration
  - !!python/tuple
    - 851
    - 870
    - 828
    - 851
    - list
  - !!python/tuple
    - 851
    - 861
    - 861
    - 870
    - elaboration
  - !!python/tuple
    - 828
    - 870
    - 870
    - 1077
    - elaboration
  - !!python/tuple
    - 870
    - 872
    - 872
    - 1077
    - elaboration
  - !!python/tuple
    - 872
    - 881
    - 881
    - 888
    - elaboration
  - !!python/tuple
    - 872
    - 888
    - 888
    - 1077
    - elaboration
  - !!python/tuple
    - 890
    - 894
    - 888
    - 890
    - attribution
  - !!python/tuple
    - 888
    - 894
    - 894
    - 1077
    - elaboration
  - !!python/tuple
    - 894
    - 896
    - 896
    - 1077
    - elaboration
  - !!python/tuple
    - 906
    - 918
    - 896
    - 906
    - attribution
  - !!python/tuple
    - 896
    - 918
    - 918
    - 1077
    - elaboration
  - !!python/tuple
    - 918
    - 919
    - 919
    - 923
    - elaboration
  - !!python/tuple
    - 918
    - 923
    - 923
    - 1077
    - elaboration
  - !!python/tuple
    - 924
    - 930
    - 923
    - 924
    - attribution
  - !!python/tuple
    - 923
    - 930
    - 930
    - 1077
    - elaboration
  - !!python/tuple
    - 930
    - 951
    - 951
    - 1077
    - elaboration
  - !!python/tuple
    - 955
    - 972
    - 951
    - 955
    - attribution
  - !!python/tuple
    - 951
    - 972
    - 972
    - 1077
    - elaboration
  - !!python/tuple
    - 979
    - 988
    - 972
    - 979
    - attribution
  - !!python/tuple
    - 972
    - 988
    - 988
    - 1077
    - elaboration
  - !!python/tuple
    - 988
    - 990
    - 990
    - 991
    - elaboration
  - !!python/tuple
    - 988
    - 991
    - 991
    - 1005
    - elaboration
  - !!python/tuple
    - 991
    - 997
    - 997
    - 1005
    - same_unit
  - !!python/tuple
    - 997
    - 1005
    - 991
    - 997
    - same_unit
  - !!python/tuple
    - 988
    - 1005
    - 1005
    - 1020
    - elaboration
  - !!python/tuple
    - 988
    - 1020
    - 1020
    - 1077
    - elaboration
  - !!python/tuple
    - 1020
    - 1022
    - 1022
    - 1047
    - elaboration
  - !!python/tuple
    - 1020
    - 1047
    - 1047
    - 1077
    - elaboration
  - !!python/tuple
    - 1047
    - 1062
    - 1062
    - 1067
    - circumstance
  - !!python/tuple
    - 1047
    - 1067
    - 1067
    - 1077
    - elaboration
  - !!python/tuple
    - 1077
    - 1227
    - 0
    - 1077
    - topic
  - !!python/tuple
    - 1077
    - 1081
    - 1081
    - 1088
    - same_unit
  - !!python/tuple
    - 1078
    - 1081
    - 1077
    - 1078
    - attribution
  - !!python/tuple
    - 1081
    - 1088
    - 1077
    - 1081
    - same_unit
  - !!python/tuple
    - 1077
    - 1088
    - 1088
    - 1227
    - circumstance
  - !!python/tuple
    - 1088
    - 1108
    - 1108
    - 1227
    - elaboration
  - !!python/tuple
    - 1108
    - 1114
    - 1114
    - 1227
    - list
  - !!python/tuple
    - 1108
    - 1112
    - 1112
    - 1114
    - elaboration
  - !!python/tuple
    - 1114
    - 1227
    - 1108
    - 1114
    - list
  - !!python/tuple
    - 1115
    - 1227
    - 1114
    - 1115
    - attribution
  - !!python/tuple
    - 1115
    - 1116
    - 1116
    - 1227
    - condition
  - !!python/tuple
    - 1116
    - 1132
    - 1132
    - 1227
    - elaboration
  - !!python/tuple
    - 1132
    - 1139
    - 1139
    - 1227
    - textualorganization
  - !!python/tuple
    - 1139
    - 1227
    - 1132
    - 1139
    - textualorganization
  - !!python/tuple
    - 1139
    - 1163
    - 1163
    - 1178
    - list
  - !!python/tuple
    - 1139
    - 1150
    - 1150
    - 1163
    - elaboration
  - !!python/tuple
    - 1163
    - 1178
    - 1139
    - 1163
    - list
  - !!python/tuple
    - 1139
    - 1178
    - 1178
    - 1227
    - elaboration
  - !!python/tuple
    - 1178
    - 1193
    - 1193
    - 1209
    - elaboration
  - !!python/tuple
    - 1193
    - 1197
    - 1197
    - 1209
    - elaboration
  - !!python/tuple
    - 1197
    - 1203
    - 1203
    - 1209
    - elaboration
  - !!python/tuple
    - 1178
    - 1209
    - 1209
    - 1227
    - elaboration
  - !!python/tuple
    - 1216
    - 1227
    - 1209
    - 1216
    - attribution
  tokens:
  - The
  - authors
  - propose
  - a
  - new
  - on-policy
  - exploration
  - strategy
  - by
  - using
  - a
  - policy
  - with
  - a
  - hierarchy
  - of
  - stochasticity
  - .
  - The
  - authors
  - use
  - a
  - two-level
  - hierarchical
  - distribution
  - as
  - a
  - policy
  - ','
  - where
  - the
  - global
  - variable
  - is
  - used
  - for
  - dropout
  - .
  - This
  - work
  - is
  - interesting
  - since
  - the
  - authors
  - use
  - dropout
  - for
  - policy
  - learning
  - and
  - exploration
  - .
  - The
  - authors
  - show
  - that
  - parameter
  - noise
  - exploration
  - is
  - a
  - particular
  - case
  - of
  - the
  - proposed
  - policy
  - .
  - The
  - main
  - concern
  - is
  - the
  - gap
  - between
  - the
  - problem
  - formulation
  - and
  - the
  - actual
  - optimization
  - problem
  - in
  - Eq
  - '12'
  - .
  - I
  - am
  - very
  - happy
  - to
  - give
  - a
  - higher
  - rating
  - if
  - the
  - authors
  - address
  - the
  - following
  - points
  - .
  - Detailed
  - Comments
  - -LRB-
  - '1'
  - -RRB-
  - The
  - authors
  - give
  - the
  - derivation
  - for
  - Eq
  - '10'
  - .
  - However
  - ','
  - it
  - is
  - not
  - obvious
  - that
  - how
  - to
  - move
  - from
  - line
  - '3'
  - to
  - line
  - '4'
  - at
  - Eq
  - '15'
  - .
  - Minor
  - ':'
  - Since
  - the
  - action
  - is
  - denoted
  - by
  - '``'
  - a
  - ''''''
  - ','
  - it
  - will
  - be
  - more
  - clear
  - if
  - the
  - authors
  - use
  - another
  - symbol
  - to
  - denote
  - the
  - parameter
  - of
  - q
  - -LRB-
  - z
  - -RRB-
  - instead
  - of
  - ''''''
  - \
  - alpha
  - ''''''
  - at
  - Eq
  - '10'
  - and
  - '15'
  - .
  - -LRB-
  - '2'
  - -RRB-
  - Due
  - to
  - the
  - use
  - of
  - the
  - likelihood
  - ratio
  - trick
  - ','
  - the
  - authors
  - use
  - the
  - mean
  - policy
  - as
  - an
  - approximation
  - at
  - Eq
  - '12'
  - .
  - Does
  - such
  - approximation
  - guarantee
  - the
  - policy
  - improvement
  - '?'
  - Any
  - justification
  - '?'
  - -LRB-
  - '3'
  - -RRB-
  - Instead
  - of
  - using
  - the
  - mean
  - policy
  - approximation
  - in
  - Eq
  - '12'
  - ','
  - the
  - authors
  - should
  - consider
  - existing
  - Monte
  - Carlo
  - techniques
  - to
  - reduce
  - the
  - variance
  - of
  - the
  - gradient
  - estimation
  - .
  - For
  - example
  - ','
  - -LSB-
  - '1'
  - -RSB-
  - could
  - be
  - used
  - to
  - reduce
  - the
  - variance
  - of
  - gradient
  - w.r.t.
  - \
  - phi
  - .
  - Note
  - that
  - the
  - gradient
  - is
  - biased
  - if
  - the
  - mean
  - policy
  - approximation
  - is
  - used
  - .
  - -LRB-
  - '4'
  - -RRB-
  - Are
  - \
  - theta
  - and
  - \
  - phi
  - jointly
  - and
  - simultaneously
  - optimized
  - at
  - Eq
  - '12'
  - '?'
  - The
  - authors
  - should
  - clarify
  - this
  - point
  - .
  - -LRB-
  - '5'
  - -RRB-
  - Due
  - to
  - the
  - mean
  - policy
  - approximation
  - ','
  - does
  - the
  - mean
  - policy
  - depend
  - 'on'
  - \
  - phi
  - '?'
  - The
  - authors
  - should
  - clearly
  - explain
  - how
  - to
  - update
  - \
  - phi
  - when
  - optimizing
  - Eq
  - '12'
  - .
  - -LRB-
  - '6'
  - -RRB-
  - If
  - the
  - authors
  - jointly
  - and
  - simultaneously
  - optimize
  - \
  - theta
  - and
  - \
  - phi
  - ','
  - why
  - a
  - regularization
  - term
  - about
  - q
  - _
  - -LCB-
  - \
  - phi
  - -RCB-
  - -LRB-
  - z
  - -RRB-
  - is
  - missing
  - in
  - Eq
  - '12'
  - while
  - a
  - regularization
  - term
  - about
  - \
  - pi
  - _
  - -LCB-
  - \
  - theta
  - '|'
  - z
  - -RCB-
  - does
  - appear
  - in
  - Eq
  - '12'
  - '?'
  - -LRB-
  - '7'
  - -RRB-
  - The
  - authors
  - give
  - the
  - derivations
  - about
  - \
  - theta
  - such
  - as
  - the
  - gradient
  - and
  - the
  - regularization
  - term
  - about
  - \
  - theta
  - -LRB-
  - see
  - ','
  - Eq
  - 18-19
  - -RRB-
  - .
  - However
  - ','
  - the
  - derivations
  - about
  - \
  - phi
  - are
  - missing
  - .
  - For
  - example
  - ','
  - how
  - to
  - compute
  - the
  - gradient
  - w.r.t.
  - \
  - phi
  - '?'
  - Since
  - the
  - mean
  - policy
  - is
  - used
  - ','
  - it
  - is
  - not
  - apparent
  - that
  - how
  - to
  - compute
  - the
  - gradient
  - w.r.t.
  - \
  - phi
  - .
  - Minor
  - ','
  - 1/2
  - is
  - missing
  - in
  - the
  - last
  - line
  - of
  - Eq
  - '19'
  - .
  - Reference
  - ':'
  - -LSB-
  - '1'
  - -RSB-
  - AUEB
  - ','
  - Michalis
  - Titsias
  - RC
  - ','
  - and
  - Miguel
  - Lzaro-Gredilla
  - .
  - '``'
  - Local
  - expectation
  - gradients
  - for
  - black
  - box
  - variational
  - inference
  - .
  - ''''''
  - In
  - Advances
  - in
  - neural
  - information
  - processing
  - systems
  - ','
  - pp.
  - 2638-2646
  - .
  - '2015'
  - .
  - Thank
  - your
  - very
  - much
  - for
  - your
  - review
  - .
  - We
  - have
  - updated
  - the
  - manuscript
  - with
  - more
  - details
  - in
  - the
  - derivation
  - of
  - the
  - first
  - order
  - approximation
  - of
  - KL
  - divergence
  - .
  - '1'
  - -RRB-
  - Elaborated
  - derivation
  - of
  - Eq
  - .
  - '10'
  - Q1
  - ':'
  - We
  - have
  - added
  - one
  - more
  - line
  - to
  - explain
  - the
  - derivation
  - .
  - Basically
  - a
  - baseline
  - is
  - subtracted
  - ','
  - and
  - GAE
  - is
  - introduced
  - .
  - '2'
  - -RRB-
  - Gradient
  - update
  - 'on'
  - \
  - phi
  - from
  - KL
  - divergence
  - The
  - gradients
  - w.r.t.
  - \
  - phi
  - from
  - the
  - KL
  - divergence
  - is
  - stopped
  - for
  - variance
  - reduction
  - with
  - acceptable
  - bias
  - ','
  - which
  - we
  - prove
  - with
  - MuProp
  - -LSB-
  - '1'
  - -RSB-
  - .
  - Details
  - could
  - be
  - found
  - in
  - Appendix
  - C.
  - Q3
  - ':'
  - Rather
  - than
  - -LSB-
  - '2'
  - -RSB-
  - ','
  - we
  - employ
  - MuProp
  - to
  - reduce
  - variance
  - in
  - our
  - development
  - of
  - NADPEx
  - .
  - Thank
  - your
  - for
  - your
  - suggestion
  - .
  - Q4
  - ':'
  - 'Yes'
  - \
  - theta
  - and
  - \
  - phi
  - are
  - jointly
  - and
  - simultaneously
  - optimized
  - at
  - Eq
  - .
  - '12'
  - ','
  - though
  - the
  - gradients
  - w.r.t.
  - \
  - phi
  - from
  - the
  - KL
  - divergence
  - is
  - stopped
  - .
  - Q7
  - ':'
  - Due
  - to
  - the
  - stop-gradient
  - manipulation
  - in
  - the
  - KL
  - divergence
  - ','
  - gradients
  - w.r.t.
  - \
  - phi
  - remains
  - the
  - same
  - as
  - in
  - stated
  - in
  - last
  - subsection
  - .
  - '3'
  - -RRB-
  - Mean
  - policy
  - in
  - the
  - KL
  - divergence
  - What
  - motivates
  - the
  - mean
  - policy
  - is
  - not
  - variance
  - reduction
  - ','
  - but
  - the
  - idea
  - that
  - dropout
  - policy
  - had
  - better
  - to
  - be
  - close
  - to
  - each
  - other
  - .
  - As
  - intuitively
  - \
  - phi
  - is
  - controlling
  - the
  - distance
  - between
  - dropout
  - policies
  - ','
  - it
  - would
  - further
  - remedy
  - the
  - little
  - bias
  - mentioned
  - above
  - .
  - However
  - ','
  - the
  - computation
  - complexity
  - for
  - '``'
  - close
  - to
  - each
  - other
  - ''''''
  - would
  - be
  - O
  - -LRB-
  - N
  - ^
  - '2'
  - -RRB-
  - ','
  - with
  - N
  - being
  - the
  - number
  - of
  - dropout
  - policies
  - in
  - this
  - batch
  - .
  - We
  - employ
  - mean
  - policy
  - to
  - make
  - it
  - linear
  - .
  - And
  - it
  - could
  - be
  - regarded
  - as
  - an
  - integration
  - 'on'
  - a
  - Gaussian
  - approximation
  - of
  - the
  - Monte
  - Carlo
  - estimate
  - according
  - to
  - -LSB-
  - '3'
  - -RSB-
  - .
  - Details
  - could
  - be
  - found
  - in
  - Appendix
  - C.
  - Q2
  - ':'
  - 'No'
  - the
  - mean
  - policy
  - is
  - not
  - used
  - due
  - to
  - the
  - likelihood
  - ratio
  - trick
  - .
  - And
  - the
  - approximation
  - of
  - using
  - mean
  - policy
  - is
  - discussed
  - in
  - -LSB-
  - '3'
  - -RSB-
  - ','
  - with
  - a
  - sound
  - deduction
  - .
  - Q3
  - ':'
  - Mean
  - policy
  - is
  - not
  - motivated
  - by
  - variance
  - reduction
  - ','
  - which
  - is
  - addressed
  - as
  - introduced
  - above
  - .
  - Thank
  - you
  - for
  - your
  - suggestion
  - .
  - Q5
  - ':'
  - In
  - the
  - updated
  - version
  - ','
  - we
  - have
  - explicitly
  - pointed
  - out
  - that
  - the
  - gradients
  - w.r.t.
  - \
  - phi
  - from
  - KL
  - divergence
  - is
  - stopped
  - .
  - Thanks
  - for
  - this
  - suggestion
  - .
  - Hope
  - our
  - response
  - addresses
  - your
  - concerns
  - '!'
  - -LSB-
  - '1'
  - -RSB-
  - Gu
  - et
  - al.
  - ','
  - '``'
  - MuProp
  - ':'
  - Unbiased
  - Backpropagation
  - for
  - Stochastic
  - Neural
  - Networks
  - ''''''
  - ','
  - ICLR
  - '2016'
  - .
  - -LSB-
  - '2'
  - -RSB-
  - Titsias
  - et
  - al.
  - ','
  - '``'
  - Local
  - Expectation
  - Gradients
  - for
  - Black
  - Box
  - Variational
  - Inference
  - ''''''
  - ','
  - NIPS
  - '2015'
  - .
  - -LSB-
  - '3'
  - -RSB-
  - Wang
  - et
  - al.
  - ','
  - '``'
  - Fast
  - dropout
  - training
  - ''''''
  - ','
  - ICML
  - '2013'
  - .
  - '``'
  - Q4
  - ':'
  - 'Yes'
  - \
  - theta
  - and
  - \
  - phi
  - are
  - jointly
  - and
  - simultaneously
  - optimized
  - at
  - Eq
  - .
  - '12'
  - ','
  - though
  - the
  - gradients
  - w.r.t.
  - \
  - phi
  - from
  - the
  - KL
  - divergence
  - is
  - stopped
  - .
  - Q7
  - ':'
  - Due
  - to
  - the
  - stop-gradient
  - manipulation
  - in
  - the
  - KL
  - divergence
  - ','
  - gradients
  - w.r.t.
  - \
  - phi
  - remains
  - the
  - same
  - as
  - in
  - stated
  - in
  - last
  - subsection
  - .
  - ''''''
  - My
  - guess
  - is
  - that
  - due
  - to
  - the
  - stop-gradient
  - manipulation
  - ','
  - \
  - phi
  - remains
  - the
  - same
  - when
  - optimizing
  - Eq
  - '12'
  - .
  - In
  - other
  - words
  - ','
  - \
  - phi
  - is
  - not
  - updated
  - .
  - Is
  - it
  - correct
  - '?'
  - Can
  - the
  - authors
  - comment
  - 'on'
  - this
  - '?'
  - As
  - explained
  - in
  - our
  - last
  - response
  - ','
  - \
  - phi
  - is
  - still
  - updated
  - with
  - gradients
  - from
  - surrogate
  - loss
  - i.e.
  - Eq
  - .
  - '10'
  - and
  - Eq
  - .
  - '11'
  - .
  - Note
  - that
  - if
  - there
  - was
  - stop
  - gradient
  - operation
  - ','
  - there
  - would
  - be
  - two
  - streams
  - of
  - gradients
  - in
  - NADPEx
  - when
  - a
  - KL
  - regularizer
  - is
  - added
  - ':'
  - one
  - from
  - surrogate
  - loss
  - ','
  - another
  - one
  - from
  - KL
  - divergence
  - .
  - We
  - only
  - stop
  - gradients
  - w.r.t.
  - \
  - phi
  - from
  - the
  - KL
  - divergence
  - .
  - ''''''
  - \
  - phi
  - is
  - not
  - updated
  - ''''''
  - means
  - gradients
  - from
  - surrogate
  - loss
  - are
  - also
  - stopped
  - .
  - Actually
  - in
  - our
  - paper
  - ','
  - it
  - is
  - referred
  - to
  - as
  - bootstrap
  - ','
  - named
  - with
  - BootstrapDQN
  - -LSB-
  - '1'
  - -RSB-
  - ','
  - for
  - which
  - we
  - provided
  - a
  - comparison
  - with
  - NADPEx
  - in
  - Section
  - '4.3'
  - .
  - -LSB-
  - '1'
  - -RSB-
  - Osband
  - et
  - al.
  - ','
  - '``'
  - Deep
  - exploration
  - via
  - bootstrapped
  - dqn
  - ''''''
  - ','
  - NIPS
  - '2016'
  - .
- comment_id: B1et5yXJ14
  rels:
  - !!python/tuple
    - 0
    - 38
    - 38
    - 44
    - elaboration
  - !!python/tuple
    - 0
    - 44
    - 44
    - 58
    - elaboration
  - !!python/tuple
    - 0
    - 58
    - 58
    - 1934
    - elaboration
  - !!python/tuple
    - 58
    - 65
    - 65
    - 1934
    - elaboration
  - !!python/tuple
    - 65
    - 79
    - 79
    - 1934
    - elaboration
  - !!python/tuple
    - 79
    - 90
    - 90
    - 142
    - elaboration
  - !!python/tuple
    - 90
    - 106
    - 106
    - 142
    - elaboration
  - !!python/tuple
    - 106
    - 109
    - 109
    - 142
    - elaboration
  - !!python/tuple
    - 109
    - 138
    - 138
    - 142
    - elaboration
  - !!python/tuple
    - 79
    - 142
    - 142
    - 1934
    - elaboration
  - !!python/tuple
    - 142
    - 149
    - 149
    - 154
    - elaboration
  - !!python/tuple
    - 142
    - 154
    - 154
    - 161
    - elaboration
  - !!python/tuple
    - 142
    - 161
    - 161
    - 175
    - elaboration
  - !!python/tuple
    - 142
    - 175
    - 175
    - 194
    - elaboration
  - !!python/tuple
    - 142
    - 194
    - 194
    - 355
    - elaboration
  - !!python/tuple
    - 194
    - 196
    - 196
    - 229
    - elaboration
  - !!python/tuple
    - 196
    - 222
    - 222
    - 229
    - same_unit
  - !!python/tuple
    - 196
    - 208
    - 208
    - 222
    - elaboration
  - !!python/tuple
    - 222
    - 229
    - 196
    - 222
    - same_unit
  - !!python/tuple
    - 222
    - 223
    - 223
    - 229
    - elaboration
  - !!python/tuple
    - 194
    - 229
    - 229
    - 355
    - elaboration
  - !!python/tuple
    - 229
    - 236
    - 236
    - 241
    - elaboration
  - !!python/tuple
    - 229
    - 241
    - 241
    - 277
    - elaboration
  - !!python/tuple
    - 241
    - 261
    - 261
    - 277
    - contrast
  - !!python/tuple
    - 241
    - 248
    - 248
    - 261
    - circumstance
  - !!python/tuple
    - 261
    - 277
    - 241
    - 261
    - contrast
  - !!python/tuple
    - 265
    - 277
    - 261
    - 265
    - attribution
  - !!python/tuple
    - 229
    - 277
    - 277
    - 355
    - elaboration
  - !!python/tuple
    - 281
    - 311
    - 277
    - 281
    - attribution
  - !!python/tuple
    - 281
    - 296
    - 296
    - 311
    - elaboration
  - !!python/tuple
    - 277
    - 311
    - 311
    - 355
    - elaboration
  - !!python/tuple
    - 311
    - 325
    - 325
    - 355
    - same_unit
  - !!python/tuple
    - 311
    - 313
    - 313
    - 325
    - elaboration
  - !!python/tuple
    - 325
    - 355
    - 311
    - 325
    - same_unit
  - !!python/tuple
    - 325
    - 334
    - 334
    - 355
    - elaboration
  - !!python/tuple
    - 334
    - 341
    - 341
    - 355
    - elaboration
  - !!python/tuple
    - 341
    - 347
    - 347
    - 355
    - elaboration
  - !!python/tuple
    - 142
    - 355
    - 355
    - 1934
    - elaboration
  - !!python/tuple
    - 355
    - 398
    - 398
    - 1934
    - contrast
  - !!python/tuple
    - 355
    - 380
    - 380
    - 398
    - elaboration
  - !!python/tuple
    - 398
    - 1934
    - 355
    - 398
    - contrast
  - !!python/tuple
    - 398
    - 426
    - 426
    - 434
    - elaboration
  - !!python/tuple
    - 398
    - 434
    - 434
    - 1934
    - elaboration
  - !!python/tuple
    - 434
    - 442
    - 442
    - 453
    - elaboration
  - !!python/tuple
    - 434
    - 453
    - 453
    - 1934
    - example
  - !!python/tuple
    - 453
    - 473
    - 473
    - 484
    - same_unit
  - !!python/tuple
    - 453
    - 467
    - 467
    - 473
    - elaboration
  - !!python/tuple
    - 473
    - 484
    - 453
    - 473
    - same_unit
  - !!python/tuple
    - 473
    - 479
    - 479
    - 484
    - elaboration
  - !!python/tuple
    - 453
    - 484
    - 484
    - 1934
    - elaboration
  - !!python/tuple
    - 489
    - 524
    - 484
    - 489
    - attribution
  - !!python/tuple
    - 489
    - 499
    - 499
    - 524
    - purpose
  - !!python/tuple
    - 484
    - 524
    - 524
    - 545
    - elaboration
  - !!python/tuple
    - 484
    - 545
    - 545
    - 1934
    - elaboration
  - !!python/tuple
    - 545
    - 555
    - 555
    - 1934
    - elaboration
  - !!python/tuple
    - 555
    - 557
    - 557
    - 568
    - elaboration
  - !!python/tuple
    - 557
    - 559
    - 559
    - 568
    - elaboration
  - !!python/tuple
    - 555
    - 568
    - 568
    - 1934
    - elaboration
  - !!python/tuple
    - 568
    - 584
    - 584
    - 1934
    - topic
  - !!python/tuple
    - 568
    - 579
    - 579
    - 584
    - elaboration
  - !!python/tuple
    - 584
    - 1934
    - 568
    - 584
    - topic
  - !!python/tuple
    - 590
    - 616
    - 584
    - 590
    - attribution
  - !!python/tuple
    - 584
    - 616
    - 616
    - 1934
    - explanation
  - !!python/tuple
    - 623
    - 640
    - 616
    - 623
    - attribution
  - !!python/tuple
    - 623
    - 630
    - 630
    - 640
    - elaboration
  - !!python/tuple
    - 630
    - 634
    - 634
    - 640
    - elaboration
  - !!python/tuple
    - 616
    - 640
    - 640
    - 707
    - elaboration
  - !!python/tuple
    - 642
    - 664
    - 640
    - 642
    - attribution
  - !!python/tuple
    - 642
    - 656
    - 656
    - 664
    - list
  - !!python/tuple
    - 656
    - 664
    - 642
    - 656
    - list
  - !!python/tuple
    - 640
    - 664
    - 664
    - 707
    - explanation
  - !!python/tuple
    - 664
    - 672
    - 672
    - 680
    - concession
  - !!python/tuple
    - 664
    - 680
    - 680
    - 707
    - elaboration
  - !!python/tuple
    - 680
    - 692
    - 692
    - 707
    - elaboration
  - !!python/tuple
    - 692
    - 693
    - 693
    - 707
    - elaboration
  - !!python/tuple
    - 693
    - 696
    - 696
    - 707
    - elaboration
  - !!python/tuple
    - 616
    - 707
    - 707
    - 851
    - elaboration
  - !!python/tuple
    - 707
    - 717
    - 717
    - 851
    - elaboration
  - !!python/tuple
    - 717
    - 729
    - 729
    - 745
    - same_unit
  - !!python/tuple
    - 717
    - 725
    - 725
    - 729
    - elaboration
  - !!python/tuple
    - 725
    - 726
    - 726
    - 729
    - elaboration
  - !!python/tuple
    - 729
    - 745
    - 717
    - 729
    - same_unit
  - !!python/tuple
    - 717
    - 745
    - 745
    - 851
    - elaboration
  - !!python/tuple
    - 745
    - 759
    - 759
    - 763
    - same_unit
  - !!python/tuple
    - 745
    - 755
    - 755
    - 759
    - elaboration
  - !!python/tuple
    - 755
    - 756
    - 756
    - 759
    - elaboration
  - !!python/tuple
    - 759
    - 763
    - 745
    - 759
    - same_unit
  - !!python/tuple
    - 745
    - 763
    - 763
    - 851
    - elaboration
  - !!python/tuple
    - 763
    - 788
    - 788
    - 796
    - same_unit
  - !!python/tuple
    - 763
    - 784
    - 784
    - 788
    - elaboration
  - !!python/tuple
    - 788
    - 796
    - 763
    - 788
    - same_unit
  - !!python/tuple
    - 763
    - 796
    - 796
    - 851
    - elaboration
  - !!python/tuple
    - 796
    - 800
    - 800
    - 804
    - elaboration
  - !!python/tuple
    - 796
    - 804
    - 804
    - 851
    - elaboration
  - !!python/tuple
    - 807
    - 851
    - 804
    - 807
    - attribution
  - !!python/tuple
    - 807
    - 814
    - 814
    - 829
    - list
  - !!python/tuple
    - 814
    - 829
    - 807
    - 814
    - list
  - !!python/tuple
    - 807
    - 829
    - 829
    - 851
    - elaboration
  - !!python/tuple
    - 829
    - 841
    - 841
    - 851
    - elaboration
  - !!python/tuple
    - 844
    - 851
    - 841
    - 844
    - attribution
  - !!python/tuple
    - 616
    - 851
    - 851
    - 1934
    - elaboration
  - !!python/tuple
    - 878
    - 893
    - 851
    - 878
    - attribution
  - !!python/tuple
    - 851
    - 893
    - 893
    - 904
    - elaboration
  - !!python/tuple
    - 851
    - 904
    - 904
    - 1934
    - elaboration
  - !!python/tuple
    - 908
    - 928
    - 904
    - 908
    - attribution
  - !!python/tuple
    - 908
    - 914
    - 914
    - 928
    - purpose
  - !!python/tuple
    - 914
    - 920
    - 920
    - 928
    - purpose
  - !!python/tuple
    - 904
    - 928
    - 928
    - 1934
    - elaboration
  - !!python/tuple
    - 928
    - 940
    - 940
    - 969
    - same_unit
  - !!python/tuple
    - 928
    - 932
    - 932
    - 940
    - elaboration
  - !!python/tuple
    - 940
    - 969
    - 928
    - 940
    - same_unit
  - !!python/tuple
    - 940
    - 959
    - 959
    - 969
    - elaboration
  - !!python/tuple
    - 928
    - 969
    - 969
    - 1934
    - elaboration
  - !!python/tuple
    - 972
    - 999
    - 969
    - 972
    - attribution
  - !!python/tuple
    - 983
    - 999
    - 972
    - 983
    - condition
  - !!python/tuple
    - 969
    - 999
    - 999
    - 1022
    - elaboration
  - !!python/tuple
    - 969
    - 1022
    - 1022
    - 1934
    - elaboration
  - !!python/tuple
    - 1022
    - 1039
    - 1039
    - 1934
    - elaboration
  - !!python/tuple
    - 1039
    - 1041
    - 1041
    - 1934
    - textualorganization
  - !!python/tuple
    - 1041
    - 1934
    - 1039
    - 1041
    - textualorganization
  - !!python/tuple
    - 1041
    - 1052
    - 1052
    - 1056
    - elaboration
  - !!python/tuple
    - 1041
    - 1056
    - 1056
    - 1934
    - elaboration
  - !!python/tuple
    - 1056
    - 1074
    - 1074
    - 1934
    - elaboration
  - !!python/tuple
    - 1076
    - 1105
    - 1074
    - 1076
    - attribution
  - !!python/tuple
    - 1077
    - 1105
    - 1076
    - 1077
    - attribution
  - !!python/tuple
    - 1077
    - 1099
    - 1099
    - 1105
    - elaboration
  - !!python/tuple
    - 1074
    - 1105
    - 1105
    - 1934
    - elaboration
  - !!python/tuple
    - 1105
    - 1123
    - 1123
    - 1934
    - list
  - !!python/tuple
    - 1105
    - 1118
    - 1118
    - 1123
    - list
  - !!python/tuple
    - 1118
    - 1123
    - 1105
    - 1118
    - list
  - !!python/tuple
    - 1123
    - 1934
    - 1105
    - 1123
    - list
  - !!python/tuple
    - 1123
    - 1125
    - 1125
    - 1934
    - list
  - !!python/tuple
    - 1125
    - 1934
    - 1123
    - 1125
    - list
  - !!python/tuple
    - 1125
    - 1141
    - 1141
    - 1934
    - list
  - !!python/tuple
    - 1125
    - 1127
    - 1127
    - 1141
    - means
  - !!python/tuple
    - 1141
    - 1934
    - 1125
    - 1141
    - list
  - !!python/tuple
    - 1143
    - 1934
    - 1141
    - 1143
    - attribution
  - !!python/tuple
    - 1143
    - 1151
    - 1151
    - 1934
    - elaboration
  - !!python/tuple
    - 1151
    - 1180
    - 1180
    - 1934
    - elaboration
  - !!python/tuple
    - 1180
    - 1214
    - 1214
    - 1934
    - list
  - !!python/tuple
    - 1180
    - 1190
    - 1190
    - 1214
    - elaboration
  - !!python/tuple
    - 1214
    - 1934
    - 1180
    - 1214
    - list
  - !!python/tuple
    - 1214
    - 1226
    - 1226
    - 1934
    - topic
  - !!python/tuple
    - 1214
    - 1216
    - 1216
    - 1226
    - list
  - !!python/tuple
    - 1216
    - 1226
    - 1214
    - 1216
    - list
  - !!python/tuple
    - 1216
    - 1217
    - 1217
    - 1218
    - elaboration
  - !!python/tuple
    - 1216
    - 1218
    - 1218
    - 1226
    - elaboration
  - !!python/tuple
    - 1218
    - 1221
    - 1221
    - 1226
    - elaboration
  - !!python/tuple
    - 1226
    - 1934
    - 1214
    - 1226
    - topic
  - !!python/tuple
    - 1226
    - 1236
    - 1236
    - 1934
    - elaboration
  - !!python/tuple
    - 1236
    - 1249
    - 1249
    - 1934
    - elaboration
  - !!python/tuple
    - 1249
    - 1265
    - 1265
    - 1288
    - elaboration
  - !!python/tuple
    - 1249
    - 1288
    - 1288
    - 1934
    - elaboration
  - !!python/tuple
    - 1288
    - 1303
    - 1303
    - 1312
    - elaboration
  - !!python/tuple
    - 1288
    - 1312
    - 1312
    - 1400
    - elaboration
  - !!python/tuple
    - 1312
    - 1323
    - 1323
    - 1400
    - elaboration
  - !!python/tuple
    - 1323
    - 1337
    - 1337
    - 1343
    - elaboration
  - !!python/tuple
    - 1323
    - 1343
    - 1343
    - 1400
    - elaboration
  - !!python/tuple
    - 1343
    - 1348
    - 1348
    - 1400
    - elaboration
  - !!python/tuple
    - 1348
    - 1357
    - 1357
    - 1400
    - elaboration
  - !!python/tuple
    - 1357
    - 1375
    - 1375
    - 1400
    - elaboration
  - !!python/tuple
    - 1375
    - 1380
    - 1380
    - 1400
    - elaboration
  - !!python/tuple
    - 1288
    - 1400
    - 1400
    - 1934
    - elaboration
  - !!python/tuple
    - 1400
    - 1429
    - 1429
    - 1439
    - same_unit
  - !!python/tuple
    - 1400
    - 1406
    - 1406
    - 1429
    - elaboration
  - !!python/tuple
    - 1429
    - 1439
    - 1400
    - 1429
    - same_unit
  - !!python/tuple
    - 1400
    - 1439
    - 1439
    - 1934
    - elaboration
  - !!python/tuple
    - 1469
    - 1488
    - 1439
    - 1469
    - attribution
  - !!python/tuple
    - 1469
    - 1483
    - 1483
    - 1488
    - same_unit
  - !!python/tuple
    - 1469
    - 1476
    - 1476
    - 1483
    - elaboration
  - !!python/tuple
    - 1483
    - 1488
    - 1469
    - 1483
    - same_unit
  - !!python/tuple
    - 1439
    - 1488
    - 1488
    - 1934
    - elaboration
  - !!python/tuple
    - 1488
    - 1509
    - 1509
    - 1934
    - elaboration
  - !!python/tuple
    - 1509
    - 1524
    - 1524
    - 1934
    - list
  - !!python/tuple
    - 1516
    - 1524
    - 1509
    - 1516
    - attribution
  - !!python/tuple
    - 1516
    - 1518
    - 1518
    - 1524
    - purpose
  - !!python/tuple
    - 1524
    - 1934
    - 1509
    - 1524
    - list
  - !!python/tuple
    - 1524
    - 1547
    - 1547
    - 1934
    - topic
  - !!python/tuple
    - 1524
    - 1534
    - 1534
    - 1547
    - elaboration
  - !!python/tuple
    - 1534
    - 1536
    - 1536
    - 1547
    - elaboration
  - !!python/tuple
    - 1536
    - 1541
    - 1541
    - 1547
    - elaboration
  - !!python/tuple
    - 1547
    - 1934
    - 1524
    - 1547
    - topic
  - !!python/tuple
    - 1547
    - 1566
    - 1566
    - 1934
    - question
  - !!python/tuple
    - 1547
    - 1565
    - 1565
    - 1566
    - same_unit
  - !!python/tuple
    - 1547
    - 1559
    - 1559
    - 1565
    - elaboration
  - !!python/tuple
    - 1565
    - 1566
    - 1547
    - 1565
    - same_unit
  - !!python/tuple
    - 1566
    - 1934
    - 1547
    - 1566
    - question
  - !!python/tuple
    - 1566
    - 1584
    - 1584
    - 1934
    - list
  - !!python/tuple
    - 1566
    - 1575
    - 1575
    - 1584
    - same_unit
  - !!python/tuple
    - 1566
    - 1571
    - 1571
    - 1575
    - elaboration
  - !!python/tuple
    - 1575
    - 1584
    - 1566
    - 1575
    - same_unit
  - !!python/tuple
    - 1575
    - 1578
    - 1578
    - 1584
    - list
  - !!python/tuple
    - 1578
    - 1584
    - 1575
    - 1578
    - list
  - !!python/tuple
    - 1584
    - 1934
    - 1566
    - 1584
    - list
  - !!python/tuple
    - 1584
    - 1597
    - 1597
    - 1934
    - list
  - !!python/tuple
    - 1584
    - 1592
    - 1592
    - 1597
    - attribution
  - !!python/tuple
    - 1597
    - 1934
    - 1584
    - 1597
    - list
  - !!python/tuple
    - 1597
    - 1603
    - 1603
    - 1608
    - list
  - !!python/tuple
    - 1603
    - 1608
    - 1597
    - 1603
    - list
  - !!python/tuple
    - 1597
    - 1608
    - 1608
    - 1637
    - elaboration
  - !!python/tuple
    - 1608
    - 1615
    - 1615
    - 1620
    - elaboration
  - !!python/tuple
    - 1608
    - 1620
    - 1620
    - 1637
    - elaboration
  - !!python/tuple
    - 1621
    - 1637
    - 1620
    - 1621
    - attribution
  - !!python/tuple
    - 1621
    - 1632
    - 1632
    - 1637
    - elaboration
  - !!python/tuple
    - 1597
    - 1637
    - 1637
    - 1722
    - elaboration
  - !!python/tuple
    - 1637
    - 1643
    - 1643
    - 1670
    - elaboration
  - !!python/tuple
    - 1643
    - 1644
    - 1644
    - 1670
    - circumstance
  - !!python/tuple
    - 1644
    - 1650
    - 1650
    - 1670
    - elaboration
  - !!python/tuple
    - 1650
    - 1654
    - 1654
    - 1670
    - elaboration
  - !!python/tuple
    - 1654
    - 1655
    - 1655
    - 1670
    - elaboration
  - !!python/tuple
    - 1637
    - 1670
    - 1670
    - 1722
    - elaboration
  - !!python/tuple
    - 1686
    - 1722
    - 1670
    - 1686
    - concession
  - !!python/tuple
    - 1670
    - 1680
    - 1680
    - 1686
    - elaboration
  - !!python/tuple
    - 1688
    - 1722
    - 1686
    - 1688
    - attribution
  - !!python/tuple
    - 1688
    - 1700
    - 1700
    - 1722
    - elaboration
  - !!python/tuple
    - 1597
    - 1722
    - 1722
    - 1934
    - elaboration
  - !!python/tuple
    - 1724
    - 1752
    - 1722
    - 1724
    - attribution
  - !!python/tuple
    - 1726
    - 1752
    - 1724
    - 1726
    - attribution
  - !!python/tuple
    - 1726
    - 1741
    - 1741
    - 1752
    - elaboration
  - !!python/tuple
    - 1722
    - 1752
    - 1752
    - 1934
    - elaboration
  - !!python/tuple
    - 1752
    - 1809
    - 1809
    - 1934
    - topic
  - !!python/tuple
    - 1752
    - 1758
    - 1758
    - 1809
    - elaboration
  - !!python/tuple
    - 1758
    - 1767
    - 1767
    - 1790
    - elaboration
  - !!python/tuple
    - 1758
    - 1790
    - 1790
    - 1809
    - elaboration
  - !!python/tuple
    - 1790
    - 1797
    - 1797
    - 1809
    - elaboration
  - !!python/tuple
    - 1809
    - 1934
    - 1752
    - 1809
    - topic
  - !!python/tuple
    - 1809
    - 1835
    - 1835
    - 1843
    - elaboration
  - !!python/tuple
    - 1835
    - 1842
    - 1842
    - 1843
    - purpose
  - !!python/tuple
    - 1809
    - 1843
    - 1843
    - 1852
    - elaboration
  - !!python/tuple
    - 1809
    - 1852
    - 1852
    - 1934
    - elaboration
  - !!python/tuple
    - 1852
    - 1856
    - 1856
    - 1880
    - elaboration
  - !!python/tuple
    - 1856
    - 1859
    - 1859
    - 1880
    - elaboration
  - !!python/tuple
    - 1852
    - 1880
    - 1880
    - 1934
    - elaboration
  - !!python/tuple
    - 1880
    - 1891
    - 1891
    - 1912
    - purpose
  - !!python/tuple
    - 1893
    - 1912
    - 1891
    - 1893
    - attribution
  - !!python/tuple
    - 1893
    - 1904
    - 1904
    - 1912
    - same_unit
  - !!python/tuple
    - 1893
    - 1898
    - 1898
    - 1904
    - elaboration
  - !!python/tuple
    - 1904
    - 1912
    - 1893
    - 1904
    - same_unit
  - !!python/tuple
    - 1880
    - 1912
    - 1912
    - 1934
    - elaboration
  - !!python/tuple
    - 1912
    - 1922
    - 1922
    - 1934
    - elaboration
  tokens:
  - This
  - paper
  - presents
  - a
  - very
  - interesting
  - investigation
  - of
  - the
  - expressive
  - capabilities
  - of
  - graph
  - neural
  - networks
  - ','
  - in
  - particular
  - focusing
  - 'on'
  - the
  - discriminative
  - power
  - of
  - such
  - GNN
  - models
  - ','
  - i.e.
  - the
  - ability
  - to
  - tell
  - that
  - two
  - inputs
  - are
  - different
  - when
  - they
  - are
  - actually
  - different
  - .
  - The
  - analysis
  - is
  - based
  - 'on'
  - the
  - study
  - of
  - injective
  - representation
  - functions
  - 'on'
  - multisets
  - .
  - This
  - perspective
  - in
  - particular
  - allows
  - the
  - authors
  - to
  - distinguish
  - different
  - aggregation
  - methods
  - ','
  - sum
  - ','
  - mean
  - and
  - max
  - ','
  - as
  - well
  - as
  - to
  - distinguish
  - one
  - layer
  - linear
  - transformations
  - from
  - multi-layer
  - MLPs
  - .
  - Based
  - 'on'
  - the
  - analysis
  - the
  - authors
  - proposed
  - a
  - variant
  - of
  - the
  - GNN
  - called
  - Graph
  - Isomorphism
  - Networks
  - -LRB-
  - GINs
  - -RRB-
  - that
  - use
  - MLPs
  - instead
  - of
  - linear
  - transformations
  - 'on'
  - each
  - layer
  - ','
  - and
  - sum
  - instead
  - of
  - mean
  - or
  - max
  - as
  - the
  - aggregation
  - method
  - ','
  - which
  - has
  - the
  - most
  - discriminative
  - power
  - following
  - the
  - analysis
  - .
  - Experiments
  - were
  - done
  - 'on'
  - node
  - classification
  - benchmarks
  - to
  - support
  - the
  - claims
  - .
  - Overall
  - I
  - quite
  - liked
  - this
  - paper
  - .
  - The
  - study
  - of
  - the
  - expressive
  - capabilities
  - of
  - GNNs
  - is
  - a
  - very
  - important
  - problem
  - .
  - Given
  - the
  - popularity
  - of
  - this
  - class
  - of
  - models
  - recently
  - ','
  - theoretical
  - analysis
  - for
  - these
  - models
  - is
  - largely
  - missing
  - .
  - Previous
  - attempts
  - at
  - studying
  - the
  - capability
  - of
  - GNNs
  - focus
  - 'on'
  - the
  - function
  - approximation
  - perspective
  - -LRB-
  - e.g.
  - Mapping
  - Images
  - to
  - Scene
  - Graphs
  - with
  - Permutation-Invariant
  - Structured
  - Prediction
  - by
  - Hertiz
  - et
  - al.
  - which
  - is
  - worth
  - discussing
  - -RRB-
  - .
  - This
  - paper
  - presents
  - a
  - very
  - different
  - angle
  - focusing
  - 'on'
  - discriminative
  - capabilities
  - .
  - Being
  - able
  - to
  - tell
  - two
  - inputs
  - apart
  - when
  - they
  - are
  - different
  - is
  - obviously
  - just
  - one
  - aspect
  - of
  - representation
  - power
  - ','
  - but
  - this
  - paper
  - showed
  - that
  - studying
  - this
  - aspect
  - can
  - already
  - give
  - us
  - some
  - interesting
  - insights
  - .
  - I
  - do
  - feel
  - however
  - that
  - the
  - authors
  - should
  - make
  - it
  - clear
  - that
  - discriminative
  - power
  - is
  - not
  - the
  - only
  - thing
  - we
  - care
  - ','
  - and
  - in
  - most
  - applications
  - we
  - are
  - not
  - doing
  - graph
  - isomorphism
  - tests
  - .
  - The
  - ability
  - to
  - tell
  - ','
  - for
  - example
  - ','
  - how
  - far
  - two
  - inputs
  - are
  - ','
  - when
  - they
  - are
  - not
  - the
  - same
  - is
  - also
  - very
  - -LRB-
  - and
  - maybe
  - more
  - -RRB-
  - important
  - ','
  - which
  - such
  - isomorphism
  - /
  - injective
  - map
  - based
  - analysis
  - does
  - not
  - capture
  - at
  - all
  - .
  - In
  - fact
  - the
  - assumption
  - that
  - each
  - feature
  - vector
  - can
  - be
  - mapped
  - to
  - a
  - unique
  - label
  - in
  - -LCB-
  - a
  - ','
  - b
  - ','
  - c
  - ','
  - '...'
  - -RCB-
  - -LRB-
  - Section
  - '3'
  - first
  - paragraph
  - -RRB-
  - is
  - overly
  - simplistic
  - and
  - only
  - makes
  - sense
  - for
  - analyzing
  - injective
  - maps
  - .
  - If
  - we
  - want
  - to
  - reason
  - anything
  - about
  - the
  - continuity
  - of
  - the
  - features
  - and
  - representations
  - ','
  - this
  - assumption
  - does
  - not
  - apply
  - ','
  - and
  - the
  - real
  - set
  - is
  - not
  - countable
  - so
  - such
  - a
  - mapping
  - can
  - not
  - exist
  - .
  - In
  - equation
  - '4.1'
  - describes
  - the
  - GIN
  - update
  - ','
  - which
  - is
  - proposed
  - as
  - '``'
  - the
  - most
  - powerful
  - GNN
  - ''''''
  - .
  - However
  - ','
  - such
  - architecture
  - is
  - not
  - really
  - new
  - ','
  - for
  - example
  - the
  - Interaction
  - Networks
  - -LRB-
  - Battaglia
  - et
  - al.
  - '2016'
  - -RRB-
  - already
  - uses
  - sum
  - aggregation
  - and
  - MLP
  - as
  - the
  - building
  - blocks
  - .
  - Also
  - ','
  - it
  - is
  - said
  - that
  - in
  - the
  - first
  - iteration
  - a
  - simple
  - sum
  - is
  - enough
  - to
  - implement
  - injective
  - map
  - ','
  - this
  - is
  - 'true'
  - for
  - sum
  - ','
  - but
  - replacing
  - that
  - with
  - mean
  - and
  - max
  - can
  - lose
  - information
  - very
  - early
  - 'on'
  - .
  - Another
  - MLP
  - 'on'
  - the
  - input
  - features
  - at
  - least
  - for
  - mean
  - or
  - max
  - aggregation
  - for
  - the
  - first
  - iteration
  - is
  - therefore
  - necessary
  - .
  - This
  - is
  - n't
  - made
  - very
  - clear
  - in
  - the
  - paper
  - .
  - The
  - training
  - set
  - results
  - presented
  - in
  - section
  - '6.1'
  - is
  - not
  - very
  - clear
  - .
  - The
  - plots
  - show
  - only
  - one
  - run
  - for
  - each
  - model
  - variant
  - ','
  - which
  - run
  - was
  - it
  - '?'
  - As
  - the
  - purpose
  - is
  - to
  - show
  - that
  - some
  - variants
  - fit
  - well
  - ','
  - and
  - some
  - others
  - overfit
  - ','
  - these
  - runs
  - should
  - be
  - chosen
  - to
  - optimize
  - training
  - set
  - performance
  - ','
  - rather
  - than
  - generalization
  - .
  - Also
  - the
  - restrictions
  - should
  - be
  - made
  - clear
  - that
  - all
  - models
  - are
  - given
  - the
  - same
  - -LRB-
  - small
  - -RRB-
  - amount
  - of
  - hidden
  - units
  - per
  - node
  - .
  - I
  - imagine
  - if
  - the
  - amount
  - of
  - hidden
  - units
  - are
  - allowed
  - to
  - be
  - much
  - bigger
  - ','
  - mean
  - and
  - max
  - aggregators
  - should
  - also
  - catch
  - up
  - .
  - As
  - mentioned
  - earlier
  - I
  - quite
  - liked
  - the
  - paper
  - despite
  - some
  - restrictions
  - anc
  - things
  - to
  - clarify
  - .
  - I
  - would
  - vote
  - for
  - accepting
  - this
  - paper
  - for
  - publication
  - at
  - ICLR
  - .
  - '--------'
  - Considering
  - the
  - counter-example
  - given
  - above
  - ','
  - I
  - '''m'
  - lowering
  - my
  - scores
  - a
  - bit
  - .
  - The
  - proof
  - of
  - theorem
  - '3'
  - is
  - less
  - than
  - clear
  - .
  - The
  - proof
  - for
  - the
  - first
  - half
  - of
  - theorem
  - '3'
  - -LRB-
  - a
  - -RRB-
  - is
  - quite
  - obvious
  - ','
  - but
  - the
  - proof
  - for
  - the
  - second
  - half
  - is
  - a
  - bit
  - hand-wavy
  - .
  - In
  - the
  - worst
  - case
  - ','
  - the
  - second
  - half
  - of
  - theorem
  - '3'
  - -LRB-
  - a
  - -RRB-
  - will
  - be
  - invalid
  - .
  - The
  - most
  - general
  - GNN
  - will
  - then
  - have
  - to
  - use
  - an
  - update
  - function
  - in
  - the
  - form
  - of
  - the
  - first
  - half
  - of
  - '3'
  - -LRB-
  - a
  - -RRB-
  - ','
  - and
  - all
  - the
  - other
  - analysis
  - still
  - holds
  - .
  - The
  - experiments
  - will
  - need
  - to
  - be
  - rerun
  - .
  - '--------'
  - Update
  - ':'
  - the
  - new
  - revision
  - resolved
  - the
  - counter-example
  - issue
  - and
  - I
  - '''m'
  - mostly
  - happy
  - with
  - it
  - ','
  - so
  - my
  - rating
  - was
  - adjusted
  - again
  - .
  - We
  - thank
  - the
  - reviewer
  - for
  - the
  - positive
  - review
  - and
  - constructive
  - feedback
  - '!'
  - We
  - are
  - glad
  - that
  - the
  - reviewer
  - likes
  - our
  - paper
  - .
  - First
  - ','
  - we
  - completely
  - agree
  - that
  - the
  - ability
  - of
  - GNNs
  - to
  - capture
  - structural
  - similarity
  - of
  - graphs
  - is
  - very
  - important
  - besides
  - their
  - discriminative
  - power
  - ','
  - and
  - we
  - believe
  - this
  - is
  - one
  - of
  - the
  - most
  - important
  - benefits
  - of
  - using
  - GNNs
  - over
  - WL
  - kernel
  - .
  - We
  - have
  - now
  - made
  - this
  - point
  - clearer
  - in
  - Section
  - '4'
  - .
  - Furthermore
  - ','
  - we
  - emphasized
  - that
  - we
  - do
  - consider
  - node
  - features
  - to
  - lie
  - in
  - R
  - ^
  - d
  - so
  - that
  - they
  - can
  - capture
  - the
  - similarity
  - .
  - The
  - subtlety
  - is
  - that
  - -LRB-
  - as
  - R1
  - nicely
  - pointed
  - out
  - -RRB-
  - ','
  - we
  - need
  - a
  - common
  - assumption
  - that
  - node
  - features
  - at
  - each
  - layer
  - are
  - from
  - countable
  - set
  - in
  - R
  - ^
  - d
  - -LRB-
  - not
  - from
  - the
  - entire
  - R
  - ^
  - d
  - -RRB-
  - .
  - This
  - is
  - satisfied
  - if
  - the
  - input
  - node
  - features
  - are
  - from
  - a
  - countable
  - set
  - ','
  - because
  - for
  - a
  - graph
  - neural
  - network
  - ','
  - countability
  - propagates
  - across
  - all
  - layers
  - in
  - a
  - GNN
  - .
  - We
  - leave
  - uncountable
  - node
  - input
  - features
  - for
  - future
  - work
  - and
  - add
  - a
  - more
  - detailed
  - discussion
  - in
  - Section
  - '4'
  - of
  - the
  - revised
  - paper
  - .
  - In
  - the
  - following
  - ','
  - we
  - respond
  - to
  - R3
  - '''s'
  - other
  - helpful
  - comments
  - and
  - suggestions
  - ':'
  - '1'
  - .
  - RE
  - ':'
  - Architecture
  - is
  - similar
  - to
  - ','
  - e.g.
  - ','
  - Interaction
  - Networks
  - Thank
  - you
  - for
  - the
  - pointers
  - .
  - Some
  - of
  - our
  - GIN
  - '''s'
  - building
  - blocks
  - ','
  - e.g.
  - sum
  - and
  - MLP
  - indeed
  - appeared
  - in
  - other
  - architectures
  - .
  - We
  - emphasize
  - that
  - while
  - previous
  - work
  - tend
  - to
  - be
  - somewhat
  - ad-hoc
  - in
  - designing
  - GNN
  - architectures
  - ','
  - our
  - main
  - emphasis
  - is
  - 'on'
  - deriving
  - our
  - GIN
  - architecture
  - based
  - 'on'
  - the
  - theoretical
  - motivation
  - .
  - In
  - Section
  - '6'
  - of
  - the
  - revised
  - version
  - ','
  - we
  - mention
  - related
  - GNN
  - architectures
  - and
  - discuss
  - the
  - differences
  - .
  - '2'
  - .
  - RE
  - ':'
  - Using
  - MLP
  - for
  - mean
  - or
  - max
  - in
  - the
  - initial
  - step
  - is
  - more
  - fair
  - '?'
  - We
  - think
  - there
  - might
  - be
  - a
  - slight
  - misunderstanding
  - here
  - ':'
  - as
  - we
  - discussed
  - with
  - concrete
  - examples
  - in
  - Section
  - '5.2'
  - ','
  - mean
  - or
  - max
  - pooling
  - are
  - inherently
  - incapable
  - of
  - capturing
  - the
  - multiset
  - information
  - regardless
  - of
  - the
  - use
  - of
  - MLP
  - .
  - Especially
  - ','
  - in
  - our
  - experiments
  - ','
  - we
  - use
  - one-hot
  - encodings
  - as
  - input
  - node
  - features
  - ','
  - so
  - the
  - use
  - of
  - MLP
  - 'on'
  - top
  - of
  - them
  - does
  - not
  - increase
  - the
  - discriminative
  - power
  - of
  - mean/max
  - pooling
  - .
  - '3'
  - .
  - RE
  - ':'
  - Training
  - set
  - results
  - optimized
  - for
  - test
  - performance
  - '?'
  - The
  - results
  - were
  - not
  - actually
  - optimized
  - for
  - test
  - performance
  - .
  - Instead
  - ','
  - we
  - used
  - exactly
  - the
  - same
  - configurations
  - for
  - all
  - the
  - datasets
  - ':'
  - For
  - all
  - the
  - GNNs
  - ','
  - the
  - same
  - configurations
  - were
  - used
  - across
  - datasets
  - ':'
  - '5'
  - GNN
  - layers
  - -LRB-
  - including
  - the
  - input
  - layer
  - -RRB-
  - ','
  - hidden
  - units
  - of
  - size
  - '64'
  - ','
  - minibatch
  - of
  - size
  - '128'
  - ','
  - and
  - '0.5'
  - dropout
  - ratio
  - .
  - For
  - the
  - WL
  - subtree
  - kernel
  - ','
  - we
  - set
  - the
  - number
  - of
  - iterations
  - to
  - '4'
  - ','
  - which
  - is
  - comparable
  - to
  - the
  - '5'
  - GNN
  - layers
  - .
  - We
  - clarified
  - this
  - in
  - Figure
  - '6'
  - of
  - the
  - revised
  - paper
  - .
  - I
  - thank
  - the
  - authors
  - for
  - the
  - revision
  - of
  - the
  - paper
  - and
  - the
  - response
  - .
  - I
  - have
  - readjusted
  - my
  - rating
  - .
  - The
  - solution
  - to
  - the
  - question
  - raised
  - by
  - the
  - counter
  - example
  - in
  - the
  - new
  - equation
  - -LRB-
  - '4.1'
  - -RRB-
  - is
  - a
  - technical
  - one
  - ','
  - I
  - would
  - rather
  - prefer
  - not
  - to
  - simplify
  - the
  - function
  - g
  - -LRB-
  - c
  - ','
  - X
  - -RRB-
  - which
  - uses
  - two
  - functions
  - phi
  - and
  - f
  - in
  - this
  - form
  - ','
  - as
  - it
  - really
  - does
  - n't
  - buy
  - us
  - much
  - .
  - W.r.t.
  - related
  - work
  - ','
  - the
  - statement
  - '``'
  - Not
  - surprisingly
  - ','
  - some
  - building
  - blocks
  - of
  - GIN
  - ','
  - e.g.
  - sum
  - aggregation
  - and
  - MLP
  - encoding
  - ','
  - also
  - appeared
  - in
  - other
  - models
  - ''''''
  - -LRB-
  - section
  - '6'
  - -RRB-
  - is
  - not
  - fair
  - and
  - misleading
  - .
  - As
  - it
  - is
  - not
  - the
  - case
  - that
  - '``'
  - some
  - building
  - blocks
  - ''''''
  - also
  - appear
  - in
  - other
  - models
  - ','
  - but
  - rather
  - some
  - other
  - models
  - ','
  - like
  - interaction
  - networks
  - ','
  - already
  - contains
  - '``'
  - all
  - ''''''
  - the
  - essential
  - building
  - blocks
  - -LRB-
  - sum
  - ','
  - MLP
  - ','
  - etc.
  - -RRB-
  - presented
  - in
  - this
  - paper
  - .
  - This
  - does
  - n't
  - undermine
  - the
  - theoretical
  - contribution
  - of
  - this
  - paper
  - ','
  - but
  - the
  - authors
  - should
  - be
  - fair
  - to
  - previous
  - work
  - .
  - Thank
  - you
  - for
  - the
  - encouraging
  - review
  - '!'
  - We
  - respond
  - to
  - your
  - further
  - comments
  - below
  - .
  - '1'
  - -RRB-
  - We
  - probably
  - do
  - not
  - fully
  - understand
  - your
  - comment
  - regarding
  - Eqn
  - -LRB-
  - '4.1'
  - -RRB-
  - and
  - g
  - -LRB-
  - c
  - ','
  - X
  - -RRB-
  - .
  - Especially
  - ','
  - could
  - you
  - please
  - clarify
  - your
  - meaning
  - of
  - '``'
  - simplify
  - g
  - -LRB-
  - c
  - ','
  - X
  - -RRB-
  - ''''''
  - '?'
  - In
  - our
  - GIN
  - in
  - Eqn
  - -LRB-
  - '4.1'
  - -RRB-
  - ','
  - we
  - compose
  - phi
  - and
  - f
  - in
  - Corollary
  - '6'
  - .
  - '2'
  - -RRB-
  - We
  - will
  - further
  - edit
  - related
  - work
  - according
  - to
  - your
  - suggestions
  - .
  - Interaction
  - Networks
  - is
  - a
  - great
  - work
  - and
  - we
  - like
  - it
  - .
  - What
  - I
  - meant
  - was
  - ','
  - in
  - g
  - -LRB-
  - c
  - ','
  - X
  - -RRB-
  - you
  - have
  - two
  - functions
  - phi
  - and
  - f
  - ','
  - which
  - is
  - the
  - form
  - required
  - by
  - Theorem
  - '3'
  - .
  - The
  - problem
  - of
  - the
  - counter-example
  - comes
  - in
  - when
  - you
  - used
  - a
  - single
  - function
  - instead
  - of
  - '2'
  - functions
  - ','
  - which
  - ignores
  - the
  - difference
  - between
  - the
  - node
  - at
  - the
  - center
  - and
  - all
  - its
  - neighbors
  - .
  - Introducing
  - an
  - epsilon
  - is
  - a
  - technical
  - solution
  - to
  - this
  - problem
  - -LRB-
  - in
  - my
  - opinion
  - -RRB-
  - ','
  - I
  - think
  - you
  - actually
  - do
  - n't
  - need
  - this
  - because
  - the
  - original
  - form
  - of
  - g
  - -LRB-
  - c
  - ','
  - X
  - -RRB-
  - is
  - enough
  - ','
  - and
  - using
  - a
  - single
  - function
  - rather
  - than
  - '2'
  - does
  - not
  - save
  - you
  - much
  - .
  - Note
  - ':'
  - I
  - think
  - of
  - phi
  - and
  - f
  - as
  - MLPs
  - ','
  - ''''''
  - ','
  - ''''''
  - as
  - concat
  - ','
  - and
  - ''''''
  - -LCB-
  - -RCB-
  - ''''''
  - as
  - some
  - aggregation
  - operator
  - ','
  - like
  - sum
  - .
  - Thank
  - you
  - for
  - the
  - clarification
  - .
  - We
  - would
  - like
  - to
  - first
  - clarify
  - that
  - the
  - letters
  - -LRB-
  - phi
  - ','
  - f
  - -RRB-
  - in
  - Corollary
  - '6'
  - and
  - Theorem
  - '3'
  - do
  - not
  - have
  - direct
  - correspondence
  - ;
  - but
  - we
  - can
  - easily
  - rearrange
  - Eqn
  - -LRB-
  - '4.1'
  - -RRB-
  - to
  - obtain
  - the
  - corresponding
  - -LRB-
  - phi
  - ','
  - f
  - -RRB-
  - in
  - the
  - form
  - of
  - Theorem
  - '3'
  - .
  - Intuitively
  - ','
  - what
  - Theorem
  - '3'
  - asks
  - for
  - is
  - to
  - injectively
  - represent
  - a
  - pair
  - of
  - a
  - node
  - and
  - its
  - neighbors
  - ','
  - so
  - the
  - injective
  - function
  - ','
  - g
  - -LRB-
  - c
  - ','
  - X
  - -RRB-
  - ','
  - corresponds
  - to
  - -LRB-
  - phi
  - ','
  - f
  - -RRB-
  - in
  - Theorem
  - '3'
  - .
  - Furthermore
  - ','
  - our
  - motivation
  - for
  - designing
  - Eqn
  - -LRB-
  - '4.1'
  - -RRB-
  - ','
  - i.e.
  - GIN-0
  - and
  - GIN-eps
  - ','
  - rather
  - than
  - simply
  - applying
  - concatenation
  - ','
  - is
  - for
  - better
  - empirical
  - performance
  - .
  - In
  - our
  - preliminary
  - experiments
  - ','
  - we
  - found
  - such
  - concatenation
  - was
  - harder
  - to
  - train
  - compared
  - to
  - our
  - simple
  - GINs
  - -LRB-
  - both
  - GIN-0
  - and
  - GIN-eps
  - -RRB-
  - and
  - achieved
  - lower
  - test
  - accuracy
  - than
  - GINs
  - .
  - The
  - simplicity
  - of
  - GINs
  - brings
  - better
  - performance
  - in
  - practice
  - .
  - We
  - leave
  - the
  - extensive
  - investigation
  - and
  - comparison
  - to
  - our
  - future
  - work
  - .
- comment_id: B1e7cN_6RX
  rels:
  - !!python/tuple
    - 0
    - 17
    - 17
    - 29
    - condition
  - !!python/tuple
    - 0
    - 29
    - 29
    - 264
    - elaboration
  - !!python/tuple
    - 29
    - 34
    - 34
    - 200
    - elaboration
  - !!python/tuple
    - 34
    - 39
    - 39
    - 200
    - elaboration
  - !!python/tuple
    - 39
    - 47
    - 47
    - 75
    - elaboration
  - !!python/tuple
    - 47
    - 52
    - 52
    - 75
    - same_unit
  - !!python/tuple
    - 52
    - 75
    - 47
    - 52
    - same_unit
  - !!python/tuple
    - 52
    - 65
    - 65
    - 75
    - elaboration
  - !!python/tuple
    - 65
    - 70
    - 70
    - 75
    - list
  - !!python/tuple
    - 70
    - 75
    - 65
    - 70
    - list
  - !!python/tuple
    - 39
    - 75
    - 75
    - 200
    - elaboration
  - !!python/tuple
    - 75
    - 91
    - 91
    - 200
    - elaboration
  - !!python/tuple
    - 91
    - 111
    - 111
    - 200
    - elaboration
  - !!python/tuple
    - 111
    - 144
    - 144
    - 200
    - list
  - !!python/tuple
    - 111
    - 125
    - 125
    - 144
    - contrast
  - !!python/tuple
    - 125
    - 144
    - 111
    - 125
    - contrast
  - !!python/tuple
    - 125
    - 126
    - 126
    - 144
    - manner
  - !!python/tuple
    - 144
    - 200
    - 111
    - 144
    - list
  - !!python/tuple
    - 144
    - 150
    - 150
    - 175
    - elaboration
  - !!python/tuple
    - 157
    - 175
    - 150
    - 157
    - purpose
  - !!python/tuple
    - 157
    - 164
    - 164
    - 175
    - elaboration
  - !!python/tuple
    - 144
    - 175
    - 175
    - 200
    - elaboration
  - !!python/tuple
    - 175
    - 180
    - 180
    - 200
    - purpose
  - !!python/tuple
    - 180
    - 193
    - 193
    - 200
    - elaboration
  - !!python/tuple
    - 29
    - 200
    - 200
    - 264
    - elaboration
  - !!python/tuple
    - 200
    - 209
    - 209
    - 231
    - list
  - !!python/tuple
    - 200
    - 206
    - 206
    - 209
    - elaboration
  - !!python/tuple
    - 209
    - 231
    - 200
    - 209
    - list
  - !!python/tuple
    - 211
    - 231
    - 209
    - 211
    - attribution
  - !!python/tuple
    - 211
    - 215
    - 215
    - 231
    - elaboration
  - !!python/tuple
    - 200
    - 231
    - 231
    - 264
    - elaboration
  - !!python/tuple
    - 231
    - 235
    - 235
    - 264
    - elaboration
  - !!python/tuple
    - 235
    - 241
    - 241
    - 264
    - elaboration
  - !!python/tuple
    - 241
    - 247
    - 247
    - 264
    - list
  - !!python/tuple
    - 247
    - 264
    - 241
    - 247
    - list
  - !!python/tuple
    - 247
    - 258
    - 258
    - 264
    - elaboration
  - !!python/tuple
    - 0
    - 264
    - 264
    - 456
    - elaboration
  - !!python/tuple
    - 264
    - 305
    - 305
    - 320
    - elaboration
  - !!python/tuple
    - 264
    - 320
    - 320
    - 456
    - elaboration
  - !!python/tuple
    - 320
    - 360
    - 360
    - 456
    - list
  - !!python/tuple
    - 320
    - 325
    - 325
    - 360
    - reason
  - !!python/tuple
    - 325
    - 351
    - 351
    - 360
    - list
  - !!python/tuple
    - 325
    - 339
    - 339
    - 351
    - elaboration
  - !!python/tuple
    - 351
    - 360
    - 325
    - 351
    - list
  - !!python/tuple
    - 360
    - 456
    - 320
    - 360
    - list
  - !!python/tuple
    - 360
    - 366
    - 366
    - 369
    - elaboration
  - !!python/tuple
    - 360
    - 369
    - 369
    - 456
    - elaboration
  - !!python/tuple
    - 369
    - 380
    - 380
    - 456
    - elaboration
  - !!python/tuple
    - 380
    - 389
    - 389
    - 456
    - elaboration
  - !!python/tuple
    - 389
    - 394
    - 394
    - 456
    - list
  - !!python/tuple
    - 394
    - 456
    - 389
    - 394
    - list
  - !!python/tuple
    - 394
    - 405
    - 405
    - 456
    - same_unit
  - !!python/tuple
    - 394
    - 399
    - 399
    - 405
    - elaboration
  - !!python/tuple
    - 405
    - 456
    - 394
    - 405
    - same_unit
  - !!python/tuple
    - 408
    - 456
    - 405
    - 408
    - attribution
  - !!python/tuple
    - 408
    - 409
    - 409
    - 433
    - elaboration
  - !!python/tuple
    - 409
    - 422
    - 422
    - 433
    - elaboration
  - !!python/tuple
    - 408
    - 433
    - 433
    - 456
    - elaboration
  tokens:
  - This
  - paper
  - presents
  - and
  - empirical
  - and
  - theoretical
  - study
  - of
  - the
  - convergence
  - of
  - asynchronous
  - stochastic
  - gradient
  - descent
  - training
  - if
  - there
  - are
  - delays
  - due
  - to
  - the
  - asynchronous
  - part
  - of
  - it
  - .
  - The
  - paper
  - can
  - be
  - neatly
  - split
  - in
  - two
  - parts
  - ':'
  - a
  - simulation
  - study
  - and
  - a
  - theoretical
  - analysis
  - .
  - The
  - simulation
  - study
  - compares
  - ','
  - under
  - fixed
  - hyperparameters
  - ','
  - the
  - behavior
  - of
  - distributed
  - training
  - under
  - different
  - simulated
  - levels
  - of
  - delay
  - 'on'
  - different
  - problems
  - and
  - different
  - model
  - architectures
  - .
  - Overall
  - the
  - results
  - are
  - very
  - interesting
  - ','
  - but
  - the
  - simulation
  - could
  - have
  - been
  - more
  - thorough
  - .
  - Specifically
  - ','
  - the
  - same
  - hyperparameter
  - values
  - were
  - used
  - across
  - batch
  - sizes
  - and
  - across
  - different
  - values
  - of
  - the
  - distributed
  - delay
  - .
  - Some
  - algorithms
  - failed
  - to
  - converge
  - under
  - some
  - settings
  - and
  - others
  - experienced
  - dramatic
  - slowdowns
  - ','
  - but
  - without
  - careful
  - study
  - of
  - hyperparameters
  - it
  - '''s'
  - hard
  - to
  - tell
  - whether
  - these
  - behaviors
  - are
  - normal
  - or
  - outliers
  - .
  - Also
  - it
  - would
  - have
  - been
  - interesting
  - to
  - see
  - a
  - recurrent
  - architecture
  - there
  - ','
  - as
  - I
  - '''ve'
  - heard
  - much
  - anecdotal
  - evidence
  - about
  - the
  - robustness
  - of
  - RNNs
  - and
  - LSTMs
  - to
  - asynchronous
  - training
  - .
  - I
  - strongly
  - advise
  - the
  - authors
  - to
  - redo
  - the
  - experiments
  - with
  - some
  - hyperparameter
  - tuning
  - for
  - different
  - levels
  - of
  - staleness
  - to
  - make
  - these
  - results
  - more
  - believable
  - .
  - The
  - theoretical
  - analysis
  - identifies
  - a
  - quantity
  - called
  - gradient
  - coherence
  - and
  - proves
  - that
  - a
  - learning
  - rate
  - based
  - 'on'
  - the
  - coherence
  - can
  - lead
  - to
  - an
  - optimal
  - convergence
  - rate
  - even
  - under
  - asynchronous
  - training
  - .
  - The
  - proof
  - is
  - correct
  - -LRB-
  - I
  - checked
  - the
  - major
  - steps
  - but
  - not
  - all
  - details
  - -RRB-
  - ','
  - and
  - it
  - '''s'
  - sufficiently
  - different
  - from
  - the
  - analysis
  - of
  - hogwild
  - algorithms
  - to
  - be
  - of
  - independent
  - interest
  - .
  - The
  - paper
  - also
  - shows
  - the
  - empirical
  - behavior
  - of
  - the
  - gradient
  - coherence
  - statistic
  - during
  - model
  - training
  - ;
  - interestingly
  - this
  - seems
  - to
  - also
  - explain
  - the
  - heuristic
  - commonly
  - believed
  - that
  - to
  - make
  - asynchronous
  - training
  - work
  - one
  - needs
  - to
  - slowly
  - anneal
  - the
  - number
  - of
  - workers
  - -LRB-
  - coherence
  - is
  - much
  - worse
  - in
  - the
  - earlier
  - than
  - later
  - phases
  - of
  - training
  - -RRB-
  - .
  - This
  - quantity
  - is
  - interesting
  - also
  - because
  - it
  - '''s'
  - somewhat
  - independent
  - of
  - the
  - variance
  - of
  - the
  - stochastic
  - gradient
  - across
  - minibatches
  - -LRB-
  - it
  - '''s'
  - the
  - time
  - variance
  - ','
  - in
  - a
  - way
  - -RRB-
  - ','
  - and
  - further
  - analysis
  - might
  - also
  - show
  - interesting
  - results
  - .
  - LSTM
  - is
  - indeed
  - an
  - interesting
  - piece
  - to
  - add
  - .
  - We
  - have
  - added
  - new
  - results
  - 'on'
  - LSTMs
  - in
  - Appendix
  - A.
  - '8'
  - --
  - we
  - vary
  - the
  - number
  - of
  - layers
  - of
  - LSTMs
  - -LRB-
  - see
  - Figure
  - '13'
  - -RRB-
  - and
  - types
  - of
  - SGD
  - algorithms
  - -LRB-
  - see
  - Figure
  - '14'
  - -RRB-
  - ','
  - and
  - have
  - observed
  - that
  - -LRB-
  - '1'
  - -RRB-
  - staleness
  - impacts
  - deeper
  - network
  - variants
  - more
  - than
  - shallower
  - counterparts
  - ','
  - which
  - is
  - consistent
  - with
  - our
  - observation
  - in
  - CNNs
  - and
  - DNNs
  - ;
  - -LRB-
  - '2'
  - -RRB-
  - different
  - algorithms
  - respond
  - to
  - staleness
  - differently
  - ','
  - with
  - SGD
  - and
  - Adam
  - more
  - robust
  - to
  - staleness
  - than
  - Momentum
  - and
  - RMSProp
  - .
- comment_id: B1eHKRmEe4
  rels:
  - !!python/tuple
    - 0
    - 18
    - 18
    - 863
    - topic
  - !!python/tuple
    - 0
    - 7
    - 7
    - 8
    - elaboration
  - !!python/tuple
    - 0
    - 8
    - 8
    - 18
    - elaboration
  - !!python/tuple
    - 18
    - 863
    - 0
    - 18
    - topic
  - !!python/tuple
    - 18
    - 143
    - 143
    - 863
    - topic
  - !!python/tuple
    - 18
    - 39
    - 39
    - 50
    - same_unit
  - !!python/tuple
    - 18
    - 29
    - 29
    - 39
    - elaboration
  - !!python/tuple
    - 39
    - 50
    - 18
    - 39
    - same_unit
  - !!python/tuple
    - 40
    - 50
    - 39
    - 40
    - attribution
  - !!python/tuple
    - 49
    - 50
    - 40
    - 49
    - attribution
  - !!python/tuple
    - 18
    - 50
    - 50
    - 58
    - elaboration
  - !!python/tuple
    - 18
    - 58
    - 58
    - 143
    - elaboration
  - !!python/tuple
    - 58
    - 70
    - 70
    - 71
    - elaboration
  - !!python/tuple
    - 58
    - 71
    - 71
    - 78
    - elaboration
  - !!python/tuple
    - 58
    - 78
    - 78
    - 143
    - elaboration
  - !!python/tuple
    - 80
    - 97
    - 78
    - 80
    - attribution
  - !!python/tuple
    - 78
    - 97
    - 97
    - 143
    - elaboration
  - !!python/tuple
    - 108
    - 113
    - 97
    - 108
    - attribution
  - !!python/tuple
    - 97
    - 113
    - 113
    - 143
    - elaboration
  - !!python/tuple
    - 113
    - 115
    - 115
    - 121
    - elaboration
  - !!python/tuple
    - 115
    - 116
    - 116
    - 120
    - elaboration
  - !!python/tuple
    - 115
    - 120
    - 120
    - 121
    - elaboration
  - !!python/tuple
    - 113
    - 121
    - 121
    - 143
    - elaboration
  - !!python/tuple
    - 121
    - 129
    - 129
    - 143
    - elaboration
  - !!python/tuple
    - 129
    - 132
    - 132
    - 143
    - elaboration
  - !!python/tuple
    - 132
    - 137
    - 137
    - 143
    - purpose
  - !!python/tuple
    - 143
    - 863
    - 18
    - 143
    - topic
  - !!python/tuple
    - 143
    - 181
    - 181
    - 863
    - list
  - !!python/tuple
    - 143
    - 146
    - 146
    - 152
    - elaboration
  - !!python/tuple
    - 143
    - 152
    - 152
    - 181
    - elaboration
  - !!python/tuple
    - 152
    - 167
    - 167
    - 181
    - same_unit
  - !!python/tuple
    - 152
    - 159
    - 159
    - 167
    - sequence
  - !!python/tuple
    - 159
    - 167
    - 152
    - 159
    - sequence
  - !!python/tuple
    - 167
    - 181
    - 152
    - 167
    - same_unit
  - !!python/tuple
    - 167
    - 180
    - 180
    - 181
    - same_unit
  - !!python/tuple
    - 167
    - 171
    - 171
    - 180
    - elaboration
  - !!python/tuple
    - 171
    - 175
    - 175
    - 180
    - elaboration
  - !!python/tuple
    - 180
    - 181
    - 167
    - 180
    - same_unit
  - !!python/tuple
    - 181
    - 863
    - 143
    - 181
    - list
  - !!python/tuple
    - 181
    - 189
    - 189
    - 863
    - textualorganization
  - !!python/tuple
    - 189
    - 863
    - 181
    - 189
    - textualorganization
  - !!python/tuple
    - 189
    - 209
    - 209
    - 863
    - same_unit
  - !!python/tuple
    - 201
    - 209
    - 189
    - 201
    - condition
  - !!python/tuple
    - 209
    - 863
    - 189
    - 209
    - same_unit
  - !!python/tuple
    - 209
    - 219
    - 219
    - 863
    - list
  - !!python/tuple
    - 219
    - 863
    - 209
    - 219
    - list
  - !!python/tuple
    - 219
    - 240
    - 240
    - 863
    - textualorganization
  - !!python/tuple
    - 219
    - 224
    - 224
    - 230
    - elaboration
  - !!python/tuple
    - 219
    - 230
    - 230
    - 231
    - elaboration
  - !!python/tuple
    - 219
    - 231
    - 231
    - 240
    - temporal
  - !!python/tuple
    - 240
    - 863
    - 219
    - 240
    - textualorganization
  - !!python/tuple
    - 240
    - 243
    - 243
    - 863
    - textualorganization
  - !!python/tuple
    - 243
    - 863
    - 240
    - 243
    - textualorganization
  - !!python/tuple
    - 243
    - 255
    - 255
    - 863
    - textualorganization
  - !!python/tuple
    - 255
    - 863
    - 243
    - 255
    - textualorganization
  - !!python/tuple
    - 255
    - 268
    - 268
    - 863
    - textualorganization
  - !!python/tuple
    - 268
    - 863
    - 255
    - 268
    - textualorganization
  - !!python/tuple
    - 268
    - 316
    - 316
    - 863
    - list
  - !!python/tuple
    - 268
    - 284
    - 284
    - 316
    - elaboration
  - !!python/tuple
    - 284
    - 299
    - 299
    - 316
    - purpose
  - !!python/tuple
    - 299
    - 304
    - 304
    - 316
    - attribution
  - !!python/tuple
    - 316
    - 863
    - 268
    - 316
    - list
  - !!python/tuple
    - 316
    - 368
    - 368
    - 863
    - list
  - !!python/tuple
    - 322
    - 368
    - 316
    - 322
    - attribution
  - !!python/tuple
    - 322
    - 328
    - 328
    - 368
    - elaboration
  - !!python/tuple
    - 328
    - 338
    - 338
    - 368
    - elaboration
  - !!python/tuple
    - 338
    - 348
    - 348
    - 368
    - contrast
  - !!python/tuple
    - 348
    - 368
    - 338
    - 348
    - contrast
  - !!python/tuple
    - 348
    - 367
    - 367
    - 368
    - same_unit
  - !!python/tuple
    - 348
    - 361
    - 361
    - 367
    - elaboration
  - !!python/tuple
    - 367
    - 368
    - 348
    - 367
    - same_unit
  - !!python/tuple
    - 368
    - 863
    - 316
    - 368
    - list
  - !!python/tuple
    - 368
    - 373
    - 373
    - 390
    - elaboration
  - !!python/tuple
    - 373
    - 384
    - 384
    - 390
    - list
  - !!python/tuple
    - 384
    - 390
    - 373
    - 384
    - list
  - !!python/tuple
    - 368
    - 390
    - 390
    - 464
    - elaboration
  - !!python/tuple
    - 390
    - 404
    - 404
    - 417
    - elaboration
  - !!python/tuple
    - 390
    - 417
    - 417
    - 464
    - elaboration
  - !!python/tuple
    - 417
    - 422
    - 422
    - 428
    - purpose
  - !!python/tuple
    - 417
    - 428
    - 428
    - 464
    - explanation
  - !!python/tuple
    - 428
    - 435
    - 435
    - 454
    - purpose
  - !!python/tuple
    - 435
    - 438
    - 438
    - 454
    - purpose
  - !!python/tuple
    - 428
    - 454
    - 454
    - 464
    - elaboration
  - !!python/tuple
    - 454
    - 459
    - 459
    - 464
    - elaboration
  - !!python/tuple
    - 368
    - 464
    - 464
    - 863
    - elaboration
  - !!python/tuple
    - 470
    - 487
    - 464
    - 470
    - antithesis
  - !!python/tuple
    - 473
    - 487
    - 470
    - 473
    - attribution
  - !!python/tuple
    - 473
    - 486
    - 486
    - 487
    - same_unit
  - !!python/tuple
    - 473
    - 480
    - 480
    - 486
    - elaboration
  - !!python/tuple
    - 486
    - 487
    - 473
    - 486
    - same_unit
  - !!python/tuple
    - 464
    - 487
    - 487
    - 863
    - elaboration
  - !!python/tuple
    - 487
    - 521
    - 521
    - 863
    - topic
  - !!python/tuple
    - 487
    - 494
    - 494
    - 521
    - purpose
  - !!python/tuple
    - 494
    - 496
    - 496
    - 521
    - means
  - !!python/tuple
    - 496
    - 509
    - 509
    - 521
    - elaboration
  - !!python/tuple
    - 509
    - 510
    - 510
    - 521
    - elaboration
  - !!python/tuple
    - 521
    - 863
    - 487
    - 521
    - topic
  - !!python/tuple
    - 521
    - 528
    - 528
    - 557
    - elaboration
  - !!python/tuple
    - 528
    - 542
    - 542
    - 550
    - elaboration
  - !!python/tuple
    - 528
    - 550
    - 550
    - 557
    - elaboration
  - !!python/tuple
    - 521
    - 557
    - 557
    - 863
    - elaboration
  - !!python/tuple
    - 557
    - 565
    - 565
    - 580
    - elaboration
  - !!python/tuple
    - 565
    - 567
    - 567
    - 571
    - purpose
  - !!python/tuple
    - 565
    - 571
    - 571
    - 580
    - concession
  - !!python/tuple
    - 557
    - 580
    - 580
    - 863
    - result
  - !!python/tuple
    - 580
    - 594
    - 594
    - 863
    - list
  - !!python/tuple
    - 594
    - 863
    - 580
    - 594
    - list
  - !!python/tuple
    - 594
    - 604
    - 604
    - 605
    - elaboration
  - !!python/tuple
    - 594
    - 605
    - 605
    - 863
    - elaboration
  - !!python/tuple
    - 605
    - 606
    - 606
    - 863
    - textualorganization
  - !!python/tuple
    - 606
    - 863
    - 605
    - 606
    - textualorganization
  - !!python/tuple
    - 606
    - 822
    - 822
    - 863
    - list
  - !!python/tuple
    - 606
    - 626
    - 626
    - 822
    - list
  - !!python/tuple
    - 626
    - 822
    - 606
    - 626
    - list
  - !!python/tuple
    - 626
    - 630
    - 630
    - 643
    - elaboration
  - !!python/tuple
    - 630
    - 635
    - 635
    - 643
    - elaboration
  - !!python/tuple
    - 626
    - 643
    - 643
    - 822
    - elaboration
  - !!python/tuple
    - 643
    - 654
    - 654
    - 822
    - list
  - !!python/tuple
    - 643
    - 648
    - 648
    - 654
    - purpose
  - !!python/tuple
    - 648
    - 653
    - 653
    - 654
    - elaboration
  - !!python/tuple
    - 654
    - 822
    - 643
    - 654
    - list
  - !!python/tuple
    - 654
    - 656
    - 656
    - 662
    - elaboration
  - !!python/tuple
    - 656
    - 659
    - 659
    - 662
    - elaboration
  - !!python/tuple
    - 654
    - 662
    - 662
    - 822
    - elaboration
  - !!python/tuple
    - 662
    - 684
    - 684
    - 822
    - list
  - !!python/tuple
    - 662
    - 668
    - 668
    - 684
    - purpose
  - !!python/tuple
    - 684
    - 822
    - 662
    - 684
    - list
  - !!python/tuple
    - 684
    - 704
    - 704
    - 822
    - list
  - !!python/tuple
    - 704
    - 822
    - 684
    - 704
    - list
  - !!python/tuple
    - 704
    - 717
    - 717
    - 822
    - list
  - !!python/tuple
    - 717
    - 822
    - 704
    - 717
    - list
  - !!python/tuple
    - 717
    - 719
    - 719
    - 822
    - list
  - !!python/tuple
    - 719
    - 822
    - 717
    - 719
    - list
  - !!python/tuple
    - 719
    - 748
    - 748
    - 822
    - list
  - !!python/tuple
    - 719
    - 729
    - 729
    - 748
    - list
  - !!python/tuple
    - 729
    - 748
    - 719
    - 729
    - list
  - !!python/tuple
    - 729
    - 741
    - 741
    - 748
    - elaboration
  - !!python/tuple
    - 748
    - 822
    - 719
    - 748
    - list
  - !!python/tuple
    - 748
    - 750
    - 750
    - 822
    - list
  - !!python/tuple
    - 750
    - 822
    - 748
    - 750
    - list
  - !!python/tuple
    - 750
    - 769
    - 769
    - 822
    - list
  - !!python/tuple
    - 769
    - 822
    - 750
    - 769
    - list
  - !!python/tuple
    - 769
    - 771
    - 771
    - 822
    - list
  - !!python/tuple
    - 771
    - 822
    - 769
    - 771
    - list
  - !!python/tuple
    - 771
    - 792
    - 792
    - 801
    - elaboration
  - !!python/tuple
    - 771
    - 801
    - 801
    - 822
    - elaboration
  - !!python/tuple
    - 801
    - 812
    - 812
    - 822
    - elaboration
  - !!python/tuple
    - 822
    - 863
    - 606
    - 822
    - list
  - !!python/tuple
    - 822
    - 832
    - 832
    - 863
    - list
  - !!python/tuple
    - 832
    - 863
    - 822
    - 832
    - list
  - !!python/tuple
    - 832
    - 859
    - 859
    - 863
    - list
  - !!python/tuple
    - 844
    - 859
    - 832
    - 844
    - attribution
  - !!python/tuple
    - 844
    - 848
    - 848
    - 859
    - elaboration
  - !!python/tuple
    - 848
    - 854
    - 854
    - 859
    - elaboration
  - !!python/tuple
    - 859
    - 863
    - 832
    - 859
    - list
  tokens:
  - Overall
  - I
  - am
  - positive
  - about
  - this
  - manuscript
  - ':'
  - '-'
  - I
  - find
  - the
  - motivation
  - is
  - clear
  - and
  - valid
  - .
  - As
  - far
  - as
  - I
  - know
  - ','
  - this
  - is
  - a
  - novel
  - contribution
  - -LRB-
  - my
  - confidence
  - is
  - not
  - very
  - high
  - 'on'
  - that
  - one
  - though
  - '-'
  - I
  - might
  - be
  - unaware
  - of
  - related
  - work
  - -RRB-
  - .
  - '-'
  - The
  - paper
  - is
  - well-written
  - and
  - organized
  - .
  - '-'
  - Experiments
  - are
  - conducted
  - systematically
  - ','
  - although
  - certain
  - parts
  - could
  - be
  - better
  - explained
  - -LRB-
  - see
  - my
  - questions
  - below
  - -RRB-
  - .
  - I
  - think
  - this
  - paper
  - adds
  - an
  - original
  - and
  - valuable
  - angle
  - to
  - the
  - existing
  - literature
  - 'on'
  - data
  - poisoning
  - attacks
  - .
  - I
  - do
  - n't
  - see
  - any
  - major
  - flaws
  - ','
  - therefore
  - I
  - think
  - it
  - should
  - be
  - accepted
  - .
  - A
  - few
  - points
  - which
  - might
  - need
  - clarification
  - ':'
  - '-'
  - How
  - exactly
  - is
  - '``'
  - attack
  - success
  - ''''''
  - being
  - measured
  - '?'
  - '-'
  - Which
  - model
  - is
  - used
  - to
  - generate
  - the
  - adversarial
  - samples
  - '?'
  - Is
  - this
  - an
  - -LRB-
  - adversarially
  - -RRB-
  - pretrained
  - model
  - '?'
  - -LRB-
  - If
  - that
  - '''s'
  - the
  - case
  - ','
  - then
  - what
  - is
  - the
  - model
  - architecture
  - '?'
  - -RRB-
  - Or
  - are
  - adversarial
  - samples
  - generated
  - 'on'
  - the
  - fly
  - using
  - the
  - currently
  - trained/poisoned
  - model
  - '?'
  - '-'
  - At
  - the
  - end
  - of
  - Section
  - '4.4'
  - ':'
  - if
  - the
  - images
  - with
  - larger
  - noise
  - rely
  - more
  - 'on'
  - the
  - backdoor
  - ','
  - why
  - does
  - this
  - have
  - an
  - adverse
  - effect
  - '?'
  - Should
  - n't
  - it
  - increase
  - the
  - effectiveness
  - of
  - the
  - attack
  - '?'
  - '-'
  - Was
  - the
  - data
  - augmentation
  - -LRB-
  - flips
  - ','
  - crops
  - etc
  - -RRB-
  - performed
  - before
  - or
  - after
  - the
  - poisoning
  - pattern
  - was
  - applied
  - '?'
  - Minor
  - comments
  - ':'
  - '-'
  - definition
  - of
  - the
  - encoding
  - at
  - the
  - bottom
  - of
  - page
  - '4'
  - ':'
  - this
  - should
  - be
  - argmax
  - instead
  - of
  - max
  - '-'
  - typo
  - in
  - Sec.
  - '5.1'
  - ':'
  - '``'
  - to
  - evaluate
  - the
  - uat
  - a
  - wide
  - variety
  - ''''''
  - '-'
  - repetitive
  - sentence
  - in
  - Sec.
  - '5.2'
  - ':'
  - '``'
  - we
  - find
  - that
  - images
  - generated
  - with
  - $
  - \
  - tau
  - \
  - leq
  - '0.2'
  - $
  - remain
  - -LSB-
  - fairly
  - -RSB-
  - plausible
  - ''''''
  - We
  - thank
  - the
  - reviewer
  - for
  - the
  - kind
  - comments
  - and
  - helpful
  - suggestions
  - .
  - We
  - will
  - address
  - points
  - raised
  - below
  - ':'
  - '-'
  - The
  - attack
  - success
  - rate
  - -LRB-
  - ASR
  - -RRB-
  - is
  - computed
  - as
  - the
  - fraction
  - of
  - inputs
  - that
  - are
  - _
  - not
  - _
  - labeled
  - with
  - the
  - target
  - class
  - but
  - are
  - classified
  - as
  - the
  - target
  - class
  - after
  - the
  - backdoor
  - pattern
  - is
  - applied
  - -LRB-
  - Beginning
  - of
  - Section
  - '5'
  - -RRB-
  - .
  - We
  - have
  - edited
  - the
  - manuscript
  - to
  - make
  - this
  - definition
  - appear
  - more
  - prominently
  - earlier
  - in
  - the
  - paper
  - and
  - edited
  - the
  - relevant
  - captions
  - .
  - '-'
  - We
  - use
  - adversarially
  - trained
  - models
  - trained
  - with
  - the
  - publicly
  - available
  - code
  - from
  - https://github.com/MadryLab/cifar10_challenge
  - -LRB-
  - we
  - train
  - the
  - non-wide
  - variant
  - both
  - with
  - L2
  - and
  - Linf
  - -RRB-
  - .
  - The
  - adversarial
  - examples
  - are
  - generated
  - once
  - using
  - this
  - pre-trained
  - network
  - .
  - Since
  - our
  - threat
  - model
  - only
  - allows
  - us
  - to
  - add
  - examples
  - to
  - the
  - training
  - set
  - ','
  - we
  - can
  - not
  - compute
  - these
  - adversarial
  - perturbations
  - 'on'
  - the
  - fly
  - .
  - We
  - have
  - edited
  - the
  - manuscript
  - to
  - incorporate
  - this
  - discussion
  - .
  - '-'
  - We
  - were
  - also
  - surprised
  - initially
  - but
  - we
  - believe
  - that
  - there
  - is
  - a
  - fairly
  - simple
  - explanation
  - -LRB-
  - outlined
  - in
  - Section
  - '4.4'
  - -RRB-
  - .
  - 'On'
  - noisy
  - images
  - ','
  - the
  - classifier
  - learns
  - to
  - predict
  - by
  - relying
  - 'on'
  - the
  - backdoor
  - '*'
  - in
  - the
  - absence
  - of
  - strong
  - image
  - signal
  - '*'
  - -LRB-
  - since
  - the
  - salient
  - image
  - features
  - are
  - fairly
  - corrupted
  - -RRB-
  - .
  - However
  - ','
  - when
  - evaluated
  - 'on'
  - the
  - test
  - set
  - with
  - a
  - backdoor
  - applied
  - ','
  - the
  - image
  - itself
  - will
  - have
  - a
  - strong
  - signal
  - -LRB-
  - since
  - it
  - will
  - not
  - be
  - noisy
  - -RRB-
  - that
  - can
  - overcome
  - the
  - backdoor
  - pattern
  - .
  - Therefore
  - ','
  - it
  - is
  - necessary
  - for
  - the
  - classifier
  - to
  - learn
  - to
  - predict
  - the
  - backdoor
  - even
  - when
  - the
  - salient
  - image
  - characteristics
  - are
  - present
  - .
  - As
  - a
  - result
  - ','
  - random
  - noise
  - is
  - not
  - very
  - effective
  - at
  - injecting
  - backdoors
  - .
  - We
  - have
  - updated
  - Section
  - '4.4'
  - to
  - better
  - reflect
  - this
  - argument
  - .
  - '-'
  - Since
  - we
  - do
  - not
  - have
  - access
  - to
  - the
  - training
  - procedure
  - ','
  - the
  - pattern
  - is
  - applied
  - before
  - any
  - data
  - augmentation
  - .
  - This
  - is
  - the
  - reason
  - why
  - this
  - setting
  - is
  - challenging
  - --
  - data
  - augmentation
  - might
  - obscure
  - the
  - pattern
  - .
  - We
  - have
  - updated
  - the
  - manuscript
  - to
  - incorporate
  - the
  - other
  - comments
  - .
  - Those
  - answer
  - all
  - the
  - questions
  - I
  - had
  - .
  - Dear
  - AnonReviewer2
  - ','
  - I
  - am
  - writing
  - to
  - bring
  - your
  - attention
  - to
  - my
  - comments
  - below
  - ','
  - per
  - the
  - Area
  - Chair
  - '''s'
  - request
  - .
  - In
  - my
  - opinion
  - ','
  - this
  - paper
  - '''s'
  - overarching
  - idea
  - is
  - very
  - interesting
  - and
  - yet
  - the
  - implementation
  - could
  - be
  - improved
  - .
  - In
  - particular
  - ','
  - I
  - had
  - three
  - major
  - concerns
  - in
  - my
  - initial
  - review
  - .
  - '1'
  - .
  - This
  - paper
  - does
  - not
  - propose
  - any
  - new
  - attack
  - algorithms
  - .
  - Instead
  - ','
  - it
  - investigates
  - an
  - existing
  - adversarial
  - attack
  - method
  - and
  - the
  - GAN
  - based
  - interpolation
  - for
  - the
  - backdoor
  - attack
  - .
  - '2'
  - .
  - As
  - experiments
  - are
  - conducted
  - 'on'
  - small-scale
  - datasets
  - ','
  - it
  - is
  - unclear
  - how
  - effective
  - the
  - improved
  - backdoor
  - attack
  - is
  - .
  - '3'
  - .
  - Moreover
  - ','
  - one
  - of
  - the
  - main
  - disadvantages
  - of
  - the
  - proposed
  - attack
  - method
  - is
  - that
  - simple
  - data
  - augmentation
  - techniques
  - ','
  - especially
  - random
  - cropping
  - ','
  - can
  - successfully
  - defend
  - against
  - the
  - attack
  - .
  - The
  - first
  - two
  - concerns
  - were
  - reinforced
  - by
  - the
  - authors
  - ''''
  - responses
  - -LRB-
  - at
  - least
  - from
  - my
  - point
  - of
  - view
  - -RRB-
  - .
  - The
  - answer
  - to
  - the
  - third
  - concern
  - is
  - not
  - convincing
  - .
  - My
  - background
  - is
  - largely
  - from
  - computer
  - vision
  - ','
  - and
  - I
  - can
  - think
  - of
  - many
  - data
  - augmentations
  - to
  - overcome
  - the
  - four-corner
  - backdoor
  - patterns
  - studied
  - in
  - this
  - paper
  - .
  - Best
  - regards
  - ','
  - AnonReviewer1
- comment_id: B1eV9q7URm
  rels:
  - !!python/tuple
    - 0
    - 6
    - 6
    - 23
    - elaboration
  - !!python/tuple
    - 0
    - 23
    - 23
    - 44
    - elaboration
  - !!python/tuple
    - 23
    - 31
    - 31
    - 44
    - elaboration
  - !!python/tuple
    - 0
    - 44
    - 44
    - 718
    - elaboration
  - !!python/tuple
    - 96
    - 718
    - 44
    - 96
    - antithesis
  - !!python/tuple
    - 44
    - 59
    - 59
    - 96
    - elaboration
  - !!python/tuple
    - 59
    - 78
    - 78
    - 96
    - same_unit
  - !!python/tuple
    - 59
    - 65
    - 65
    - 78
    - elaboration
  - !!python/tuple
    - 78
    - 96
    - 59
    - 78
    - same_unit
  - !!python/tuple
    - 78
    - 87
    - 87
    - 96
    - elaboration
  - !!python/tuple
    - 96
    - 108
    - 108
    - 125
    - list
  - !!python/tuple
    - 96
    - 104
    - 104
    - 108
    - elaboration
  - !!python/tuple
    - 108
    - 125
    - 96
    - 108
    - list
  - !!python/tuple
    - 96
    - 125
    - 125
    - 718
    - elaboration
  - !!python/tuple
    - 125
    - 131
    - 131
    - 718
    - elaboration
  - !!python/tuple
    - 160
    - 718
    - 131
    - 160
    - antithesis
  - !!python/tuple
    - 140
    - 160
    - 131
    - 140
    - attribution
  - !!python/tuple
    - 140
    - 155
    - 155
    - 160
    - elaboration
  - !!python/tuple
    - 160
    - 187
    - 187
    - 718
    - list
  - !!python/tuple
    - 160
    - 165
    - 165
    - 187
    - same_unit
  - !!python/tuple
    - 165
    - 187
    - 160
    - 165
    - same_unit
  - !!python/tuple
    - 165
    - 178
    - 178
    - 187
    - circumstance
  - !!python/tuple
    - 187
    - 718
    - 160
    - 187
    - list
  - !!python/tuple
    - 187
    - 202
    - 202
    - 228
    - contrast
  - !!python/tuple
    - 187
    - 197
    - 197
    - 202
    - elaboration
  - !!python/tuple
    - 202
    - 228
    - 187
    - 202
    - contrast
  - !!python/tuple
    - 202
    - 223
    - 223
    - 228
    - elaboration
  - !!python/tuple
    - 187
    - 228
    - 228
    - 718
    - elaboration
  - !!python/tuple
    - 228
    - 238
    - 238
    - 718
    - list
  - !!python/tuple
    - 228
    - 231
    - 231
    - 235
    - elaboration
  - !!python/tuple
    - 228
    - 235
    - 235
    - 238
    - elaboration
  - !!python/tuple
    - 238
    - 718
    - 228
    - 238
    - list
  - !!python/tuple
    - 238
    - 241
    - 241
    - 245
    - elaboration
  - !!python/tuple
    - 238
    - 245
    - 245
    - 251
    - elaboration
  - !!python/tuple
    - 238
    - 251
    - 251
    - 718
    - elaboration
  - !!python/tuple
    - 251
    - 252
    - 252
    - 257
    - restatement
  - !!python/tuple
    - 252
    - 253
    - 253
    - 257
    - restatement
  - !!python/tuple
    - 251
    - 257
    - 257
    - 260
    - elaboration
  - !!python/tuple
    - 251
    - 260
    - 260
    - 718
    - elaboration
  - !!python/tuple
    - 267
    - 312
    - 260
    - 267
    - attribution
  - !!python/tuple
    - 260
    - 263
    - 263
    - 267
    - elaboration
  - !!python/tuple
    - 268
    - 312
    - 267
    - 268
    - attribution
  - !!python/tuple
    - 268
    - 295
    - 295
    - 312
    - elaboration
  - !!python/tuple
    - 295
    - 307
    - 307
    - 312
    - same_unit
  - !!python/tuple
    - 295
    - 296
    - 296
    - 307
    - restatement
  - !!python/tuple
    - 307
    - 312
    - 295
    - 307
    - same_unit
  - !!python/tuple
    - 260
    - 312
    - 312
    - 718
    - elaboration
  - !!python/tuple
    - 312
    - 330
    - 330
    - 718
    - list
  - !!python/tuple
    - 330
    - 718
    - 312
    - 330
    - list
  - !!python/tuple
    - 330
    - 340
    - 340
    - 718
    - list
  - !!python/tuple
    - 340
    - 718
    - 330
    - 340
    - list
  - !!python/tuple
    - 340
    - 346
    - 346
    - 718
    - list
  - !!python/tuple
    - 346
    - 718
    - 340
    - 346
    - list
  - !!python/tuple
    - 346
    - 355
    - 355
    - 718
    - list
  - !!python/tuple
    - 355
    - 718
    - 346
    - 355
    - list
  - !!python/tuple
    - 355
    - 365
    - 365
    - 718
    - condition
  - !!python/tuple
    - 365
    - 383
    - 383
    - 718
    - list
  - !!python/tuple
    - 365
    - 372
    - 372
    - 375
    - elaboration
  - !!python/tuple
    - 365
    - 375
    - 375
    - 383
    - purpose
  - !!python/tuple
    - 383
    - 718
    - 365
    - 383
    - list
  - !!python/tuple
    - 383
    - 385
    - 385
    - 718
    - list
  - !!python/tuple
    - 385
    - 718
    - 383
    - 385
    - list
  - !!python/tuple
    - 385
    - 394
    - 394
    - 718
    - elaboration
  - !!python/tuple
    - 394
    - 405
    - 405
    - 409
    - comparison
  - !!python/tuple
    - 394
    - 409
    - 409
    - 718
    - elaboration
  - !!python/tuple
    - 409
    - 436
    - 436
    - 718
    - list
  - !!python/tuple
    - 409
    - 424
    - 424
    - 425
    - same_unit
  - !!python/tuple
    - 409
    - 414
    - 414
    - 424
    - elaboration
  - !!python/tuple
    - 414
    - 421
    - 421
    - 424
    - elaboration
  - !!python/tuple
    - 424
    - 425
    - 409
    - 424
    - same_unit
  - !!python/tuple
    - 409
    - 425
    - 425
    - 436
    - purpose
  - !!python/tuple
    - 436
    - 718
    - 409
    - 436
    - list
  - !!python/tuple
    - 436
    - 522
    - 522
    - 718
    - topic
  - !!python/tuple
    - 436
    - 438
    - 438
    - 522
    - elaboration
  - !!python/tuple
    - 438
    - 450
    - 450
    - 522
    - elaboration
  - !!python/tuple
    - 450
    - 456
    - 456
    - 470
    - elaboration
  - !!python/tuple
    - 450
    - 470
    - 470
    - 522
    - elaboration
  - !!python/tuple
    - 470
    - 482
    - 482
    - 522
    - elaboration
  - !!python/tuple
    - 482
    - 484
    - 484
    - 501
    - elaboration
  - !!python/tuple
    - 482
    - 501
    - 501
    - 522
    - elaboration
  - !!python/tuple
    - 501
    - 503
    - 503
    - 520
    - elaboration
  - !!python/tuple
    - 503
    - 511
    - 511
    - 520
    - same_unit
  - !!python/tuple
    - 503
    - 505
    - 505
    - 511
    - elaboration
  - !!python/tuple
    - 511
    - 520
    - 503
    - 511
    - same_unit
  - !!python/tuple
    - 511
    - 515
    - 515
    - 520
    - list
  - !!python/tuple
    - 515
    - 520
    - 511
    - 515
    - list
  - !!python/tuple
    - 501
    - 520
    - 520
    - 522
    - elaboration
  - !!python/tuple
    - 522
    - 718
    - 436
    - 522
    - topic
  - !!python/tuple
    - 522
    - 531
    - 531
    - 718
    - textualorganization
  - !!python/tuple
    - 531
    - 718
    - 522
    - 531
    - textualorganization
  - !!python/tuple
    - 531
    - 539
    - 539
    - 718
    - condition
  - !!python/tuple
    - 539
    - 547
    - 547
    - 718
    - list
  - !!python/tuple
    - 547
    - 718
    - 539
    - 547
    - list
  - !!python/tuple
    - 547
    - 549
    - 549
    - 565
    - elaboration
  - !!python/tuple
    - 549
    - 555
    - 555
    - 565
    - reason
  - !!python/tuple
    - 555
    - 560
    - 560
    - 561
    - elaboration
  - !!python/tuple
    - 555
    - 561
    - 561
    - 565
    - means
  - !!python/tuple
    - 547
    - 565
    - 565
    - 718
    - elaboration
  - !!python/tuple
    - 565
    - 579
    - 579
    - 593
    - same_unit
  - !!python/tuple
    - 565
    - 574
    - 574
    - 579
    - elaboration
  - !!python/tuple
    - 579
    - 593
    - 565
    - 579
    - same_unit
  - !!python/tuple
    - 579
    - 580
    - 580
    - 593
    - elaboration
  - !!python/tuple
    - 565
    - 593
    - 593
    - 718
    - elaboration
  - !!python/tuple
    - 593
    - 595
    - 595
    - 718
    - elaboration
  - !!python/tuple
    - 595
    - 629
    - 629
    - 718
    - list
  - !!python/tuple
    - 595
    - 606
    - 606
    - 629
    - elaboration
  - !!python/tuple
    - 606
    - 618
    - 618
    - 629
    - elaboration
  - !!python/tuple
    - 618
    - 625
    - 625
    - 629
    - elaboration
  - !!python/tuple
    - 629
    - 718
    - 595
    - 629
    - list
  - !!python/tuple
    - 629
    - 631
    - 631
    - 718
    - explanation
  - !!python/tuple
    - 631
    - 670
    - 670
    - 676
    - elaboration
  - !!python/tuple
    - 631
    - 676
    - 676
    - 718
    - elaboration
  - !!python/tuple
    - 676
    - 678
    - 678
    - 718
    - elaboration
  - !!python/tuple
    - 678
    - 691
    - 691
    - 718
    - list
  - !!python/tuple
    - 691
    - 718
    - 678
    - 691
    - list
  - !!python/tuple
    - 691
    - 693
    - 693
    - 718
    - elaboration
  - !!python/tuple
    - 695
    - 718
    - 693
    - 695
    - attribution
  tokens:
  - This
  - paper
  - proposed
  - a
  - mixed
  - strategy
  - to
  - obtain
  - better
  - precision
  - 'on'
  - robustness
  - verifications
  - of
  - feed-forward
  - neural
  - networks
  - with
  - piecewise
  - linear
  - activation
  - functions
  - .
  - The
  - topic
  - of
  - robustness
  - verification
  - is
  - important
  - .
  - The
  - paper
  - is
  - well-written
  - and
  - the
  - overview
  - example
  - is
  - nice
  - and
  - helpful
  - .
  - The
  - central
  - idea
  - of
  - this
  - paper
  - is
  - simple
  - and
  - the
  - results
  - can
  - be
  - expected
  - ':'
  - the
  - authors
  - combine
  - several
  - verification
  - methods
  - -LRB-
  - the
  - complete
  - verifier
  - MILP
  - ','
  - the
  - incomplete
  - verifier
  - LP
  - and
  - AI2
  - -RRB-
  - and
  - thus
  - achieve
  - better
  - precision
  - compared
  - with
  - imcomplete
  - verifiers
  - while
  - being
  - more
  - scalable
  - than
  - the
  - complete
  - verifiers
  - .
  - However
  - ','
  - the
  - verified
  - networks
  - are
  - fairly
  - small
  - -LRB-
  - '1800'
  - neurons
  - -RRB-
  - and
  - it
  - is
  - not
  - clear
  - how
  - good
  - the
  - performance
  - is
  - compared
  - to
  - other
  - state-of-the-art
  - complete/incomplete
  - verifiers
  - .
  - About
  - experiments
  - questions
  - ':'
  - '1'
  - .
  - The
  - experiments
  - compare
  - verified
  - robustness
  - with
  - AI2
  - and
  - show
  - that
  - RefineAI
  - can
  - verify
  - more
  - than
  - AI2
  - at
  - the
  - expense
  - of
  - much
  - more
  - computation
  - time
  - -LRB-
  - Figure
  - '3'
  - -RRB-
  - .
  - However
  - ','
  - the
  - problem
  - here
  - is
  - how
  - is
  - RefineAI
  - or
  - AI2
  - compare
  - with
  - other
  - complete
  - and
  - incomplete
  - verifiers
  - as
  - described
  - in
  - the
  - second
  - paragraph
  - of
  - introduction
  - '?'
  - The
  - AI2
  - does
  - not
  - seem
  - to
  - have
  - public
  - available
  - codes
  - that
  - readers
  - can
  - try
  - out
  - but
  - for
  - some
  - complete
  - and
  - incomplete
  - verifiers
  - papers
  - mentioned
  - in
  - the
  - introductions
  - ','
  - I
  - do
  - find
  - some
  - public
  - codes
  - available
  - ':'
  - '*'
  - complete
  - verifiers
  - '1'
  - .
  - Tjeng
  - '&'
  - Tedrake
  - -LRB-
  - '2017'
  - -RRB-
  - ':'
  - github.com/vtjeng/MIPVerify.jl
  - '2'
  - .
  - SMT
  - Katz
  - etal
  - -LRB-
  - '2017'
  - -RRB-
  - ':'
  - https://github.com/guykatzz/ReluplexCav2017
  - '*'
  - incomplete
  - verifiers
  - '3'
  - .
  - Weng
  - etal
  - -LRB-
  - '2018'
  - -RRB-
  - ':'
  - https://github.com/huanzhang12/CertifiedReLURobustness
  - '4'
  - .
  - Wong
  - '&'
  - Kolter
  - -LRB-
  - '2018'
  - -RRB-
  - ':'
  - http://github.com/locuslab/convex_adversarial
  - How
  - does
  - Refine
  - AI
  - proposed
  - in
  - this
  - paper
  - compare
  - with
  - the
  - above
  - four
  - papers
  - in
  - terms
  - of
  - the
  - verified
  - robustness
  - percentage
  - 'on'
  - test
  - set
  - ','
  - the
  - robustness
  - bound
  - -LRB-
  - the
  - epsilon
  - in
  - the
  - paragraph
  - Abstract
  - Interpretation
  - p.
  - '4'
  - -RRB-
  - and
  - the
  - run
  - time
  - '?'
  - The
  - verified
  - robustness
  - percentage
  - of
  - Tjeng
  - '&'
  - Tedrake
  - is
  - reported
  - but
  - the
  - robustness
  - bound
  - is
  - not
  - reported
  - .
  - Also
  - ','
  - can
  - Refine
  - AI
  - scale
  - to
  - other
  - datasets
  - '?'
  - About
  - other
  - questions
  - ':'
  - '1'
  - .
  - Can
  - RefineAI
  - handle
  - only
  - piece-wise
  - linear
  - activation
  - functions
  - '?'
  - How
  - about
  - other
  - activation
  - functions
  - ','
  - such
  - as
  - sigmoid
  - '?'
  - If
  - so
  - ','
  - what
  - are
  - the
  - modifications
  - to
  - be
  - made
  - to
  - handle
  - other
  - non-piece-wise
  - linear
  - activation
  - functions
  - '?'
  - '2'
  - .
  - In
  - Sec
  - '4'
  - ','
  - the
  - Robustness
  - properties
  - paragraph
  - .
  - '``'
  - The
  - adversarial
  - attack
  - considered
  - here
  - is
  - untargeted
  - and
  - therefore
  - stronger
  - than
  - '...'
  - ''''''
  - .
  - The
  - approaches
  - in
  - Weng
  - etal
  - -LRB-
  - '2018'
  - -RRB-
  - and
  - Tjeng
  - '&'
  - Tedrake
  - -LRB-
  - '2017'
  - -RRB-
  - seem
  - to
  - be
  - able
  - to
  - handle
  - the
  - untargeted
  - robustness
  - as
  - well
  - '?'
  - '3'
  - .
  - In
  - Sec
  - '4'
  - ','
  - the
  - Effect
  - of
  - neural
  - selection
  - heuristic
  - paragraph
  - .
  - '``'
  - Although
  - the
  - number
  - of
  - images
  - verified
  - change
  - by
  - only
  - '3'
  - '%'
  - '...'
  - produces
  - tighter
  - output
  - bounds
  - '...'
  - ''''''
  - .
  - How
  - tight
  - the
  - output
  - bounds
  - improved
  - by
  - the
  - neuron
  - selection
  - heuristics
  - '?'
  - Q1
  - .
  - The
  - verified
  - robustness
  - percentage
  - of
  - Tjeng
  - '&'
  - Tedrake
  - is
  - reported
  - but
  - the
  - robustness
  - bound
  - is
  - not
  - .
  - R1
  - .
  - The
  - epsilon
  - considered
  - for
  - this
  - experiment
  - is
  - reported
  - -LRB-
  - page
  - '7'
  - -RRB-
  - and
  - it
  - is
  - '0.03'
  - .
  - Q2
  - .
  - Can
  - RefineAI
  - handle
  - only
  - piecewise
  - linear
  - activation
  - functions
  - '?'
  - How
  - about
  - other
  - activations
  - such
  - as
  - sigmoid
  - '?'
  - If
  - so
  - ','
  - what
  - modifications
  - are
  - needed
  - '?'
  - R2
  - .
  - RefineAI
  - provides
  - better
  - approximations
  - for
  - ReLU
  - because
  - it
  - uses
  - tighter
  - bounds
  - returned
  - by
  - MILP/LP
  - solvers
  - .
  - Similarly
  - ','
  - we
  - can
  - refine
  - DeepZ
  - approximations
  - for
  - sigmoid
  - -LRB-
  - which
  - already
  - exist
  - -RRB-
  - by
  - using
  - better
  - bounds
  - from
  - a
  - tighter
  - approximation
  - ','
  - e.g.
  - ','
  - quadratic
  - approximation
  - .
  - Q3
  - .
  - How
  - is
  - the
  - verification
  - problem
  - affected
  - by
  - considering
  - the
  - untargeted
  - attack
  - as
  - in
  - this
  - paper
  - vs.
  - the
  - targeted
  - attack
  - in
  - Weng
  - et
  - al
  - -LRB-
  - '2018'
  - -RRB-
  - and
  - Tjeng
  - '&'
  - Tedrake
  - -LRB-
  - '2017'
  - -RRB-
  - '?'
  - R3
  - .
  - Since
  - the
  - targeted
  - attack
  - is
  - weaker
  - ','
  - the
  - complete
  - verifier
  - from
  - Tjeng
  - and
  - Tedrake
  - runs
  - faster
  - and
  - the
  - incomplete
  - verifier
  - from
  - Weng
  - et
  - al.
  - proves
  - more
  - properties
  - in
  - their
  - respective
  - evaluation
  - than
  - it
  - would
  - if
  - they
  - considered
  - untargeted
  - attacks
  - as
  - considered
  - in
  - this
  - paper
  - .
  - Q4
  - .
  - How
  - tight
  - are
  - the
  - output
  - bounds
  - improved
  - by
  - the
  - neuron
  - selection
  - heuristics
  - '?'
  - R4
  - .
  - We
  - observed
  - that
  - the
  - width
  - of
  - the
  - interval
  - for
  - the
  - correctly
  - classified
  - label
  - is
  - up
  - to
  - '37'
  - '%'
  - smaller
  - with
  - our
  - neuron
  - selection
  - heuristic
  - .
- comment_id: B1enjzWvCm
  rels:
  - !!python/tuple
    - 0
    - 217
    - 217
    - 1715
    - topic
  - !!python/tuple
    - 0
    - 6
    - 6
    - 26
    - elaboration
  - !!python/tuple
    - 6
    - 8
    - 8
    - 26
    - purpose
  - !!python/tuple
    - 8
    - 15
    - 15
    - 26
    - means
  - !!python/tuple
    - 0
    - 26
    - 26
    - 217
    - elaboration
  - !!python/tuple
    - 26
    - 49
    - 49
    - 217
    - elaboration
  - !!python/tuple
    - 49
    - 65
    - 65
    - 78
    - elaboration
  - !!python/tuple
    - 49
    - 78
    - 78
    - 217
    - elaboration
  - !!python/tuple
    - 78
    - 82
    - 82
    - 118
    - elaboration
  - !!python/tuple
    - 82
    - 102
    - 102
    - 118
    - elaboration
  - !!python/tuple
    - 102
    - 108
    - 108
    - 118
    - purpose
  - !!python/tuple
    - 78
    - 118
    - 118
    - 217
    - elaboration
  - !!python/tuple
    - 118
    - 138
    - 138
    - 156
    - elaboration
  - !!python/tuple
    - 138
    - 144
    - 144
    - 156
    - same_unit
  - !!python/tuple
    - 138
    - 139
    - 139
    - 144
    - elaboration
  - !!python/tuple
    - 144
    - 156
    - 138
    - 144
    - same_unit
  - !!python/tuple
    - 118
    - 156
    - 156
    - 217
    - elaboration
  - !!python/tuple
    - 156
    - 171
    - 171
    - 182
    - elaboration
  - !!python/tuple
    - 171
    - 173
    - 173
    - 182
    - same_unit
  - !!python/tuple
    - 171
    - 172
    - 172
    - 173
    - elaboration
  - !!python/tuple
    - 173
    - 182
    - 171
    - 173
    - same_unit
  - !!python/tuple
    - 156
    - 182
    - 182
    - 217
    - elaboration
  - !!python/tuple
    - 182
    - 187
    - 187
    - 217
    - elaboration
  - !!python/tuple
    - 189
    - 217
    - 187
    - 189
    - attribution
  - !!python/tuple
    - 189
    - 195
    - 195
    - 217
    - list
  - !!python/tuple
    - 195
    - 217
    - 189
    - 195
    - list
  - !!python/tuple
    - 195
    - 208
    - 208
    - 217
    - elaboration
  - !!python/tuple
    - 217
    - 1715
    - 0
    - 217
    - topic
  - !!python/tuple
    - 217
    - 244
    - 244
    - 1715
    - list
  - !!python/tuple
    - 217
    - 219
    - 219
    - 244
    - example
  - !!python/tuple
    - 219
    - 227
    - 227
    - 244
    - elaboration
  - !!python/tuple
    - 227
    - 241
    - 241
    - 244
    - same_unit
  - !!python/tuple
    - 227
    - 237
    - 237
    - 241
    - elaboration
  - !!python/tuple
    - 241
    - 244
    - 227
    - 241
    - same_unit
  - !!python/tuple
    - 244
    - 1715
    - 217
    - 244
    - list
  - !!python/tuple
    - 244
    - 254
    - 254
    - 273
    - elaboration
  - !!python/tuple
    - 254
    - 256
    - 256
    - 273
    - elaboration
  - !!python/tuple
    - 256
    - 263
    - 263
    - 273
    - same_unit
  - !!python/tuple
    - 256
    - 259
    - 259
    - 263
    - attribution
  - !!python/tuple
    - 263
    - 273
    - 256
    - 263
    - same_unit
  - !!python/tuple
    - 244
    - 273
    - 273
    - 1715
    - elaboration
  - !!python/tuple
    - 273
    - 301
    - 301
    - 1715
    - elaboration
  - !!python/tuple
    - 301
    - 320
    - 320
    - 1715
    - textualorganization
  - !!python/tuple
    - 301
    - 316
    - 316
    - 320
    - same_unit
  - !!python/tuple
    - 301
    - 302
    - 302
    - 303
    - elaboration
  - !!python/tuple
    - 301
    - 303
    - 303
    - 316
    - elaboration
  - !!python/tuple
    - 316
    - 320
    - 301
    - 316
    - same_unit
  - !!python/tuple
    - 320
    - 1715
    - 301
    - 320
    - textualorganization
  - !!python/tuple
    - 320
    - 330
    - 330
    - 1715
    - list
  - !!python/tuple
    - 320
    - 327
    - 327
    - 330
    - elaboration
  - !!python/tuple
    - 330
    - 1715
    - 320
    - 330
    - list
  - !!python/tuple
    - 330
    - 341
    - 341
    - 1715
    - question
  - !!python/tuple
    - 341
    - 1715
    - 330
    - 341
    - question
  - !!python/tuple
    - 342
    - 356
    - 341
    - 342
    - attribution
  - !!python/tuple
    - 342
    - 352
    - 352
    - 356
    - temporal
  - !!python/tuple
    - 341
    - 356
    - 356
    - 1715
    - elaboration
  - !!python/tuple
    - 357
    - 391
    - 356
    - 357
    - attribution
  - !!python/tuple
    - 364
    - 391
    - 357
    - 364
    - attribution
  - !!python/tuple
    - 368
    - 391
    - 364
    - 368
    - attribution
  - !!python/tuple
    - 377
    - 391
    - 368
    - 377
    - attribution
  - !!python/tuple
    - 356
    - 391
    - 391
    - 1715
    - elaboration
  - !!python/tuple
    - 391
    - 418
    - 418
    - 1715
    - elaboration
  - !!python/tuple
    - 419
    - 460
    - 418
    - 419
    - attribution
  - !!python/tuple
    - 418
    - 460
    - 460
    - 1715
    - elaboration
  - !!python/tuple
    - 460
    - 639
    - 639
    - 1715
    - textualorganization
  - !!python/tuple
    - 460
    - 485
    - 485
    - 639
    - list
  - !!python/tuple
    - 460
    - 464
    - 464
    - 485
    - purpose
  - !!python/tuple
    - 485
    - 639
    - 460
    - 485
    - list
  - !!python/tuple
    - 485
    - 510
    - 510
    - 639
    - list
  - !!python/tuple
    - 485
    - 495
    - 495
    - 510
    - purpose
  - !!python/tuple
    - 510
    - 639
    - 485
    - 510
    - list
  - !!python/tuple
    - 510
    - 514
    - 514
    - 530
    - purpose
  - !!python/tuple
    - 514
    - 517
    - 517
    - 530
    - purpose
  - !!python/tuple
    - 517
    - 525
    - 525
    - 530
    - elaboration
  - !!python/tuple
    - 510
    - 530
    - 530
    - 639
    - elaboration
  - !!python/tuple
    - 533
    - 557
    - 530
    - 533
    - attribution
  - !!python/tuple
    - 533
    - 539
    - 539
    - 557
    - elaboration
  - !!python/tuple
    - 539
    - 552
    - 552
    - 557
    - elaboration
  - !!python/tuple
    - 530
    - 557
    - 557
    - 639
    - elaboration
  - !!python/tuple
    - 561
    - 592
    - 557
    - 561
    - attribution
  - !!python/tuple
    - 562
    - 592
    - 561
    - 562
    - attribution
  - !!python/tuple
    - 566
    - 592
    - 562
    - 566
    - attribution
  - !!python/tuple
    - 569
    - 592
    - 566
    - 569
    - attribution
  - !!python/tuple
    - 569
    - 582
    - 582
    - 592
    - purpose
  - !!python/tuple
    - 557
    - 592
    - 592
    - 639
    - elaboration
  - !!python/tuple
    - 592
    - 598
    - 598
    - 605
    - purpose
  - !!python/tuple
    - 598
    - 601
    - 601
    - 605
    - elaboration
  - !!python/tuple
    - 592
    - 605
    - 605
    - 639
    - elaboration
  - !!python/tuple
    - 605
    - 611
    - 611
    - 639
    - elaboration
  - !!python/tuple
    - 611
    - 622
    - 622
    - 639
    - list
  - !!python/tuple
    - 622
    - 639
    - 611
    - 622
    - list
  - !!python/tuple
    - 622
    - 628
    - 628
    - 639
    - elaboration
  - !!python/tuple
    - 639
    - 1715
    - 460
    - 639
    - textualorganization
  - !!python/tuple
    - 639
    - 658
    - 658
    - 1715
    - list
  - !!python/tuple
    - 658
    - 1715
    - 639
    - 658
    - list
  - !!python/tuple
    - 658
    - 710
    - 710
    - 1715
    - topic
  - !!python/tuple
    - 659
    - 710
    - 658
    - 659
    - attribution
  - !!python/tuple
    - 659
    - 662
    - 662
    - 672
    - elaboration
  - !!python/tuple
    - 659
    - 672
    - 672
    - 710
    - elaboration
  - !!python/tuple
    - 673
    - 710
    - 672
    - 673
    - attribution
  - !!python/tuple
    - 673
    - 703
    - 703
    - 709
    - elaboration
  - !!python/tuple
    - 673
    - 709
    - 709
    - 710
    - elaboration
  - !!python/tuple
    - 710
    - 1715
    - 658
    - 710
    - topic
  - !!python/tuple
    - 710
    - 723
    - 723
    - 1715
    - elaboration
  - !!python/tuple
    - 723
    - 741
    - 741
    - 1715
    - list
  - !!python/tuple
    - 723
    - 737
    - 737
    - 741
    - same_unit
  - !!python/tuple
    - 723
    - 731
    - 731
    - 737
    - elaboration
  - !!python/tuple
    - 737
    - 741
    - 723
    - 737
    - same_unit
  - !!python/tuple
    - 741
    - 1715
    - 723
    - 741
    - list
  - !!python/tuple
    - 741
    - 747
    - 747
    - 755
    - elaboration
  - !!python/tuple
    - 741
    - 755
    - 755
    - 1715
    - example
  - !!python/tuple
    - 770
    - 796
    - 755
    - 770
    - attribution
  - !!python/tuple
    - 770
    - 792
    - 792
    - 796
    - elaboration
  - !!python/tuple
    - 755
    - 796
    - 796
    - 833
    - elaboration
  - !!python/tuple
    - 796
    - 832
    - 832
    - 833
    - same_unit
  - !!python/tuple
    - 796
    - 797
    - 797
    - 832
    - condition
  - !!python/tuple
    - 797
    - 812
    - 812
    - 832
    - same_unit
  - !!python/tuple
    - 797
    - 809
    - 809
    - 812
    - elaboration
  - !!python/tuple
    - 812
    - 832
    - 797
    - 812
    - same_unit
  - !!python/tuple
    - 812
    - 821
    - 821
    - 832
    - elaboration
  - !!python/tuple
    - 821
    - 826
    - 826
    - 832
    - elaboration
  - !!python/tuple
    - 832
    - 833
    - 796
    - 832
    - same_unit
  - !!python/tuple
    - 755
    - 833
    - 833
    - 1715
    - elaboration
  - !!python/tuple
    - 833
    - 837
    - 837
    - 1715
    - textualorganization
  - !!python/tuple
    - 837
    - 1715
    - 833
    - 837
    - textualorganization
  - !!python/tuple
    - 837
    - 881
    - 881
    - 1715
    - textualorganization
  - !!python/tuple
    - 838
    - 881
    - 837
    - 838
    - attribution
  - !!python/tuple
    - 881
    - 1715
    - 837
    - 881
    - textualorganization
  - !!python/tuple
    - 881
    - 1165
    - 1165
    - 1715
    - textualorganization
  - !!python/tuple
    - 881
    - 885
    - 885
    - 1165
    - elaboration
  - !!python/tuple
    - 885
    - 895
    - 895
    - 1165
    - elaboration
  - !!python/tuple
    - 895
    - 904
    - 904
    - 939
    - list
  - !!python/tuple
    - 904
    - 939
    - 895
    - 904
    - list
  - !!python/tuple
    - 905
    - 939
    - 904
    - 905
    - attribution
  - !!python/tuple
    - 911
    - 939
    - 905
    - 911
    - attribution
  - !!python/tuple
    - 914
    - 939
    - 911
    - 914
    - attribution
  - !!python/tuple
    - 914
    - 915
    - 915
    - 939
    - means
  - !!python/tuple
    - 915
    - 934
    - 934
    - 939
    - purpose
  - !!python/tuple
    - 895
    - 939
    - 939
    - 1165
    - elaboration
  - !!python/tuple
    - 939
    - 954
    - 954
    - 968
    - contrast
  - !!python/tuple
    - 941
    - 954
    - 939
    - 941
    - attribution
  - !!python/tuple
    - 941
    - 948
    - 948
    - 954
    - same_unit
  - !!python/tuple
    - 941
    - 943
    - 943
    - 948
    - elaboration
  - !!python/tuple
    - 948
    - 954
    - 941
    - 948
    - same_unit
  - !!python/tuple
    - 954
    - 968
    - 939
    - 954
    - contrast
  - !!python/tuple
    - 939
    - 968
    - 968
    - 1165
    - elaboration
  - !!python/tuple
    - 968
    - 1002
    - 1002
    - 1165
    - list
  - !!python/tuple
    - 968
    - 972
    - 972
    - 1002
    - example
  - !!python/tuple
    - 972
    - 982
    - 982
    - 1002
    - same_unit
  - !!python/tuple
    - 972
    - 977
    - 977
    - 982
    - elaboration
  - !!python/tuple
    - 982
    - 1002
    - 972
    - 982
    - same_unit
  - !!python/tuple
    - 982
    - 987
    - 987
    - 1002
    - purpose
  - !!python/tuple
    - 987
    - 994
    - 994
    - 1002
    - circumstance
  - !!python/tuple
    - 1002
    - 1165
    - 968
    - 1002
    - list
  - !!python/tuple
    - 1002
    - 1020
    - 1020
    - 1062
    - textualorganization
  - !!python/tuple
    - 1020
    - 1062
    - 1002
    - 1020
    - textualorganization
  - !!python/tuple
    - 1020
    - 1029
    - 1029
    - 1039
    - elaboration
  - !!python/tuple
    - 1020
    - 1039
    - 1039
    - 1062
    - elaboration
  - !!python/tuple
    - 1002
    - 1062
    - 1062
    - 1165
    - elaboration
  - !!python/tuple
    - 1062
    - 1086
    - 1086
    - 1165
    - list
  - !!python/tuple
    - 1062
    - 1066
    - 1066
    - 1086
    - elaboration
  - !!python/tuple
    - 1066
    - 1070
    - 1070
    - 1086
    - elaboration
  - !!python/tuple
    - 1070
    - 1079
    - 1079
    - 1083
    - elaboration
  - !!python/tuple
    - 1070
    - 1083
    - 1083
    - 1086
    - elaboration
  - !!python/tuple
    - 1086
    - 1165
    - 1062
    - 1086
    - list
  - !!python/tuple
    - 1086
    - 1100
    - 1100
    - 1165
    - elaboration
  - !!python/tuple
    - 1107
    - 1122
    - 1100
    - 1107
    - attribution
  - !!python/tuple
    - 1107
    - 1112
    - 1112
    - 1122
    - elaboration
  - !!python/tuple
    - 1100
    - 1122
    - 1122
    - 1165
    - elaboration
  - !!python/tuple
    - 1122
    - 1131
    - 1131
    - 1165
    - elaboration
  - !!python/tuple
    - 1131
    - 1147
    - 1147
    - 1165
    - same_unit
  - !!python/tuple
    - 1143
    - 1147
    - 1131
    - 1143
    - attribution
  - !!python/tuple
    - 1147
    - 1165
    - 1131
    - 1147
    - same_unit
  - !!python/tuple
    - 1147
    - 1161
    - 1161
    - 1165
    - elaboration
  - !!python/tuple
    - 1161
    - 1162
    - 1162
    - 1165
    - elaboration
  - !!python/tuple
    - 1162
    - 1164
    - 1164
    - 1165
    - elaboration
  - !!python/tuple
    - 1165
    - 1715
    - 881
    - 1165
    - textualorganization
  - !!python/tuple
    - 1165
    - 1188
    - 1188
    - 1715
    - list
  - !!python/tuple
    - 1181
    - 1188
    - 1165
    - 1181
    - attribution
  - !!python/tuple
    - 1188
    - 1715
    - 1165
    - 1188
    - list
  - !!python/tuple
    - 1188
    - 1210
    - 1210
    - 1715
    - topic
  - !!python/tuple
    - 1189
    - 1210
    - 1188
    - 1189
    - attribution
  - !!python/tuple
    - 1189
    - 1196
    - 1196
    - 1210
    - same_unit
  - !!python/tuple
    - 1196
    - 1210
    - 1189
    - 1196
    - same_unit
  - !!python/tuple
    - 1196
    - 1200
    - 1200
    - 1210
    - list
  - !!python/tuple
    - 1200
    - 1210
    - 1196
    - 1200
    - list
  - !!python/tuple
    - 1210
    - 1715
    - 1188
    - 1210
    - topic
  - !!python/tuple
    - 1210
    - 1217
    - 1217
    - 1226
    - comparison
  - !!python/tuple
    - 1210
    - 1226
    - 1226
    - 1232
    - elaboration
  - !!python/tuple
    - 1210
    - 1232
    - 1232
    - 1256
    - elaboration
  - !!python/tuple
    - 1236
    - 1256
    - 1232
    - 1236
    - attribution
  - !!python/tuple
    - 1237
    - 1256
    - 1236
    - 1237
    - attribution
  - !!python/tuple
    - 1210
    - 1256
    - 1256
    - 1286
    - elaboration
  - !!python/tuple
    - 1260
    - 1286
    - 1256
    - 1260
    - attribution
  - !!python/tuple
    - 1260
    - 1269
    - 1269
    - 1286
    - same_unit
  - !!python/tuple
    - 1260
    - 1264
    - 1264
    - 1269
    - elaboration
  - !!python/tuple
    - 1269
    - 1286
    - 1260
    - 1269
    - same_unit
  - !!python/tuple
    - 1210
    - 1286
    - 1286
    - 1487
    - elaboration
  - !!python/tuple
    - 1286
    - 1291
    - 1291
    - 1317
    - elaboration
  - !!python/tuple
    - 1291
    - 1307
    - 1307
    - 1317
    - purpose
  - !!python/tuple
    - 1286
    - 1317
    - 1317
    - 1341
    - elaboration
  - !!python/tuple
    - 1321
    - 1341
    - 1317
    - 1321
    - attribution
  - !!python/tuple
    - 1322
    - 1341
    - 1321
    - 1322
    - attribution
  - !!python/tuple
    - 1286
    - 1341
    - 1341
    - 1487
    - elaboration
  - !!python/tuple
    - 1345
    - 1371
    - 1341
    - 1345
    - attribution
  - !!python/tuple
    - 1345
    - 1354
    - 1354
    - 1371
    - same_unit
  - !!python/tuple
    - 1345
    - 1349
    - 1349
    - 1354
    - elaboration
  - !!python/tuple
    - 1354
    - 1371
    - 1345
    - 1354
    - same_unit
  - !!python/tuple
    - 1341
    - 1371
    - 1371
    - 1487
    - example
  - !!python/tuple
    - 1371
    - 1376
    - 1376
    - 1402
    - elaboration
  - !!python/tuple
    - 1376
    - 1392
    - 1392
    - 1402
    - purpose
  - !!python/tuple
    - 1371
    - 1402
    - 1402
    - 1487
    - elaboration
  - !!python/tuple
    - 1402
    - 1404
    - 1404
    - 1418
    - same_unit
  - !!python/tuple
    - 1402
    - 1403
    - 1403
    - 1404
    - elaboration
  - !!python/tuple
    - 1404
    - 1418
    - 1402
    - 1404
    - same_unit
  - !!python/tuple
    - 1408
    - 1418
    - 1404
    - 1408
    - attribution
  - !!python/tuple
    - 1402
    - 1418
    - 1418
    - 1487
    - elaboration
  - !!python/tuple
    - 1428
    - 1443
    - 1418
    - 1428
    - attribution
  - !!python/tuple
    - 1418
    - 1443
    - 1443
    - 1487
    - elaboration
  - !!python/tuple
    - 1443
    - 1458
    - 1458
    - 1470
    - elaboration
  - !!python/tuple
    - 1458
    - 1462
    - 1462
    - 1470
    - elaboration
  - !!python/tuple
    - 1443
    - 1470
    - 1470
    - 1487
    - elaboration
  - !!python/tuple
    - 1475
    - 1487
    - 1470
    - 1475
    - attribution
  - !!python/tuple
    - 1210
    - 1487
    - 1487
    - 1715
    - elaboration
  - !!python/tuple
    - 1487
    - 1497
    - 1497
    - 1519
    - elaboration
  - !!python/tuple
    - 1497
    - 1499
    - 1499
    - 1519
    - elaboration
  - !!python/tuple
    - 1506
    - 1519
    - 1499
    - 1506
    - attribution
  - !!python/tuple
    - 1506
    - 1509
    - 1509
    - 1519
    - purpose
  - !!python/tuple
    - 1487
    - 1519
    - 1519
    - 1715
    - example
  - !!python/tuple
    - 1519
    - 1539
    - 1539
    - 1561
    - elaboration
  - !!python/tuple
    - 1519
    - 1561
    - 1561
    - 1715
    - elaboration
  - !!python/tuple
    - 1561
    - 1575
    - 1575
    - 1715
    - elaboration
  - !!python/tuple
    - 1575
    - 1590
    - 1590
    - 1715
    - elaboration
  - !!python/tuple
    - 1590
    - 1618
    - 1618
    - 1715
    - list
  - !!python/tuple
    - 1590
    - 1593
    - 1593
    - 1618
    - elaboration
  - !!python/tuple
    - 1618
    - 1715
    - 1590
    - 1618
    - list
  - !!python/tuple
    - 1618
    - 1630
    - 1630
    - 1715
    - list
  - !!python/tuple
    - 1618
    - 1620
    - 1620
    - 1630
    - elaboration
  - !!python/tuple
    - 1630
    - 1715
    - 1618
    - 1630
    - list
  - !!python/tuple
    - 1630
    - 1640
    - 1640
    - 1715
    - list
  - !!python/tuple
    - 1630
    - 1634
    - 1634
    - 1640
    - elaboration
  - !!python/tuple
    - 1640
    - 1715
    - 1630
    - 1640
    - list
  - !!python/tuple
    - 1640
    - 1655
    - 1655
    - 1715
    - list
  - !!python/tuple
    - 1655
    - 1715
    - 1640
    - 1655
    - list
  - !!python/tuple
    - 1655
    - 1675
    - 1675
    - 1715
    - list
  - !!python/tuple
    - 1655
    - 1665
    - 1665
    - 1675
    - elaboration
  - !!python/tuple
    - 1665
    - 1670
    - 1670
    - 1675
    - elaboration
  - !!python/tuple
    - 1675
    - 1715
    - 1655
    - 1675
    - list
  - !!python/tuple
    - 1675
    - 1683
    - 1683
    - 1715
    - list
  - !!python/tuple
    - 1675
    - 1679
    - 1679
    - 1683
    - elaboration
  - !!python/tuple
    - 1683
    - 1715
    - 1675
    - 1683
    - list
  - !!python/tuple
    - 1683
    - 1692
    - 1692
    - 1715
    - list
  - !!python/tuple
    - 1692
    - 1715
    - 1683
    - 1692
    - list
  - !!python/tuple
    - 1692
    - 1699
    - 1699
    - 1715
    - elaboration
  tokens:
  - This
  - is
  - a
  - well
  - written
  - paper
  - which
  - proposes
  - to
  - learn
  - heteroscedastic
  - noise
  - models
  - from
  - data
  - by
  - optimizing
  - the
  - prediction
  - likelihood
  - end-to-end
  - through
  - differentiable
  - Bayesian
  - Filters
  - .
  - In
  - addition
  - to
  - existing
  - Bayesian
  - filters
  - ','
  - the
  - paper
  - also
  - proposes
  - two
  - different
  - versions
  - of
  - the
  - -LSB-
  - differentiable
  - -RSB-
  - Unscented
  - Kalman
  - Filter
  - .
  - Performance
  - of
  - the
  - different
  - filters
  - and
  - noise
  - models
  - is
  - evaluated
  - 'on'
  - two
  - real-world
  - robotic
  - problems
  - ':'
  - Visual
  - Odometry
  - and
  - visual
  - tracking
  - of
  - an
  - object
  - pushed
  - by
  - the
  - robot
  - .
  - While
  - the
  - general
  - idea
  - of
  - learning
  - the
  - noise
  - variances
  - through
  - backpropagation
  - are
  - straightforward
  - extensions
  - of
  - existing
  - work
  - 'on'
  - differential
  - Bayesian
  - filters
  - ','
  - the
  - questions
  - that
  - the
  - paper
  - explores
  - are
  - important
  - to
  - make
  - end-to-end
  - learning
  - of
  - Bayesian
  - filter
  - more
  - common
  - .
  - The
  - results
  - will
  - help
  - future
  - research
  - select
  - the
  - correct
  - differential
  - filter
  - for
  - their
  - use
  - case
  - ','
  - and
  - insight
  - in
  - potential
  - benefits
  - -LRB-
  - or
  - lack
  - thereof
  - -RRB-
  - by
  - learning
  - heteroscedastic
  - or
  - homoscedastic
  - process
  - noise
  - ','
  - and/or
  - observation
  - noise
  - .
  - A
  - downside
  - is
  - that
  - the
  - paper
  - does
  - not
  - further
  - explore
  - how
  - to
  - weigh
  - different
  - loss
  - terms
  - which
  - are
  - apparently
  - important
  - to
  - successfully
  - train
  - such
  - models
  - .
  - Also
  - unfortunate
  - is
  - the
  - footnote
  - which
  - states
  - that
  - the
  - current
  - results
  - are
  - incomplete
  - and
  - will
  - be
  - updated
  - ','
  - hence
  - as
  - a
  - reviewer
  - I
  - am
  - not
  - sure
  - which
  - results
  - and
  - conclusions
  - are
  - valid
  - right
  - now
  - .
  - Pros
  - ':'
  - +
  - clearly
  - written
  - +
  - useful
  - experiments
  - for
  - those
  - seeking
  - to
  - select
  - a
  - differential
  - Bayesian
  - filter
  - ','
  - and
  - learning
  - -LRB-
  - heteroscedastic
  - -RRB-
  - noise
  - from
  - data
  - .
  - +
  - experiments
  - 'on'
  - real-world
  - use
  - cases
  - rather
  - than
  - toy
  - problems
  - Cons
  - ':'
  - '-'
  - Incomplete
  - experiments
  - according
  - to
  - footnote
  - ','
  - thus
  - results
  - and
  - conclusions
  - might
  - change
  - after
  - this
  - review
  - .
  - '-'
  - Unclear
  - what
  - the
  - effect
  - of
  - the
  - selected
  - process
  - /
  - observation
  - model
  - is
  - 'on'
  - the
  - learned
  - noise
  - Below
  - are
  - more
  - detailed
  - comments
  - and
  - questions
  - ':'
  - '*'
  - p6
  - .
  - Footnote
  - ':'
  - '``'
  - due
  - to
  - time
  - constraints
  - ','
  - '...'
  - ','
  - results
  - will
  - be
  - updated
  - ''''''
  - Is
  - this
  - acceptable
  - '?'
  - I
  - have
  - never
  - seen
  - such
  - a
  - notice
  - when
  - reviewing
  - .
  - So
  - ','
  - are
  - the
  - current
  - results
  - 'on'
  - a
  - single
  - fold
  - '?'
  - Will
  - the
  - numbers
  - in
  - the
  - tables
  - ','
  - or
  - the
  - conclusions
  - change
  - after
  - this
  - review
  - '?'
  - '*'
  - If
  - I
  - understand
  - correctly
  - ','
  - the
  - paper
  - '`'
  - only
  - ''''
  - focuses
  - 'on'
  - learning
  - the
  - heteroscedastic
  - noise
  - variance
  - ','
  - but
  - assumes
  - that
  - the
  - deterministic
  - non-linear
  - parts
  - of
  - the
  - process
  - and
  - observation
  - models
  - are
  - fixed
  - .
  - I
  - did
  - not
  - find
  - this
  - very
  - clearly
  - stated
  - in
  - the
  - paper
  - ','
  - though
  - at
  - least
  - the
  - Appendix
  - explicitly
  - states
  - the
  - used
  - functions
  - for
  - the
  - process
  - models
  - .
  - '*'
  - I
  - would
  - have
  - liked
  - to
  - see
  - in
  - the
  - paper
  - more
  - explanation
  - 'on'
  - how
  - the
  - process
  - and
  - observations
  - models
  - were
  - selected
  - and
  - validated
  - in
  - the
  - experiments
  - ','
  - since
  - I
  - expect
  - that
  - the
  - validity
  - of
  - these
  - functions
  - affects
  - the
  - learned
  - noise
  - variances
  - .
  - Since
  - the
  - noise
  - needs
  - to
  - account
  - for
  - the
  - inaccuracies
  - in
  - the
  - deterministic
  - models
  - ','
  - would
  - the
  - choice
  - for
  - these
  - functions
  - not
  - impact
  - your
  - conclusions
  - '?'
  - And
  - ','
  - would
  - it
  - or
  - would
  - it
  - not
  - be
  - possible
  - to
  - learn
  - both
  - these
  - deterministic
  - models
  - and
  - the
  - noise
  - jointly
  - from
  - the
  - training
  - data
  - '?'
  - '*'
  - Is
  - it
  - possible
  - to
  - add
  - priors
  - 'on'
  - Q
  - and
  - R
  - parameters
  - for
  - Bayesian
  - treatment
  - of
  - learning
  - model
  - parameters
  - '?'
  - I
  - can
  - imagine
  - that
  - priors
  - can
  - guide
  - the
  - optimization
  - to
  - either
  - adjust
  - more
  - of
  - the
  - Q
  - or
  - more
  - of
  - the
  - R
  - variance
  - to
  - improve
  - the
  - likelihood
  - .
  - '*'
  - Section
  - '1'
  - ':'
  - '*'
  - '``'
  - Our
  - experiments
  - show
  - that
  - '...'
  - ''''''
  - This
  - may
  - be
  - a
  - matter
  - of
  - taste
  - ','
  - but
  - I
  - did
  - not
  - expect
  - to
  - see
  - the
  - main
  - conclusions
  - already
  - in
  - the
  - introduction
  - .
  - They
  - should
  - appear
  - in
  - the
  - abstract
  - to
  - help
  - out
  - the
  - quick
  - reader
  - .
  - In
  - the
  - introduction
  - ','
  - it
  - appears
  - as
  - if
  - you
  - are
  - talking
  - about
  - some
  - separate
  - preliminary
  - experiments
  - ','
  - and
  - which
  - you
  - base
  - some
  - conclusions
  - that
  - will
  - be
  - used
  - in
  - the
  - remainder
  - of
  - this
  - paper
  - .
  - '*'
  - Section
  - '3'
  - ':'
  - '*'
  - So
  - ','
  - mostly
  - empirical
  - study
  - ','
  - since
  - heteroscedastic
  - noise
  - models
  - were
  - already
  - used
  - '?'
  - '*'
  - '``'
  - Previous
  - work
  - evaluated
  - '...'
  - ''''''
  - please
  - add
  - citations
  - '*'
  - Section
  - '4.1'
  - ':'
  - '*'
  - '``'
  - train
  - a
  - discriminative
  - neural
  - network
  - o
  - with
  - parameters
  - wo
  - to
  - preprocess
  - the
  - raw
  - sensory
  - data
  - D
  - and
  - thus
  - create
  - a
  - more
  - compact
  - representation
  - of
  - the
  - observations
  - z
  - '='
  - o
  - -LRB-
  - D
  - ;
  - wo
  - -RRB-
  - .
  - ''''''
  - At
  - this
  - point
  - in
  - the
  - paper
  - ','
  - I
  - do
  - n't
  - understand
  - this
  - .
  - How
  - is
  - z
  - learned
  - ','
  - via
  - supervised
  - learning
  - -LRB-
  - what
  - is
  - the
  - target
  - value
  - for
  - z
  - -RRB-
  - '?'
  - Or
  - is
  - z
  - some
  - latent
  - representation
  - that
  - is
  - jointly
  - optimized
  - with
  - the
  - filters
  - '?'
  - This
  - only
  - became
  - somewhat
  - clearer
  - in
  - Sec.
  - '5.2'
  - 'on'
  - p.
  - '8'
  - where
  - it
  - states
  - that
  - '``'
  - We
  - '...'
  - train
  - a
  - neural
  - network
  - to
  - extract
  - the
  - position
  - of
  - the
  - object
  - ','
  - the
  - contact
  - point
  - and
  - normal
  - as
  - well
  - as
  - '...'
  - ''''''
  - .
  - So
  - if
  - I
  - understand
  - correctly
  - ','
  - the
  - function
  - o
  - for
  - z
  - '='
  - o
  - -LRB-
  - D
  - -RRB-
  - is
  - thus
  - learned
  - offline
  - w.r.t.
  - some
  - designed
  - observation
  - variables
  - for
  - which
  - GT
  - is
  - available
  - -LRB-
  - from
  - manual
  - annotations
  - '?'
  - -RRB-
  - .
  - '*'
  - Section
  - '4.2'
  - ':'
  - '*'
  - '``'
  - we
  - predict
  - a
  - separate
  - Qi
  - for
  - every
  - sigma
  - point
  - and
  - then
  - compute
  - Q
  - as
  - the
  - weighted
  - mean
  - ''''''
  - CD
  - So
  - ','
  - separate
  - parameters
  - w_g
  - for
  - each
  - sigma
  - point
  - i
  - ','
  - or
  - is
  - a
  - single
  - learned
  - non-linear
  - function
  - applied
  - to
  - all
  - points
  - '?'
  - '*'
  - Section
  - '4.3'
  - ':'
  - '*'
  - Equation
  - '14'
  - ':'
  - inconsistent
  - use
  - of
  - boldface
  - script
  - ':'
  - should
  - use
  - bold
  - sigma_t
  - ','
  - and
  - bold
  - l_t
  - '?'
  - '*'
  - '``'
  - In
  - practice
  - ','
  - we
  - found
  - that
  - during
  - learning
  - '...'
  - by
  - only
  - increasing
  - the
  - predicted
  - variance
  - ''''''
  - CD
  - This
  - is
  - an
  - interesting
  - observation
  - ','
  - which
  - I
  - would
  - have
  - liked
  - to
  - see
  - explored
  - more
  - .
  - I
  - understand
  - that
  - term
  - -LRB-
  - ii
  - -RRB-
  - is
  - needed
  - to
  - guide
  - the
  - learning
  - processes
  - ','
  - but
  - in
  - the
  - end
  - would
  - n't
  - we
  - want
  - to
  - optimize
  - the
  - actual
  - likelihood
  - '?'
  - So
  - ','
  - could
  - you
  - -LRB-
  - after
  - the
  - loss
  - with
  - -LRB-
  - ii
  - -RRB-
  - converged
  - -RRB-
  - reduce
  - \
  - lambda_2
  - to
  - zero
  - to
  - properly
  - optimize
  - only
  - the
  - log
  - likelihood
  - without
  - guidance
  - from
  - a
  - good
  - initial
  - state
  - '?'
  - Or
  - is
  - it
  - not
  - possible
  - to
  - reliably
  - optimize
  - the
  - likelihood
  - via
  - back-propagation
  - at
  - all
  - from
  - some
  - reason
  - '?'
  - '*'
  - Section
  - 5.1.1
  - '*'
  - ''''''
  - '...'
  - of
  - varying
  - length
  - -LRB-
  - from
  - '270'
  - to
  - over
  - '4500'
  - steps
  - -RRB-
  - '...'
  - ''''''
  - it
  - would
  - be
  - good
  - to
  - mention
  - the
  - fps
  - ','
  - to
  - get
  - understand
  - to
  - what
  - real-world
  - time
  - horizons
  - '50'
  - /
  - '100'
  - frames
  - correspond
  - .
  - '*'
  - Section
  - 5.1.2
  - ':'
  - '*'
  - Table
  - '1'
  - ':'
  - How
  - are
  - the
  - parameters
  - of
  - the
  - filters
  - in
  - the
  - '``'
  - 'no'
  - learning
  - ''''''
  - column
  - obtained
  - '?'
  - Are
  - these
  - tuned
  - in
  - some
  - other
  - way
  - ','
  - or
  - taken
  - form
  - existing
  - implementations
  - '?'
  - Also
  - ','
  - can
  - you
  - clarify
  - if
  - the
  - '`'
  - 'no'
  - learning
  - ''''
  - parameters
  - served
  - as
  - the
  - initial
  - condition
  - for
  - the
  - learning
  - approaches
  - '?'
  - '*'
  - Table
  - '1'
  - ','
  - first
  - row
  - column
  - Q+R
  - ':'
  - '``'
  - '0.2'
  - ''''''
  - CD
  - Is
  - there
  - a
  - missing
  - zero
  - here
  - ','
  - i.e.
  - '``'
  - '0.20'
  - ''''''
  - '?'
  - Otherwise
  - ','
  - the
  - precision
  - of
  - reported
  - results
  - in
  - this
  - table
  - is
  - not
  - consistent
  - .
  - Hard
  - to
  - say
  - ':'
  - is
  - the
  - mean
  - of
  - R+Q
  - '0.2'
  - ','
  - and
  - slightly
  - lower
  - than
  - R+Q
  - h
  - ','
  - or
  - could
  - it
  - be
  - as
  - high
  - as
  - '0.24'
  - '?'
  - '*'
  - '``'
  - learning
  - a
  - heteroscedastic
  - process
  - noise
  - model
  - leads
  - to
  - big
  - improvements
  - and
  - makes
  - the
  - filters
  - competitive
  - with
  - the
  - EKF
  - ''''''
  - .
  - Results
  - for
  - EKF
  - still
  - appear
  - significantly
  - better
  - than
  - the
  - novel
  - UKF
  - ','
  - and
  - even
  - the
  - PF
  - -LRB-
  - especially
  - rotational
  - error
  - -RRB-
  - .
  - '*'
  - Section
  - '6'
  - ':'
  - '*'
  - '``'
  - Large
  - outliers
  - in
  - the
  - prediction
  - of
  - the
  - preprocessing
  - networks
  - were
  - not
  - associated
  - with
  - higher
  - observation
  - noise
  - .
  - ''''''
  - I
  - do
  - n't
  - see
  - 'on'
  - what
  - presented
  - results
  - these
  - conclusions
  - were
  - drawn
  - ','
  - as
  - this
  - is
  - the
  - first
  - time
  - the
  - word
  - '``'
  - outlier
  - ''''''
  - is
  - mentioned
  - in
  - the
  - paper
  - .
  - Outliers
  - seem
  - indeed
  - important
  - ','
  - as
  - they
  - contradict
  - the
  - typical
  - assumptions
  - e.g.
  - of
  - Gaussian
  - noise
  - ','
  - so
  - it
  - would
  - be
  - useful
  - to
  - clarify
  - how
  - the
  - proposed
  - techniques
  - handle
  - such
  - outliers
  - .
  - '*'
  - Section
  - '6'
  - ':'
  - '*'
  - '``'
  - Large
  - outliers
  - in
  - the
  - prediction
  - of
  - the
  - preprocessing
  - networks
  - were
  - not
  - associated
  - with
  - higher
  - observation
  - noise
  - .
  - ''''''
  - I
  - do
  - n't
  - see
  - 'on'
  - what
  - presented
  - results
  - these
  - conclusions
  - were
  - drawn
  - ','
  - as
  - this
  - is
  - the
  - first
  - time
  - the
  - word
  - '``'
  - outlier
  - ''''''
  - is
  - mentioned
  - in
  - the
  - paper
  - .
  - Outliers
  - seem
  - indeed
  - important
  - ','
  - as
  - they
  - contradict
  - the
  - typical
  - assumptions
  - e.g.
  - of
  - Gaussian
  - noise
  - ','
  - so
  - it
  - would
  - be
  - useful
  - to
  - clarify
  - how
  - the
  - proposed
  - techniques
  - handle
  - such
  - outliers
  - .
  - Reply
  - ':'
  - Agreed
  - ','
  - we
  - tried
  - to
  - make
  - it
  - clearer
  - what
  - we
  - meant
  - by
  - outliers
  - .
  - In
  - this
  - case
  - ','
  - outliers
  - were
  - mostly
  - meant
  - to
  - mean
  - '``'
  - unusually
  - bad
  - predictions
  - ''''''
  - especially
  - of
  - the
  - object
  - position
  - in
  - the
  - pushing
  - task
  - .
  - An
  - important
  - point
  - here
  - is
  - that
  - 'on'
  - the
  - pushing
  - task
  - there
  - is
  - 'no'
  - structural
  - explanation
  - for
  - the
  - bad
  - predictions
  - -LRB-
  - such
  - as
  - for
  - example
  - occlusions
  - -RRB-
  - .
  - Therefore
  - we
  - do
  - not
  - think
  - that
  - they
  - actually
  - violate
  - a
  - gaussian
  - assumption
  - about
  - the
  - observation
  - noise
  - .
  - 'On'
  - the
  - question
  - of
  - how
  - the
  - method
  - handles
  - outliers
  - ':'
  - The
  - idea
  - behind
  - using
  - a
  - heteroscedastic
  - noise
  - model
  - is
  - that
  - it
  - allows
  - to
  - assign
  - different
  - levels
  - of
  - noise
  - to
  - different
  - inputs
  - .
  - For
  - example
  - ','
  - if
  - the
  - object
  - is
  - occluded
  - in
  - the
  - image
  - ','
  - a
  - high
  - observation
  - noise
  - can
  - be
  - predicted
  - .
  - This
  - flags
  - the
  - observations
  - in
  - this
  - timestep
  - as
  - unreliable
  - ','
  - such
  - that
  - the
  - filters
  - rely
  - more
  - 'on'
  - the
  - process
  - model
  - prediction
  - .
  - Such
  - outliers
  - would
  - indeed
  - violate
  - a
  - global
  - gaussian
  - assumption
  - about
  - the
  - observation
  - noise
  - .
  - To
  - alleviate
  - this
  - ','
  - our
  - method
  - instead
  - learns
  - input-dependent
  - '``'
  - local
  - ''''''
  - noise
  - distributions
  - .
  - This
  - allows
  - it
  - to
  - capture
  - e.g.
  - the
  - noise
  - in
  - the
  - unoccluded
  - case
  - with
  - one
  - distribution
  - and
  - the
  - prediction
  - errors
  - in
  - the
  - case
  - of
  - occlusion
  - with
  - another
  - one
  - .
  - References
  - ':'
  - Rico
  - Jonschkowski
  - ','
  - Divyam
  - Rastogi
  - ','
  - and
  - Oliver
  - Brock
  - .
  - Differentiable
  - particle
  - filters
  - ':'
  - End-to-end
  - learning
  - with
  - algorithmic
  - priors
  - .
  - In
  - Proceedings
  - of
  - Robotics
  - ':'
  - Science
  - and
  - Systems
  - ','
  - Pittsburgh
  - ','
  - USA
  - ','
  - '2018'
  - .
  - Alina
  - Kloss
  - ','
  - Stefan
  - Schaal
  - ','
  - and
  - Jeannette
  - Bohg
  - .
  - Combining
  - learned
  - and
  - analytical
  - models
  - for
  - predicting
  - action
  - effects
  - .
  - arXiv
  - preprint
  - arXiv
  - ':'
  - '1710.04102'
  - ','
  - '2017'
  - .
  - Subham
  - Sahoo
  - ','
  - Christoph
  - Lampert
  - and
  - Georg
  - Martius
  - .
  - Learning
  - Equations
  - for
  - Extrapolation
  - and
  - Control
  - .
  - In
  - Proceedings
  - of
  - the
  - 35th
  - International
  - Conference
  - 'on'
  - Machine
  - Learning
  - ','
  - PMLR
  - 80:4442-4450
  - ','
  - '2018'
  - .
- comment_id: B1e8djk9Tm
  rels:
  - !!python/tuple
    - 0
    - 11
    - 11
    - 20
    - circumstance
  - !!python/tuple
    - 0
    - 20
    - 20
    - 796
    - elaboration
  - !!python/tuple
    - 20
    - 30
    - 30
    - 796
    - elaboration
  - !!python/tuple
    - 30
    - 40
    - 40
    - 105
    - elaboration
  - !!python/tuple
    - 40
    - 60
    - 60
    - 105
    - elaboration
  - !!python/tuple
    - 60
    - 67
    - 67
    - 105
    - example
  - !!python/tuple
    - 67
    - 77
    - 77
    - 105
    - elaboration
  - !!python/tuple
    - 77
    - 78
    - 78
    - 105
    - elaboration
  - !!python/tuple
    - 78
    - 85
    - 85
    - 89
    - purpose
  - !!python/tuple
    - 78
    - 89
    - 89
    - 105
    - temporal
  - !!python/tuple
    - 89
    - 94
    - 94
    - 105
    - circumstance
  - !!python/tuple
    - 94
    - 100
    - 100
    - 105
    - elaboration
  - !!python/tuple
    - 30
    - 105
    - 105
    - 796
    - elaboration
  - !!python/tuple
    - 105
    - 113
    - 113
    - 127
    - elaboration
  - !!python/tuple
    - 113
    - 118
    - 118
    - 127
    - circumstance
  - !!python/tuple
    - 105
    - 127
    - 127
    - 796
    - elaboration
  - !!python/tuple
    - 127
    - 134
    - 134
    - 144
    - elaboration
  - !!python/tuple
    - 127
    - 144
    - 144
    - 796
    - elaboration
  - !!python/tuple
    - 144
    - 153
    - 153
    - 796
    - list
  - !!python/tuple
    - 153
    - 796
    - 144
    - 153
    - list
  - !!python/tuple
    - 153
    - 174
    - 174
    - 796
    - list
  - !!python/tuple
    - 153
    - 172
    - 172
    - 174
    - elaboration
  - !!python/tuple
    - 174
    - 796
    - 153
    - 174
    - list
  - !!python/tuple
    - 174
    - 183
    - 183
    - 796
    - elaboration
  - !!python/tuple
    - 183
    - 188
    - 188
    - 796
    - textualorganization
  - !!python/tuple
    - 188
    - 796
    - 183
    - 188
    - textualorganization
  - !!python/tuple
    - 188
    - 202
    - 202
    - 796
    - list
  - !!python/tuple
    - 202
    - 796
    - 188
    - 202
    - list
  - !!python/tuple
    - 202
    - 221
    - 221
    - 796
    - list
  - !!python/tuple
    - 202
    - 217
    - 217
    - 221
    - elaboration
  - !!python/tuple
    - 221
    - 796
    - 202
    - 221
    - list
  - !!python/tuple
    - 221
    - 247
    - 247
    - 272
    - list
  - !!python/tuple
    - 221
    - 235
    - 235
    - 247
    - same_unit
  - !!python/tuple
    - 221
    - 223
    - 223
    - 235
    - elaboration
  - !!python/tuple
    - 235
    - 247
    - 221
    - 235
    - same_unit
  - !!python/tuple
    - 235
    - 242
    - 242
    - 247
    - circumstance
  - !!python/tuple
    - 247
    - 272
    - 221
    - 247
    - list
  - !!python/tuple
    - 247
    - 264
    - 264
    - 272
    - purpose
  - !!python/tuple
    - 221
    - 272
    - 272
    - 796
    - elaboration
  - !!python/tuple
    - 279
    - 305
    - 272
    - 279
    - attribution
  - !!python/tuple
    - 272
    - 305
    - 305
    - 796
    - elaboration
  - !!python/tuple
    - 311
    - 395
    - 305
    - 311
    - attribution
  - !!python/tuple
    - 311
    - 320
    - 320
    - 395
    - elaboration
  - !!python/tuple
    - 320
    - 328
    - 328
    - 395
    - list
  - !!python/tuple
    - 328
    - 395
    - 320
    - 328
    - list
  - !!python/tuple
    - 328
    - 352
    - 352
    - 395
    - same_unit
  - !!python/tuple
    - 328
    - 335
    - 335
    - 352
    - elaboration
  - !!python/tuple
    - 352
    - 395
    - 328
    - 352
    - same_unit
  - !!python/tuple
    - 352
    - 360
    - 360
    - 395
    - list
  - !!python/tuple
    - 360
    - 395
    - 352
    - 360
    - list
  - !!python/tuple
    - 360
    - 366
    - 366
    - 395
    - list
  - !!python/tuple
    - 366
    - 395
    - 360
    - 366
    - list
  - !!python/tuple
    - 366
    - 376
    - 376
    - 395
    - purpose
  - !!python/tuple
    - 305
    - 395
    - 395
    - 796
    - elaboration
  - !!python/tuple
    - 395
    - 411
    - 411
    - 796
    - elaboration
  - !!python/tuple
    - 411
    - 436
    - 436
    - 796
    - list
  - !!python/tuple
    - 411
    - 418
    - 418
    - 436
    - list
  - !!python/tuple
    - 418
    - 436
    - 411
    - 418
    - list
  - !!python/tuple
    - 418
    - 426
    - 426
    - 436
    - elaboration
  - !!python/tuple
    - 436
    - 796
    - 411
    - 436
    - list
  - !!python/tuple
    - 436
    - 479
    - 479
    - 491
    - elaboration
  - !!python/tuple
    - 479
    - 490
    - 490
    - 491
    - same_unit
  - !!python/tuple
    - 479
    - 487
    - 487
    - 490
    - elaboration
  - !!python/tuple
    - 490
    - 491
    - 479
    - 490
    - same_unit
  - !!python/tuple
    - 436
    - 491
    - 491
    - 796
    - elaboration
  - !!python/tuple
    - 491
    - 501
    - 501
    - 796
    - elaboration
  - !!python/tuple
    - 501
    - 522
    - 522
    - 796
    - list
  - !!python/tuple
    - 501
    - 520
    - 520
    - 522
    - elaboration
  - !!python/tuple
    - 522
    - 796
    - 501
    - 522
    - list
  - !!python/tuple
    - 522
    - 622
    - 622
    - 796
    - topic
  - !!python/tuple
    - 522
    - 532
    - 532
    - 534
    - same_unit
  - !!python/tuple
    - 522
    - 524
    - 524
    - 532
    - elaboration
  - !!python/tuple
    - 524
    - 527
    - 527
    - 532
    - elaboration
  - !!python/tuple
    - 532
    - 534
    - 522
    - 532
    - same_unit
  - !!python/tuple
    - 522
    - 534
    - 534
    - 543
    - attribution
  - !!python/tuple
    - 522
    - 543
    - 543
    - 622
    - elaboration
  - !!python/tuple
    - 543
    - 560
    - 560
    - 561
    - same_unit
  - !!python/tuple
    - 543
    - 555
    - 555
    - 560
    - elaboration
  - !!python/tuple
    - 560
    - 561
    - 543
    - 560
    - same_unit
  - !!python/tuple
    - 543
    - 561
    - 561
    - 565
    - elaboration
  - !!python/tuple
    - 543
    - 565
    - 565
    - 622
    - elaboration
  - !!python/tuple
    - 565
    - 591
    - 591
    - 622
    - list
  - !!python/tuple
    - 565
    - 567
    - 567
    - 591
    - elaboration
  - !!python/tuple
    - 567
    - 569
    - 569
    - 591
    - purpose
  - !!python/tuple
    - 569
    - 574
    - 574
    - 591
    - same_unit
  - !!python/tuple
    - 574
    - 591
    - 569
    - 574
    - same_unit
  - !!python/tuple
    - 574
    - 579
    - 579
    - 591
    - elaboration
  - !!python/tuple
    - 579
    - 585
    - 585
    - 591
    - circumstance
  - !!python/tuple
    - 591
    - 622
    - 565
    - 591
    - list
  - !!python/tuple
    - 591
    - 600
    - 600
    - 607
    - circumstance
  - !!python/tuple
    - 600
    - 606
    - 606
    - 607
    - same_unit
  - !!python/tuple
    - 600
    - 601
    - 601
    - 606
    - elaboration
  - !!python/tuple
    - 606
    - 607
    - 600
    - 606
    - same_unit
  - !!python/tuple
    - 591
    - 607
    - 607
    - 622
    - elaboration
  - !!python/tuple
    - 622
    - 796
    - 522
    - 622
    - topic
  - !!python/tuple
    - 622
    - 641
    - 641
    - 796
    - list
  - !!python/tuple
    - 622
    - 637
    - 637
    - 641
    - elaboration
  - !!python/tuple
    - 641
    - 796
    - 622
    - 641
    - list
  - !!python/tuple
    - 641
    - 654
    - 654
    - 669
    - same_unit
  - !!python/tuple
    - 641
    - 652
    - 652
    - 654
    - same_unit
  - !!python/tuple
    - 641
    - 647
    - 647
    - 652
    - elaboration
  - !!python/tuple
    - 652
    - 654
    - 641
    - 652
    - same_unit
  - !!python/tuple
    - 654
    - 669
    - 641
    - 654
    - same_unit
  - !!python/tuple
    - 654
    - 660
    - 660
    - 669
    - elaboration
  - !!python/tuple
    - 641
    - 669
    - 669
    - 796
    - elaboration
  - !!python/tuple
    - 669
    - 686
    - 686
    - 796
    - list
  - !!python/tuple
    - 686
    - 796
    - 669
    - 686
    - list
  - !!python/tuple
    - 690
    - 713
    - 686
    - 690
    - attribution
  - !!python/tuple
    - 690
    - 702
    - 702
    - 713
    - same_unit
  - !!python/tuple
    - 690
    - 698
    - 698
    - 702
    - elaboration
  - !!python/tuple
    - 702
    - 713
    - 690
    - 702
    - same_unit
  - !!python/tuple
    - 702
    - 706
    - 706
    - 713
    - elaboration
  - !!python/tuple
    - 686
    - 713
    - 713
    - 796
    - elaboration
  - !!python/tuple
    - 713
    - 727
    - 727
    - 735
    - list
  - !!python/tuple
    - 727
    - 735
    - 713
    - 727
    - list
  - !!python/tuple
    - 728
    - 735
    - 727
    - 728
    - attribution
  - !!python/tuple
    - 713
    - 735
    - 735
    - 762
    - elaboration
  - !!python/tuple
    - 735
    - 738
    - 738
    - 762
    - purpose
  - !!python/tuple
    - 740
    - 762
    - 738
    - 740
    - attribution
  - !!python/tuple
    - 740
    - 753
    - 753
    - 762
    - elaboration
  - !!python/tuple
    - 753
    - 754
    - 754
    - 762
    - elaboration
  - !!python/tuple
    - 713
    - 762
    - 762
    - 796
    - elaboration
  - !!python/tuple
    - 762
    - 775
    - 775
    - 796
    - elaboration
  - !!python/tuple
    - 775
    - 783
    - 783
    - 796
    - elaboration
  - !!python/tuple
    - 783
    - 784
    - 784
    - 796
    - elaboration
  tokens:
  - The
  - present
  - paper
  - proposes
  - a
  - fast
  - approximation
  - to
  - the
  - softmax
  - computation
  - when
  - the
  - number
  - of
  - classes
  - is
  - very
  - large
  - .
  - This
  - is
  - typically
  - a
  - bottleneck
  - in
  - deep
  - learning
  - architectures
  - .
  - The
  - approximation
  - is
  - a
  - sparse
  - two-layer
  - mixture
  - of
  - experts
  - .
  - The
  - paper
  - lacks
  - rigor
  - and
  - the
  - writing
  - is
  - of
  - low
  - quality
  - ','
  - both
  - in
  - its
  - clarity
  - and
  - its
  - grammar
  - .
  - See
  - a
  - list
  - of
  - typos
  - below
  - .
  - An
  - example
  - of
  - lack
  - of
  - mathematical
  - rigor
  - is
  - equation
  - '4'
  - in
  - which
  - the
  - same
  - variable
  - name
  - is
  - used
  - to
  - describe
  - the
  - weights
  - before
  - and
  - after
  - pruning
  - ','
  - as
  - if
  - it
  - was
  - computer
  - code
  - instead
  - of
  - an
  - equation
  - .
  - Also
  - pervasive
  - is
  - the
  - use
  - of
  - the
  - asterisk
  - to
  - denote
  - multiplication
  - ','
  - again
  - as
  - if
  - it
  - was
  - code
  - and
  - not
  - math
  - .
  - Algorithm
  - '1'
  - does
  - not
  - include
  - mitosis
  - ','
  - which
  - may
  - have
  - an
  - effect
  - 'on'
  - the
  - resulting
  - approximation
  - .
  - How
  - are
  - the
  - lambda
  - and
  - threshold
  - parameters
  - tuned
  - '?'
  - The
  - authors
  - mention
  - a
  - validation
  - set
  - ','
  - are
  - they
  - just
  - exhaustively
  - explored
  - 'on'
  - a
  - 3D
  - grid
  - 'on'
  - the
  - validation
  - set
  - '?'
  - The
  - results
  - only
  - compare
  - with
  - Shim
  - et
  - al.
  - .
  - Why
  - only
  - this
  - method
  - '?'
  - Why
  - would
  - it
  - be
  - expected
  - to
  - be
  - faster
  - than
  - all
  - the
  - other
  - alternatives
  - '?'
  - Would
  - n't
  - similar
  - alternatives
  - like
  - the
  - sparsely
  - gated
  - MoE
  - ','
  - D-softmax
  - and
  - adaptive-softmax
  - have
  - chances
  - of
  - being
  - faster
  - '?'
  - The
  - column
  - '``'
  - FLOPS
  - ''''''
  - in
  - the
  - result
  - seems
  - to
  - measure
  - the
  - speedup
  - ','
  - whereas
  - the
  - actual
  - FLOPS
  - should
  - be
  - less
  - when
  - the
  - speed
  - increases
  - .
  - Also
  - ','
  - a
  - '``'
  - 1x
  - ''''''
  - label
  - seems
  - to
  - be
  - missing
  - in
  - for
  - the
  - full
  - softmax
  - ','
  - so
  - that
  - the
  - reference
  - is
  - clearly
  - specified
  - .
  - All
  - in
  - all
  - ','
  - the
  - results
  - show
  - that
  - the
  - proposed
  - method
  - provides
  - a
  - significant
  - speedup
  - with
  - respect
  - to
  - Shim
  - et
  - al.
  - ','
  - but
  - it
  - lacks
  - comparison
  - with
  - other
  - methods
  - in
  - the
  - literature
  - .
  - A
  - brief
  - list
  - of
  - typos
  - ':'
  - '``'
  - Sparse
  - Mixture
  - of
  - Sparse
  - of
  - Sparse
  - Experts
  - ''''''
  - '``'
  - if
  - we
  - only
  - search
  - right
  - answer
  - ''''''
  - '``'
  - it
  - might
  - also
  - like
  - appear
  - ''''''
  - '``'
  - which
  - is
  - to
  - design
  - to
  - choose
  - the
  - right
  - ''''''
  - sparsly
  - '``'
  - will
  - only
  - consists
  - partial
  - ''''''
  - '``'
  - with
  - NN
  - is
  - a
  - lasso
  - threshold
  - ''''''
  - '``'
  - an
  - arbitrarily
  - distance
  - function
  - ''''''
  - '``'
  - each
  - '10'
  - sub
  - classes
  - are
  - belonged
  - to
  - one
  - ''''''
  - '``'
  - is
  - also
  - needed
  - to
  - tune
  - to
  - achieve
  - ''''''
  - Dear
  - Reviewer
  - ','
  - Thank
  - you
  - for
  - your
  - valuable
  - comments
  - .
  - We
  - have
  - revised
  - our
  - writing
  - in
  - the
  - revision
  - ','
  - and
  - will
  - further
  - improve
  - its
  - clarity
  - .
  - Please
  - find
  - our
  - response
  - as
  - follows
  - .
  - '-'
  - Algorithm
  - '1'
  - does
  - not
  - include
  - mitosis
  - ','
  - which
  - may
  - have
  - an
  - effect
  - 'on'
  - the
  - resulting
  - approximation
  - .
  - Mitosis
  - training
  - can
  - be
  - considered
  - as
  - executing
  - Algorithm
  - '1'
  - for
  - multiple
  - times
  - with
  - an
  - increasing
  - number
  - of
  - experts
  - and
  - inherited
  - initialization
  - from
  - last
  - round
  - by
  - changing
  - W
  - ^
  - e
  - and
  - W
  - ^
  - g.
  - Also
  - ','
  - training
  - with
  - mitosis
  - achieves
  - similar
  - performance
  - as
  - training
  - without
  - it
  - shown
  - in
  - Appendix
  - B
  - ','
  - Figure
  - -LRB-
  - a
  - -RRB-
  - .
  - '-'
  - How
  - are
  - the
  - lambda
  - and
  - threshold
  - parameters
  - tuned
  - '?'
  - The
  - authors
  - mention
  - a
  - validation
  - set
  - ','
  - are
  - they
  - just
  - exhaustively
  - explored
  - 'on'
  - a
  - 3D
  - grid
  - 'on'
  - the
  - validation
  - set
  - '?'
  - The
  - hyper-parameters
  - related
  - to
  - DS-softmax
  - -LRB-
  - such
  - as
  - lambda
  - -RRB-
  - are
  - tuned
  - according
  - to
  - the
  - performance
  - 'on'
  - a
  - validation
  - dataset
  - .
  - Also
  - ','
  - as
  - we
  - mentioned
  - in
  - the
  - paper
  - ','
  - only
  - one
  - hyper-parameter
  - -LRB-
  - group
  - lasso
  - lambda
  - -RRB-
  - needs
  - to
  - be
  - tuned
  - .
  - The
  - heuristic
  - we
  - use
  - to
  - tune
  - group
  - lasso
  - lambda
  - is
  - to
  - increase
  - lambda
  - ','
  - starting
  - from
  - a
  - small
  - value
  - ','
  - until
  - it
  - hurts
  - the
  - performance
  - .
  - Also
  - threshold
  - and
  - balancing
  - lambda
  - variables
  - are
  - kept
  - fixed
  - as
  - -LRB-
  - '0.01'
  - and
  - '10'
  - -RRB-
  - .
  - '-'
  - Why
  - would
  - it
  - be
  - expected
  - to
  - be
  - faster
  - than
  - all
  - the
  - other
  - alternatives
  - '?'
  - Would
  - n't
  - similar
  - alternatives
  - like
  - the
  - sparsely
  - gated
  - MoE
  - ','
  - D-softmax
  - and
  - adaptive-softmax
  - have
  - chances
  - of
  - being
  - faster
  - '?'
  - In
  - terms
  - of
  - baselines
  - ','
  - SVD-softmax
  - -LRB-
  - NIPS
  - ''''
  - '17'
  - -RRB-
  - was
  - chosen
  - since
  - it
  - is
  - a
  - recent
  - method
  - that
  - provides
  - a
  - significant
  - inference
  - speedup
  - for
  - softmax
  - .
  - Other
  - alternatives
  - ','
  - such
  - as
  - D-softmax
  - and
  - adaptive-softmax
  - ','
  - focus
  - 'on'
  - training
  - instead
  - of
  - inference
  - speedup
  - .
  - Furthermore
  - ','
  - as
  - claimed
  - in
  - their
  - papers
  - ','
  - they
  - achieve
  - limited
  - speedup
  - -LRB-
  - around
  - 5x
  - -RRB-
  - in
  - language
  - modeling
  - ','
  - which
  - is
  - much
  - worse
  - than
  - ours
  - .
  - With
  - regards
  - to
  - Sparsely
  - Gated
  - MoE
  - ','
  - it
  - can
  - not
  - speed
  - up
  - inference
  - ','
  - since
  - they
  - select
  - expert
  - with
  - full
  - softmax
  - .
  - We
  - would
  - like
  - to
  - emphasize
  - that
  - most
  - existing
  - methods
  - for
  - inference
  - speedup
  - focus
  - 'on'
  - approximating
  - trained
  - softmax
  - layer
  - ','
  - which
  - usually
  - suffers
  - a
  - loss
  - 'on'
  - performance
  - .
  - Our
  - model
  - allows
  - the
  - adaptive
  - adjustment
  - of
  - the
  - softmax
  - layer
  - ','
  - achieves
  - speedup
  - through
  - capturing
  - the
  - two-level
  - overlapped
  - hierarchy
  - during
  - training
  - ','
  - which
  - is
  - novel
  - and
  - does
  - not
  - suffer
  - from
  - the
  - performance
  - loss
  - .
- comment_id: B1eDAUxO6m
  rels:
  - !!python/tuple
    - 0
    - 10
    - 10
    - 16
    - elaboration
  - !!python/tuple
    - 0
    - 16
    - 16
    - 39
    - elaboration
  - !!python/tuple
    - 16
    - 23
    - 23
    - 39
    - list
  - !!python/tuple
    - 23
    - 39
    - 16
    - 23
    - list
  - !!python/tuple
    - 23
    - 29
    - 29
    - 39
    - means
  - !!python/tuple
    - 0
    - 39
    - 39
    - 1789
    - elaboration
  - !!python/tuple
    - 39
    - 51
    - 51
    - 56
    - elaboration
  - !!python/tuple
    - 39
    - 56
    - 56
    - 1789
    - elaboration
  - !!python/tuple
    - 56
    - 65
    - 65
    - 80
    - list
  - !!python/tuple
    - 56
    - 60
    - 60
    - 65
    - elaboration
  - !!python/tuple
    - 65
    - 80
    - 56
    - 65
    - list
  - !!python/tuple
    - 65
    - 71
    - 71
    - 80
    - elaboration
  - !!python/tuple
    - 71
    - 75
    - 75
    - 80
    - elaboration
  - !!python/tuple
    - 56
    - 80
    - 80
    - 1789
    - elaboration
  - !!python/tuple
    - 80
    - 95
    - 95
    - 1789
    - elaboration
  - !!python/tuple
    - 95
    - 99
    - 99
    - 112
    - condition
  - !!python/tuple
    - 103
    - 112
    - 99
    - 103
    - condition
  - !!python/tuple
    - 95
    - 112
    - 112
    - 1789
    - elaboration
  - !!python/tuple
    - 112
    - 119
    - 119
    - 136
    - purpose
  - !!python/tuple
    - 119
    - 122
    - 122
    - 136
    - circumstance
  - !!python/tuple
    - 122
    - 128
    - 128
    - 136
    - elaboration
  - !!python/tuple
    - 112
    - 136
    - 136
    - 1789
    - elaboration
  - !!python/tuple
    - 136
    - 141
    - 141
    - 161
    - purpose
  - !!python/tuple
    - 141
    - 148
    - 148
    - 161
    - elaboration
  - !!python/tuple
    - 148
    - 152
    - 152
    - 161
    - elaboration
  - !!python/tuple
    - 152
    - 155
    - 155
    - 161
    - elaboration
  - !!python/tuple
    - 136
    - 161
    - 161
    - 1789
    - elaboration
  - !!python/tuple
    - 169
    - 186
    - 161
    - 169
    - condition
  - !!python/tuple
    - 169
    - 182
    - 182
    - 186
    - purpose
  - !!python/tuple
    - 161
    - 186
    - 186
    - 1789
    - elaboration
  - !!python/tuple
    - 190
    - 223
    - 186
    - 190
    - attribution
  - !!python/tuple
    - 190
    - 200
    - 200
    - 223
    - elaboration
  - !!python/tuple
    - 200
    - 217
    - 217
    - 218
    - same_unit
  - !!python/tuple
    - 200
    - 208
    - 208
    - 217
    - elaboration
  - !!python/tuple
    - 217
    - 218
    - 200
    - 217
    - same_unit
  - !!python/tuple
    - 200
    - 218
    - 218
    - 223
    - elaboration
  - !!python/tuple
    - 186
    - 223
    - 223
    - 1789
    - elaboration
  - !!python/tuple
    - 223
    - 232
    - 232
    - 242
    - purpose
  - !!python/tuple
    - 232
    - 237
    - 237
    - 242
    - elaboration
  - !!python/tuple
    - 223
    - 242
    - 242
    - 1789
    - elaboration
  - !!python/tuple
    - 242
    - 245
    - 245
    - 259
    - purpose
  - !!python/tuple
    - 245
    - 251
    - 251
    - 259
    - circumstance
  - !!python/tuple
    - 242
    - 259
    - 259
    - 1789
    - elaboration
  - !!python/tuple
    - 259
    - 263
    - 263
    - 293
    - purpose
  - !!python/tuple
    - 263
    - 273
    - 273
    - 293
    - elaboration
  - !!python/tuple
    - 273
    - 277
    - 277
    - 293
    - elaboration
  - !!python/tuple
    - 259
    - 293
    - 293
    - 1789
    - elaboration
  - !!python/tuple
    - 293
    - 298
    - 298
    - 326
    - purpose
  - !!python/tuple
    - 298
    - 316
    - 316
    - 326
    - elaboration
  - !!python/tuple
    - 293
    - 326
    - 326
    - 1789
    - elaboration
  - !!python/tuple
    - 326
    - 330
    - 330
    - 349
    - purpose
  - !!python/tuple
    - 332
    - 349
    - 330
    - 332
    - attribution
  - !!python/tuple
    - 326
    - 349
    - 349
    - 1789
    - elaboration
  - !!python/tuple
    - 361
    - 373
    - 349
    - 361
    - attribution
  - !!python/tuple
    - 349
    - 373
    - 373
    - 1789
    - elaboration
  - !!python/tuple
    - 373
    - 386
    - 386
    - 395
    - list
  - !!python/tuple
    - 386
    - 395
    - 373
    - 386
    - list
  - !!python/tuple
    - 390
    - 395
    - 386
    - 390
    - attribution
  - !!python/tuple
    - 373
    - 395
    - 395
    - 1789
    - elaboration
  - !!python/tuple
    - 395
    - 400
    - 400
    - 434
    - list
  - !!python/tuple
    - 400
    - 434
    - 395
    - 400
    - list
  - !!python/tuple
    - 400
    - 406
    - 406
    - 434
    - same_unit
  - !!python/tuple
    - 400
    - 404
    - 404
    - 406
    - elaboration
  - !!python/tuple
    - 406
    - 434
    - 400
    - 406
    - same_unit
  - !!python/tuple
    - 414
    - 434
    - 406
    - 414
    - attribution
  - !!python/tuple
    - 414
    - 422
    - 422
    - 434
    - circumstance
  - !!python/tuple
    - 422
    - 426
    - 426
    - 434
    - list
  - !!python/tuple
    - 426
    - 434
    - 422
    - 426
    - list
  - !!python/tuple
    - 395
    - 434
    - 434
    - 1789
    - elaboration
  - !!python/tuple
    - 434
    - 446
    - 446
    - 460
    - purpose
  - !!python/tuple
    - 452
    - 460
    - 446
    - 452
    - attribution
  - !!python/tuple
    - 452
    - 456
    - 456
    - 460
    - elaboration
  - !!python/tuple
    - 434
    - 460
    - 460
    - 1789
    - elaboration
  - !!python/tuple
    - 460
    - 475
    - 475
    - 499
    - elaboration
  - !!python/tuple
    - 475
    - 482
    - 482
    - 499
    - elaboration
  - !!python/tuple
    - 482
    - 491
    - 491
    - 499
    - elaboration
  - !!python/tuple
    - 460
    - 499
    - 499
    - 1789
    - elaboration
  - !!python/tuple
    - 499
    - 514
    - 514
    - 1789
    - elaboration
  - !!python/tuple
    - 514
    - 538
    - 538
    - 1789
    - list
  - !!python/tuple
    - 521
    - 538
    - 514
    - 521
    - attribution
  - !!python/tuple
    - 521
    - 534
    - 534
    - 538
    - same_unit
  - !!python/tuple
    - 521
    - 530
    - 530
    - 534
    - elaboration
  - !!python/tuple
    - 534
    - 538
    - 521
    - 534
    - same_unit
  - !!python/tuple
    - 538
    - 1789
    - 514
    - 538
    - list
  - !!python/tuple
    - 538
    - 548
    - 548
    - 1789
    - elaboration
  - !!python/tuple
    - 548
    - 570
    - 570
    - 1789
    - list
  - !!python/tuple
    - 570
    - 1789
    - 548
    - 570
    - list
  - !!python/tuple
    - 576
    - 609
    - 570
    - 576
    - attribution
  - !!python/tuple
    - 576
    - 592
    - 592
    - 609
    - list
  - !!python/tuple
    - 576
    - 577
    - 577
    - 592
    - elaboration
  - !!python/tuple
    - 592
    - 609
    - 576
    - 592
    - list
  - !!python/tuple
    - 592
    - 597
    - 597
    - 609
    - elaboration
  - !!python/tuple
    - 570
    - 609
    - 609
    - 1789
    - elaboration
  - !!python/tuple
    - 609
    - 625
    - 625
    - 635
    - list
  - !!python/tuple
    - 609
    - 617
    - 617
    - 625
    - elaboration
  - !!python/tuple
    - 625
    - 635
    - 609
    - 625
    - list
  - !!python/tuple
    - 609
    - 635
    - 635
    - 1789
    - elaboration
  - !!python/tuple
    - 635
    - 648
    - 648
    - 1789
    - list
  - !!python/tuple
    - 635
    - 642
    - 642
    - 648
    - elaboration
  - !!python/tuple
    - 648
    - 1789
    - 635
    - 648
    - list
  - !!python/tuple
    - 648
    - 650
    - 650
    - 665
    - elaboration
  - !!python/tuple
    - 648
    - 665
    - 665
    - 1789
    - elaboration
  - !!python/tuple
    - 665
    - 667
    - 667
    - 1789
    - elaboration
  - !!python/tuple
    - 667
    - 668
    - 668
    - 1789
    - elaboration
  - !!python/tuple
    - 668
    - 716
    - 716
    - 1789
    - topic
  - !!python/tuple
    - 673
    - 677
    - 668
    - 673
    - attribution
  - !!python/tuple
    - 668
    - 677
    - 677
    - 716
    - explanation
  - !!python/tuple
    - 677
    - 678
    - 678
    - 679
    - elaboration
  - !!python/tuple
    - 677
    - 679
    - 679
    - 716
    - elaboration
  - !!python/tuple
    - 683
    - 716
    - 679
    - 683
    - attribution
  - !!python/tuple
    - 683
    - 693
    - 693
    - 716
    - elaboration
  - !!python/tuple
    - 693
    - 710
    - 710
    - 711
    - same_unit
  - !!python/tuple
    - 693
    - 701
    - 701
    - 710
    - elaboration
  - !!python/tuple
    - 710
    - 711
    - 693
    - 710
    - same_unit
  - !!python/tuple
    - 693
    - 711
    - 711
    - 716
    - elaboration
  - !!python/tuple
    - 716
    - 1789
    - 668
    - 716
    - topic
  - !!python/tuple
    - 716
    - 935
    - 935
    - 1789
    - list
  - !!python/tuple
    - 716
    - 718
    - 718
    - 730
    - elaboration
  - !!python/tuple
    - 718
    - 723
    - 723
    - 730
    - elaboration
  - !!python/tuple
    - 716
    - 730
    - 730
    - 935
    - elaboration
  - !!python/tuple
    - 730
    - 738
    - 738
    - 755
    - purpose
  - !!python/tuple
    - 738
    - 750
    - 750
    - 755
    - elaboration
  - !!python/tuple
    - 730
    - 755
    - 755
    - 767
    - elaboration
  - !!python/tuple
    - 757
    - 767
    - 755
    - 757
    - attribution
  - !!python/tuple
    - 730
    - 767
    - 767
    - 935
    - elaboration
  - !!python/tuple
    - 767
    - 769
    - 769
    - 788
    - elaboration
  - !!python/tuple
    - 769
    - 778
    - 778
    - 788
    - purpose
  - !!python/tuple
    - 778
    - 783
    - 783
    - 788
    - elaboration
  - !!python/tuple
    - 767
    - 788
    - 788
    - 935
    - elaboration
  - !!python/tuple
    - 788
    - 791
    - 791
    - 935
    - textualorganization
  - !!python/tuple
    - 791
    - 935
    - 788
    - 791
    - textualorganization
  - !!python/tuple
    - 791
    - 807
    - 807
    - 935
    - contrast
  - !!python/tuple
    - 797
    - 807
    - 791
    - 797
    - attribution
  - !!python/tuple
    - 807
    - 935
    - 791
    - 807
    - contrast
  - !!python/tuple
    - 807
    - 813
    - 813
    - 822
    - elaboration
  - !!python/tuple
    - 807
    - 822
    - 822
    - 935
    - explanation
  - !!python/tuple
    - 822
    - 838
    - 838
    - 847
    - elaboration
  - !!python/tuple
    - 822
    - 847
    - 847
    - 935
    - elaboration
  - !!python/tuple
    - 847
    - 848
    - 848
    - 849
    - elaboration
  - !!python/tuple
    - 847
    - 849
    - 849
    - 872
    - elaboration
  - !!python/tuple
    - 849
    - 853
    - 853
    - 872
    - purpose
  - !!python/tuple
    - 855
    - 872
    - 853
    - 855
    - attribution
  - !!python/tuple
    - 847
    - 872
    - 872
    - 935
    - elaboration
  - !!python/tuple
    - 872
    - 874
    - 874
    - 885
    - elaboration
  - !!python/tuple
    - 872
    - 885
    - 885
    - 935
    - elaboration
  - !!python/tuple
    - 885
    - 892
    - 892
    - 899
    - reason
  - !!python/tuple
    - 892
    - 896
    - 896
    - 899
    - purpose
  - !!python/tuple
    - 885
    - 899
    - 899
    - 935
    - example
  - !!python/tuple
    - 899
    - 913
    - 913
    - 935
    - elaboration
  - !!python/tuple
    - 913
    - 920
    - 920
    - 935
    - elaboration
  - !!python/tuple
    - 935
    - 1789
    - 716
    - 935
    - list
  - !!python/tuple
    - 935
    - 937
    - 937
    - 1789
    - textualorganization
  - !!python/tuple
    - 937
    - 1789
    - 935
    - 937
    - textualorganization
  - !!python/tuple
    - 937
    - 942
    - 942
    - 976
    - list
  - !!python/tuple
    - 942
    - 976
    - 937
    - 942
    - list
  - !!python/tuple
    - 937
    - 976
    - 976
    - 1789
    - elaboration
  - !!python/tuple
    - 976
    - 977
    - 977
    - 978
    - elaboration
  - !!python/tuple
    - 976
    - 978
    - 978
    - 998
    - elaboration
  - !!python/tuple
    - 976
    - 998
    - 998
    - 1003
    - elaboration
  - !!python/tuple
    - 976
    - 1003
    - 1003
    - 1789
    - elaboration
  - !!python/tuple
    - 1003
    - 1021
    - 1021
    - 1789
    - list
  - !!python/tuple
    - 1003
    - 1010
    - 1010
    - 1021
    - elaboration
  - !!python/tuple
    - 1021
    - 1789
    - 1003
    - 1021
    - list
  - !!python/tuple
    - 1021
    - 1041
    - 1041
    - 1789
    - list
  - !!python/tuple
    - 1021
    - 1028
    - 1028
    - 1041
    - elaboration
  - !!python/tuple
    - 1041
    - 1789
    - 1021
    - 1041
    - list
  - !!python/tuple
    - 1041
    - 1047
    - 1047
    - 1789
    - list
  - !!python/tuple
    - 1047
    - 1789
    - 1041
    - 1047
    - list
  - !!python/tuple
    - 1047
    - 1048
    - 1048
    - 1054
    - elaboration
  - !!python/tuple
    - 1047
    - 1054
    - 1054
    - 1103
    - elaboration
  - !!python/tuple
    - 1063
    - 1079
    - 1054
    - 1063
    - attribution
  - !!python/tuple
    - 1054
    - 1057
    - 1057
    - 1063
    - purpose
  - !!python/tuple
    - 1054
    - 1079
    - 1079
    - 1103
    - elaboration
  - !!python/tuple
    - 1079
    - 1086
    - 1086
    - 1103
    - elaboration
  - !!python/tuple
    - 1086
    - 1095
    - 1095
    - 1103
    - elaboration
  - !!python/tuple
    - 1047
    - 1103
    - 1103
    - 1789
    - elaboration
  - !!python/tuple
    - 1103
    - 1118
    - 1118
    - 1142
    - elaboration
  - !!python/tuple
    - 1125
    - 1142
    - 1118
    - 1125
    - attribution
  - !!python/tuple
    - 1125
    - 1138
    - 1138
    - 1142
    - same_unit
  - !!python/tuple
    - 1125
    - 1134
    - 1134
    - 1138
    - elaboration
  - !!python/tuple
    - 1138
    - 1142
    - 1125
    - 1138
    - same_unit
  - !!python/tuple
    - 1103
    - 1142
    - 1142
    - 1789
    - elaboration
  - !!python/tuple
    - 1142
    - 1252
    - 1252
    - 1789
    - list
  - !!python/tuple
    - 1142
    - 1152
    - 1152
    - 1252
    - elaboration
  - !!python/tuple
    - 1152
    - 1174
    - 1174
    - 1213
    - elaboration
  - !!python/tuple
    - 1152
    - 1213
    - 1213
    - 1239
    - elaboration
  - !!python/tuple
    - 1213
    - 1229
    - 1229
    - 1239
    - list
  - !!python/tuple
    - 1213
    - 1221
    - 1221
    - 1229
    - elaboration
  - !!python/tuple
    - 1229
    - 1239
    - 1213
    - 1229
    - list
  - !!python/tuple
    - 1152
    - 1239
    - 1239
    - 1246
    - elaboration
  - !!python/tuple
    - 1152
    - 1246
    - 1246
    - 1252
    - elaboration
  - !!python/tuple
    - 1252
    - 1789
    - 1142
    - 1252
    - list
  - !!python/tuple
    - 1252
    - 1254
    - 1254
    - 1269
    - elaboration
  - !!python/tuple
    - 1252
    - 1269
    - 1269
    - 1789
    - elaboration
  - !!python/tuple
    - 1269
    - 1271
    - 1271
    - 1789
    - elaboration
  - !!python/tuple
    - 1271
    - 1272
    - 1272
    - 1789
    - elaboration
  - !!python/tuple
    - 1282
    - 1789
    - 1272
    - 1282
    - attribution
  - !!python/tuple
    - 1274
    - 1282
    - 1272
    - 1274
    - attribution
  - !!python/tuple
    - 1277
    - 1282
    - 1274
    - 1277
    - attribution
  - !!python/tuple
    - 1284
    - 1344
    - 1282
    - 1284
    - attribution
  - !!python/tuple
    - 1284
    - 1288
    - 1288
    - 1344
    - elaboration
  - !!python/tuple
    - 1288
    - 1301
    - 1301
    - 1305
    - same_unit
  - !!python/tuple
    - 1301
    - 1305
    - 1288
    - 1301
    - same_unit
  - !!python/tuple
    - 1288
    - 1305
    - 1305
    - 1344
    - elaboration
  - !!python/tuple
    - 1282
    - 1344
    - 1344
    - 1789
    - elaboration
  - !!python/tuple
    - 1344
    - 1350
    - 1350
    - 1789
    - textualorganization
  - !!python/tuple
    - 1350
    - 1789
    - 1344
    - 1350
    - textualorganization
  - !!python/tuple
    - 1350
    - 1370
    - 1370
    - 1789
    - elaboration
  - !!python/tuple
    - 1370
    - 1379
    - 1379
    - 1391
    - elaboration
  - !!python/tuple
    - 1370
    - 1391
    - 1391
    - 1789
    - explanation
  - !!python/tuple
    - 1420
    - 1789
    - 1391
    - 1420
    - attribution
  - !!python/tuple
    - 1391
    - 1394
    - 1394
    - 1420
    - purpose
  - !!python/tuple
    - 1394
    - 1400
    - 1400
    - 1420
    - elaboration
  - !!python/tuple
    - 1400
    - 1411
    - 1411
    - 1420
    - list
  - !!python/tuple
    - 1411
    - 1420
    - 1400
    - 1411
    - list
  - !!python/tuple
    - 1411
    - 1414
    - 1414
    - 1420
    - elaboration
  - !!python/tuple
    - 1420
    - 1449
    - 1449
    - 1460
    - elaboration
  - !!python/tuple
    - 1420
    - 1460
    - 1460
    - 1520
    - elaboration
  - !!python/tuple
    - 1460
    - 1468
    - 1468
    - 1520
    - same_unit
  - !!python/tuple
    - 1460
    - 1465
    - 1465
    - 1468
    - elaboration
  - !!python/tuple
    - 1468
    - 1520
    - 1460
    - 1468
    - same_unit
  - !!python/tuple
    - 1468
    - 1479
    - 1479
    - 1520
    - elaboration
  - !!python/tuple
    - 1479
    - 1490
    - 1490
    - 1520
    - elaboration
  - !!python/tuple
    - 1490
    - 1501
    - 1501
    - 1502
    - list
  - !!python/tuple
    - 1501
    - 1502
    - 1490
    - 1501
    - list
  - !!python/tuple
    - 1490
    - 1502
    - 1502
    - 1520
    - elaboration
  - !!python/tuple
    - 1420
    - 1520
    - 1520
    - 1789
    - elaboration
  - !!python/tuple
    - 1522
    - 1539
    - 1520
    - 1522
    - attribution
  - !!python/tuple
    - 1522
    - 1528
    - 1528
    - 1539
    - comparison
  - !!python/tuple
    - 1528
    - 1532
    - 1532
    - 1539
    - list
  - !!python/tuple
    - 1532
    - 1539
    - 1528
    - 1532
    - list
  - !!python/tuple
    - 1532
    - 1536
    - 1536
    - 1539
    - circumstance
  - !!python/tuple
    - 1520
    - 1539
    - 1539
    - 1789
    - elaboration
  - !!python/tuple
    - 1539
    - 1540
    - 1540
    - 1547
    - elaboration
  - !!python/tuple
    - 1539
    - 1547
    - 1547
    - 1601
    - elaboration
  - !!python/tuple
    - 1547
    - 1553
    - 1553
    - 1577
    - purpose
  - !!python/tuple
    - 1570
    - 1577
    - 1553
    - 1570
    - attribution
  - !!python/tuple
    - 1547
    - 1577
    - 1577
    - 1601
    - elaboration
  - !!python/tuple
    - 1580
    - 1601
    - 1577
    - 1580
    - attribution
  - !!python/tuple
    - 1580
    - 1589
    - 1589
    - 1601
    - elaboration
  - !!python/tuple
    - 1589
    - 1595
    - 1595
    - 1601
    - elaboration
  - !!python/tuple
    - 1539
    - 1601
    - 1601
    - 1789
    - elaboration
  - !!python/tuple
    - 1601
    - 1610
    - 1610
    - 1789
    - explanation
  - !!python/tuple
    - 1610
    - 1633
    - 1633
    - 1789
    - elaboration
  - !!python/tuple
    - 1633
    - 1643
    - 1643
    - 1657
    - elaboration
  - !!python/tuple
    - 1646
    - 1657
    - 1643
    - 1646
    - attribution
  - !!python/tuple
    - 1646
    - 1647
    - 1647
    - 1657
    - purpose
  - !!python/tuple
    - 1647
    - 1651
    - 1651
    - 1657
    - list
  - !!python/tuple
    - 1651
    - 1657
    - 1647
    - 1651
    - list
  - !!python/tuple
    - 1633
    - 1657
    - 1657
    - 1789
    - elaboration
  - !!python/tuple
    - 1661
    - 1685
    - 1657
    - 1661
    - attribution
  - !!python/tuple
    - 1657
    - 1685
    - 1685
    - 1789
    - elaboration
  - !!python/tuple
    - 1693
    - 1703
    - 1685
    - 1693
    - attribution
  - !!python/tuple
    - 1693
    - 1698
    - 1698
    - 1703
    - elaboration
  - !!python/tuple
    - 1685
    - 1703
    - 1703
    - 1789
    - elaboration
  - !!python/tuple
    - 1703
    - 1735
    - 1735
    - 1789
    - list
  - !!python/tuple
    - 1703
    - 1714
    - 1714
    - 1735
    - elaboration
  - !!python/tuple
    - 1735
    - 1789
    - 1703
    - 1735
    - list
  - !!python/tuple
    - 1735
    - 1745
    - 1745
    - 1755
    - elaboration
  - !!python/tuple
    - 1735
    - 1755
    - 1755
    - 1789
    - elaboration
  - !!python/tuple
    - 1755
    - 1764
    - 1764
    - 1777
    - elaboration
  - !!python/tuple
    - 1764
    - 1772
    - 1772
    - 1777
    - same_unit
  - !!python/tuple
    - 1764
    - 1768
    - 1768
    - 1772
    - elaboration
  - !!python/tuple
    - 1772
    - 1777
    - 1764
    - 1772
    - same_unit
  - !!python/tuple
    - 1755
    - 1777
    - 1777
    - 1789
    - elaboration
  tokens:
  - The
  - authors
  - propose
  - an
  - extension
  - of
  - hindsight
  - replay
  - to
  - settings
  - where
  - the
  - goal
  - is
  - moving
  - .
  - This
  - consists
  - in
  - taking
  - a
  - failed
  - episode
  - and
  - constructing
  - a
  - valid
  - moving
  - goal
  - by
  - searching
  - prior
  - experiences
  - for
  - a
  - compatible
  - goal
  - trajectory
  - .
  - Results
  - are
  - shown
  - 'on'
  - simulated
  - robotic
  - grasping
  - tasks
  - and
  - a
  - toy
  - task
  - introduced
  - by
  - the
  - authors
  - .
  - Authors
  - show
  - improved
  - results
  - compared
  - to
  - other
  - baselines
  - .
  - The
  - authors
  - also
  - show
  - a
  - demonstration
  - of
  - transfering
  - their
  - policies
  - to
  - the
  - real
  - world
  - .
  - The
  - algorithm
  - appears
  - very
  - specific
  - and
  - not
  - applicable
  - to
  - all
  - cases
  - with
  - dynamic
  - goals
  - .
  - It
  - would
  - be
  - good
  - if
  - the
  - authors
  - discussed
  - when
  - it
  - can
  - and
  - can
  - not
  - be
  - applied
  - .
  - My
  - understanding
  - is
  - it
  - would
  - be
  - hard
  - to
  - apply
  - this
  - when
  - the
  - environment
  - changes
  - across
  - episodes
  - as
  - there
  - needs
  - to
  - be
  - matching
  - trajectories
  - .
  - It
  - would
  - also
  - be
  - hard
  - to
  - apply
  - this
  - for
  - the
  - same
  - reason
  - if
  - there
  - are
  - dynamics
  - chaging
  - the
  - environment
  - -LRB-
  - besides
  - the
  - goal
  - -RRB-
  - .
  - If
  - the
  - goal
  - was
  - following
  - more
  - complex
  - dynamics
  - like
  - teleporting
  - from
  - one
  - place
  - it
  - seems
  - it
  - would
  - again
  - be
  - rather
  - hard
  - to
  - adapt
  - this
  - .
  - I
  - am
  - also
  - wondering
  - if
  - for
  - most
  - practical
  - cases
  - one
  - could
  - construct
  - a
  - heuristic
  - for
  - making
  - the
  - goal
  - trajectory
  - a
  - valid
  - one
  - -LRB-
  - not
  - necessarily
  - relying
  - 'on'
  - knowing
  - exact
  - dynamics
  - -RRB-
  - thus
  - avoiding
  - the
  - matching
  - step
  - .
  - The
  - literature
  - review
  - and
  - the
  - baselines
  - do
  - not
  - appear
  - to
  - consider
  - any
  - other
  - methods
  - designed
  - for
  - dynamic
  - goals
  - .
  - The
  - paper
  - seems
  - to
  - approach
  - the
  - dynamic
  - goal
  - problem
  - as
  - if
  - it
  - was
  - a
  - fresh
  - problem
  - .
  - It
  - would
  - be
  - good
  - to
  - have
  - a
  - better
  - overview
  - of
  - this
  - field
  - and
  - baselines
  - that
  - address
  - this
  - problem
  - as
  - it
  - has
  - certainly
  - been
  - studied
  - in
  - robotics
  - ','
  - computer
  - vision
  - ','
  - and
  - reinforcement
  - learning
  - .
  - I
  - find
  - this
  - paper
  - hard
  - to
  - assess
  - without
  - a
  - more
  - appropriate
  - context
  - for
  - this
  - problem
  - besides
  - a
  - recently
  - proposed
  - technique
  - for
  - sparse
  - rewards
  - that
  - the
  - authors
  - might
  - want
  - to
  - adapt
  - to
  - it
  - .
  - I
  - find
  - it
  - difficult
  - to
  - believe
  - that
  - nobody
  - has
  - studied
  - solutions
  - to
  - this
  - problem
  - and
  - solutions
  - specific
  - to
  - that
  - do
  - n't
  - exist
  - .
  - The
  - writing
  - is
  - a
  - bit
  - repetitive
  - at
  - times
  - and
  - I
  - do
  - believe
  - the
  - algorithm
  - can
  - be
  - more
  - tersely
  - summarized
  - earlier
  - in
  - the
  - paper
  - .
  - It
  - '''s'
  - difficult
  - to
  - get
  - the
  - full
  - idea
  - from
  - the
  - Algorithm
  - block
  - .
  - Overall
  - ','
  - I
  - think
  - the
  - paper
  - is
  - borderline
  - .
  - There
  - is
  - several
  - interesting
  - ideas
  - and
  - a
  - new
  - dataset
  - introduced
  - ','
  - but
  - I
  - would
  - like
  - to
  - be
  - more
  - convinced
  - that
  - the
  - problems
  - tackled
  - are
  - indeed
  - as
  - hard
  - as
  - the
  - authors
  - claim
  - and
  - to
  - have
  - a
  - better
  - literature
  - review
  - .
  - We
  - thank
  - the
  - reviewer
  - for
  - the
  - comments
  - ','
  - and
  - we
  - would
  - like
  - to
  - clarify
  - a
  - few
  - important
  - misconceptions
  - that
  - the
  - reviewer
  - has
  - regarding
  - our
  - work
  - .
  - '1'
  - -RRB-
  - We
  - position
  - the
  - paper
  - in
  - the
  - context
  - of
  - RL
  - with
  - sparse
  - rewards
  - .
  - We
  - follow
  - the
  - goal
  - setting
  - of
  - UVFA
  - -LRB-
  - Schaul
  - et
  - al.
  - ','
  - 2015a
  - -RRB-
  - and
  - HER
  - -LRB-
  - Andrychowicz
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - .
  - The
  - dynamic
  - goal
  - problem
  - is
  - extended
  - from
  - this
  - setting
  - ','
  - not
  - all
  - other
  - cases
  - .
  - Please
  - see
  - paragraph
  - '3'
  - in
  - Section
  - '1'
  - -LRB-
  - Introduction
  - -RRB-
  - and
  - paragraph
  - '1'
  - in
  - Section
  - '3.1'
  - -LRB-
  - Dynamic
  - goals
  - -RRB-
  - for
  - more
  - descriptions
  - .
  - '2'
  - -RRB-
  - We
  - propose
  - a
  - new
  - experience
  - replay
  - method
  - .
  - The
  - proposed
  - algorithm
  - can
  - be
  - combined
  - with
  - any
  - off-policy
  - RL
  - algorithms
  - ','
  - similar
  - to
  - HER
  - ','
  - as
  - shown
  - in
  - Figure
  - '1'
  - .
  - '3'
  - -RRB-
  - The
  - motivation
  - of
  - developing
  - algorithms
  - which
  - can
  - learn
  - from
  - unshaped
  - reward
  - signals
  - is
  - that
  - it
  - does
  - not
  - need
  - domain-specific
  - knowledge
  - and
  - is
  - applicable
  - in
  - situations
  - where
  - we
  - do
  - not
  - know
  - what
  - admissible
  - behaviour
  - may
  - look
  - like
  - .
  - The
  - similar
  - motivation
  - is
  - also
  - mentioned
  - in
  - HER
  - -LRB-
  - Andrychowicz
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - .
  - We
  - also
  - added
  - new
  - experimental
  - results
  - about
  - dense
  - rewards
  - .
  - The
  - results
  - show
  - DHER
  - works
  - better
  - .
  - See
  - Figures
  - '3'
  - and
  - '6'
  - .
  - Q1
  - ':'
  - The
  - algorithm
  - appears
  - very
  - specific
  - and
  - not
  - applicable
  - to
  - all
  - cases
  - with
  - dynamic
  - goals
  - .
  - '...'
  - A1
  - ':'
  - Please
  - see
  - '1'
  - -RRB-
  - and
  - '3'
  - -RRB-
  - above
  - .
  - Q2
  - ':'
  - I
  - am
  - also
  - wondering
  - if
  - for
  - most
  - practical
  - cases
  - one
  - could
  - construct
  - a
  - heuristic
  - for
  - making
  - the
  - goal
  - trajectory
  - a
  - valid
  - one
  - -LRB-
  - not
  - necessarily
  - relying
  - 'on'
  - knowing
  - exact
  - dynamics
  - -RRB-
  - thus
  - avoiding
  - the
  - matching
  - step
  - .
  - A2
  - ':'
  - It
  - is
  - a
  - good
  - idea
  - to
  - take
  - domain
  - heuristics
  - into
  - consideration
  - .
  - However
  - ','
  - in
  - our
  - paper
  - ','
  - we
  - aim
  - to
  - construct
  - a
  - model-free
  - method
  - for
  - dynamic
  - goals
  - to
  - avoid
  - the
  - complexity
  - of
  - constructing
  - goal
  - trajectories
  - .
  - We
  - agree
  - that
  - your
  - idea
  - worths
  - a
  - try
  - in
  - the
  - future
  - .
  - Q3
  - ':'
  - The
  - literature
  - review
  - and
  - the
  - baselines
  - do
  - not
  - appear
  - to
  - consider
  - any
  - other
  - methods
  - designed
  - for
  - dynamic
  - goals
  - .
  - '...'
  - A3
  - ':'
  - We
  - do
  - not
  - want
  - to
  - claim
  - that
  - the
  - dynamic
  - goal
  - problem
  - is
  - a
  - fresh
  - problem
  - .
  - However
  - ','
  - there
  - is
  - little
  - work
  - addressing
  - dynamic
  - goals
  - in
  - the
  - sparse
  - reward
  - setting
  - .
  - As
  - far
  - as
  - we
  - know
  - ','
  - there
  - is
  - 'no'
  - open-source
  - RL
  - environments
  - for
  - such
  - problems
  - .
  - -LRB-
  - OpenAI
  - Gym
  - Robotics
  - uses
  - fixed
  - goals
  - .
  - -RRB-
  - Q4
  - ':'
  - I
  - find
  - it
  - difficult
  - to
  - believe
  - that
  - nobody
  - has
  - studied
  - solutions
  - to
  - this
  - problem
  - and
  - solutions
  - specific
  - to
  - that
  - do
  - n't
  - exist
  - .
  - A4
  - ':'
  - Our
  - paper
  - focuses
  - 'on'
  - addressing
  - dynamic
  - goals
  - with
  - sparse
  - rewards
  - .
  - This
  - setting
  - has
  - not
  - been
  - addressed
  - probably
  - because
  - it
  - is
  - difficult
  - to
  - learn
  - .
  - For
  - example
  - ','
  - the
  - recently
  - developed
  - DDPG
  - and
  - HER
  - failed
  - in
  - our
  - tasks
  - .
  - Moreover
  - ','
  - there
  - is
  - 'no'
  - open-source
  - environments
  - for
  - the
  - dynamic
  - goals
  - and
  - sparse
  - rewards
  - ','
  - to
  - the
  - best
  - of
  - our
  - knowledge
  - .
  - Q5
  - ':'
  - There
  - is
  - several
  - interesting
  - ideas
  - and
  - a
  - new
  - dataset
  - introduced
  - ','
  - but
  - I
  - would
  - like
  - to
  - be
  - more
  - convinced
  - that
  - the
  - problems
  - tackled
  - are
  - indeed
  - as
  - hard
  - as
  - the
  - authors
  - claim
  - and
  - to
  - have
  - a
  - better
  - literature
  - review
  - .
  - A5
  - ':'
  - Except
  - for
  - sparse
  - rewards
  - ','
  - we
  - also
  - added
  - new
  - experimental
  - results
  - about
  - dense
  - rewards
  - for
  - the
  - dynamic
  - goal
  - setting
  - .
  - We
  - have
  - similar
  - results
  - .
  - Similar
  - to
  - DDPG
  - and
  - DDPG+HER
  - ','
  - DDPG
  - -LRB-
  - dense
  - -RRB-
  - does
  - not
  - work
  - well
  - in
  - our
  - tasks
  - .
  - For
  - the
  - simple
  - Dy-Snake
  - environment
  - ','
  - DQN
  - -LRB-
  - dense
  - -RRB-
  - is
  - better
  - than
  - DQN
  - but
  - not
  - better
  - than
  - DQN+DHER
  - .
  - See
  - Figures
  - '3'
  - and
  - '6'
  - .
  - Thanks
  - for
  - your
  - response
  - and
  - clarifications
  - .
  - I
  - would
  - like
  - to
  - comment
  - 'on'
  - this
  - point
  - ':'
  - '``'
  - '1'
  - -RRB-
  - We
  - position
  - the
  - paper
  - in
  - the
  - context
  - of
  - RL
  - with
  - sparse
  - rewards
  - .
  - We
  - follow
  - the
  - goal
  - setting
  - of
  - UVFA
  - -LRB-
  - Schaul
  - et
  - al.
  - ','
  - 2015a
  - -RRB-
  - and
  - HER
  - -LRB-
  - Andrychowicz
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - .
  - The
  - dynamic
  - goal
  - problem
  - is
  - extended
  - from
  - this
  - setting
  - ','
  - not
  - all
  - other
  - cases
  - .
  - Please
  - see
  - paragraph
  - '3'
  - in
  - Section
  - '1'
  - -LRB-
  - Introduction
  - -RRB-
  - and
  - paragraph
  - '1'
  - in
  - Section
  - '3.1'
  - -LRB-
  - Dynamic
  - goals
  - -RRB-
  - for
  - more
  - descriptions
  - .
  - '2'
  - -RRB-
  - We
  - propose
  - a
  - new
  - experience
  - replay
  - method
  - .
  - The
  - proposed
  - algorithm
  - can
  - be
  - combined
  - with
  - any
  - off-policy
  - RL
  - algorithms
  - ','
  - similar
  - to
  - HER
  - ','
  - as
  - shown
  - in
  - Figure
  - '1'
  - .
  - '3'
  - -RRB-
  - The
  - motivation
  - of
  - developing
  - algorithms
  - which
  - can
  - learn
  - from
  - unshaped
  - reward
  - signals
  - is
  - that
  - it
  - does
  - not
  - need
  - domain-specific
  - knowledge
  - and
  - is
  - applicable
  - in
  - situations
  - where
  - we
  - do
  - not
  - know
  - what
  - admissible
  - behaviour
  - may
  - look
  - like
  - .
  - The
  - similar
  - motivation
  - is
  - also
  - mentioned
  - in
  - HER
  - -LRB-
  - Andrychowicz
  - et
  - al.
  - ','
  - '2017'
  - -RRB-
  - .
  - We
  - also
  - added
  - new
  - experimental
  - results
  - about
  - dense
  - rewards
  - .
  - The
  - results
  - show
  - DHER
  - works
  - better
  - .
  - See
  - Figures
  - '3'
  - and
  - '6'
  - .
  - Q1
  - ':'
  - The
  - algorithm
  - appears
  - very
  - specific
  - and
  - not
  - applicable
  - to
  - all
  - cases
  - with
  - dynamic
  - goals
  - .
  - '...'
  - A1
  - ':'
  - Please
  - see
  - '1'
  - -RRB-
  - and
  - '3'
  - -RRB-
  - above
  - .
  - ''''''
  - I
  - believe
  - this
  - kind
  - of
  - motivation
  - as
  - a
  - principled
  - approach
  - to
  - RL
  - with
  - sparse
  - rewards
  - and
  - 'no'
  - domain
  - knowledge
  - is
  - an
  - overclaim
  - .
  - The
  - HER
  - algorithm
  - is
  - a
  - heuristic
  - one
  - and
  - to
  - the
  - best
  - of
  - my
  - understanding
  - requires
  - a
  - domain
  - specific
  - knowledge
  - of
  - how
  - to
  - set
  - fake
  - goals
  - ','
  - which
  - is
  - natural
  - in
  - many
  - settings
  - such
  - as
  - grid
  - worlds
  - for
  - example
  - .
  - The
  - moving
  - goal
  - case
  - described
  - here
  - requires
  - even
  - more
  - domain
  - specific
  - knowledge
  - and
  - I
  - am
  - not
  - convinced
  - is
  - truly
  - '``'
  - model-free
  - ''''''
  - in
  - most
  - cases
  - .
  - To
  - the
  - best
  - of
  - my
  - understanding
  - the
  - matching
  - phase
  - of
  - your
  - method
  - requires
  - a
  - domain
  - specific
  - understanding
  - of
  - goal
  - similarity
  - .
  - Is
  - it
  - possible
  - to
  - provide
  - a
  - dynamic
  - goal
  - example
  - that
  - is
  - not
  - just
  - a
  - simple
  - and
  - short
  - trajectory
  - in
  - space
  - and
  - makes
  - sense
  - to
  - be
  - applied
  - with
  - DHER
  - '?'
  - Could
  - the
  - authors
  - for
  - example
  - explain
  - how
  - the
  - algorithm
  - would
  - be
  - applicable
  - in
  - a
  - case
  - of
  - an
  - Atari
  - style
  - game
  - where
  - a
  - goal
  - would
  - teleport
  - or
  - have
  - long
  - trajectories
  - -LRB-
  - non-trivial
  - to
  - match
  - without
  - a
  - complex
  - matching
  - heuristic
  - -RRB-
  - .
  - It
  - seems
  - in
  - this
  - case
  - -LRB-
  - a
  - -RRB-
  - one
  - would
  - have
  - to
  - obtain
  - precise
  - coordinate
  - positions
  - of
  - the
  - goal
  - -LRB-
  - this
  - would
  - mean
  - one
  - ca
  - n't
  - just
  - solve
  - the
  - problem
  - based
  - 'on'
  - pure
  - pixels
  - and
  - must
  - rely
  - 'on'
  - domain
  - knowledge
  - -RRB-
  - and
  - -LRB-
  - b
  - -RRB-
  - the
  - matching
  - algorithm
  - itself
  - would
  - need
  - to
  - be
  - heavily
  - crafted
  - with
  - domain
  - specific
  - knowledge
  - .
  - I
  - think
  - the
  - method
  - might
  - be
  - more
  - specific
  - than
  - the
  - authors
  - claim
  - and
  - should
  - be
  - presented
  - as
  - such
  - .
  - Thanks
  - for
  - discussing
  - the
  - limitation
  - of
  - DHER
  - .
  - Similar
  - to
  - HER
  - ','
  - we
  - need
  - to
  - have
  - the
  - definition
  - of
  - goals
  - and
  - know
  - the
  - similarity
  - metric
  - between
  - goals
  - in
  - order
  - to
  - construct
  - '``'
  - success
  - ''''''
  - from
  - failed
  - experiences
  - .
  - We
  - had
  - provided
  - how
  - to
  - use
  - and
  - define
  - goals
  - in
  - Section
  - '3.1'
  - --
  - and
  - we
  - made
  - addition
  - revisions
  - to
  - make
  - it
  - more
  - clear
  - .
  - See
  - Sections
  - '1'
  - and
  - '3.1'
  - for
  - the
  - discussions
  - .
  - Because
  - we
  - have
  - the
  - same
  - multi-goal
  - assumption
  - as
  - HER
  - ','
  - we
  - did
  - not
  - claim
  - our
  - method
  - can
  - be
  - used
  - for
  - every
  - case
  - .
  - However
  - ','
  - it
  - still
  - can
  - be
  - applied
  - to
  - many
  - domains
  - if
  - we
  - know
  - how
  - to
  - define
  - the
  - goals
  - and
  - if
  - their
  - trajectories
  - intersect
  - .
  - For
  - a
  - game
  - ','
  - if
  - its
  - goals
  - can
  - be
  - used
  - as
  - part
  - of
  - the
  - observation
  - and
  - do
  - not
  - affect
  - the
  - environment
  - dynamics
  - ','
  - our
  - algorithm
  - will
  - work
  - .
  - Regarding
  - the
  - Atari
  - games
  - ','
  - we
  - did
  - find
  - that
  - there
  - is
  - 'no'
  - game
  - satisfying
  - the
  - multi-goal
  - assumption
  - .
  - However
  - ','
  - our
  - approach
  - can
  - be
  - potentially
  - used
  - for
  - other
  - games
  - where
  - we
  - know
  - the
  - similarity
  - of
  - goals
  - ','
  - for
  - example
  - ','
  - hunting
  - for
  - food
  - in
  - a
  - Minecraft-like
  - grid
  - world
  - mini-game
  - .
  - The
  - Dy-Snake
  - game
  - in
  - our
  - work
  - serves
  - as
  - a
  - reference
  - for
  - which
  - types
  - of
  - games
  - our
  - approach
  - can
  - benefit
  - .
  - The
  - algorithm
  - is
  - very
  - natural
  - for
  - many
  - manipulation
  - tasks
  - because
  - we
  - can
  - access
  - -LRB-
  - sometimes
  - noisy
  - -RRB-
  - object
  - positions
  - in
  - manipulation
  - .
  - The
  - starting
  - point
  - of
  - this
  - work
  - is
  - actually
  - for
  - manipulation
  - controls
  - .
